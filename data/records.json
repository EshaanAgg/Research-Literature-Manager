{
  "papers": [
    {
      "id": "c8efcc854d97dfc2a42b83316a2109f9d166e43f",
      "url": "https://www.semanticscholar.org/paper/c8efcc854d97dfc2a42b83316a2109f9d166e43f",
      "citationCount": 1076,
      "influentialCitationCount": 160,
      "referenceCount": 17,
      "citations": [
        {
          "paperId": "6b0ea6a6869b5f4207a6a8672ba153b747975126",
          "title": "TrajFormer: Efficient Trajectory Classification with Transformers"
        },
        {
          "paperId": "164dcacc20cf5c6285510c8b601d51169e4ebcf5",
          "title": "Continuous Pseudo-Labeling from the Start"
        },
        {
          "paperId": "0cfd0ce0c0ab737db8f717250bf0e62939dad444",
          "title": "Disentangling Past-Future Modeling in Sequential Recommendation via Dual Networks"
        },
        {
          "paperId": "c85ed937562106a6c9016dbb42e2b48cc298cadc",
          "title": "Efficient Learning with Pseudo Labels for Query Cost Estimation"
        },
        {
          "paperId": "0cd9fb453d71212f08937da7fe16b924f22bc536",
          "title": "Learning to Jointly Transcribe and Subtitle for End-to-End Spontaneous Speech Recognition"
        },
        {
          "paperId": "44fc5bc7cd0957e00159d588e433acfc3cbe7dbe",
          "title": "Why self-attention is Natural for Sequence-to-Sequence Problems? A Perspective from Symmetries"
        },
        {
          "paperId": "b49ebf36a29cf9734313066129ab0d7092d4041e",
          "title": "Categorizing Semantic Representations for Neural Machine Translation"
        },
        {
          "paperId": "927d707786ac1e19cb421aed6f5b0603b9dadc88",
          "title": "Robustify Transformers with Robust Kernel Density Estimation"
        },
        {
          "paperId": "2919cfcfc4e48fe0a17ac18ef41949bdb1d50891",
          "title": "Relational Attention: Generalizing Transformers for Graph-Structured Tasks"
        },
        {
          "paperId": "7aa43fc01045d36c2babb3edd81b0147dfa8cb45",
          "title": "Towards All Weather and Unobstructed Multi-Spectral Image Stitching: Algorithm and Benchmark"
        },
        {
          "paperId": "5cdb940cb0e8158bc5bc5aabcee84ffeee0c30fe",
          "title": "Improve Transformer Pre-Training with Decoupled Directional Relative Position Encoding and Representation Differentiations"
        },
        {
          "paperId": "628fb0736c5484f7a0006d0bb2d4a7a2ef9ae6b3",
          "title": "SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training"
        },
        {
          "paperId": "e974cb853627fc0ebc79221f6983c98213c12d75",
          "title": "Latent Neural ODEs with Sparse Bayesian Multiple Shooting"
        },
        {
          "paperId": "43a37611194f9019dde2d1ef9c86b61e7dc555bb",
          "title": "A Reverse Positional Encoding Multi-Head Attention-Based Neural Machine Translation Model for Arabic Dialects"
        },
        {
          "paperId": "03b7d2e942eafabd14b229f8962fa6e943053f75",
          "title": "Melody Infilling with User-Provided Structural Context"
        },
        {
          "paperId": "61b9b5187e8c5ca0632d16d7c916a42784eca348",
          "title": "Point Cloud Recognition with Position-to-Structure Attention Transformers"
        },
        {
          "paperId": "1b21145563d483bfa68f433fe4b425cf198c34e7",
          "title": "MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models"
        },
        {
          "paperId": "4e1489d54bf802e588e08975a086108e3b6bf169",
          "title": "Enhancing 3D-2D Representations for Convolution Occupancy Networks"
        },
        {
          "paperId": "343e4883a3aa8ab7a1b5950f8f8fb8091a2a76be",
          "title": "The encoding method of position embeddings in vision transformer"
        },
        {
          "paperId": "fb5ed9d348ca6be8544f4bc10d19da2a7046fb35",
          "title": "Extended context-based semantic communication system for text transmission"
        },
        {
          "paperId": "5a47f886781f8a76ced0c97af27f79a1575da72d",
          "title": "E-Branchformer: Branchformer with Enhanced merging for speech recognition"
        },
        {
          "paperId": "5ee2254615d53657ba688488a299d7325871515d",
          "title": "Protein structure generation via folding diffusion"
        },
        {
          "paperId": "9ae9e58cdd5975be4af3fac9a9e948adaf4bc4da",
          "title": "Verifiable and Energy Efficient Medical Image Analysis with Quantised Self-attentive Deep Neural Networks"
        },
        {
          "paperId": "e9fc0e0f3b61538b3739e4966ad819b38e6632d8",
          "title": "SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data"
        },
        {
          "paperId": "61d8390fffca540517e325dbfc1df27a1ecfbfe1",
          "title": "Thermal Infrared Tracking Method Based on Efficient Global Information Perception"
        },
        {
          "paperId": "c01e694ea2332b1ae90f96d68ac2b499598963ee",
          "title": "Taking a Respite from Representation Learning for Molecular Property Prediction"
        },
        {
          "paperId": "1fa42616e81c6efec07604d6bbf92214e388e9c5",
          "title": "S^2-Transformer for Mask-Aware Hyperspectral Image Reconstruction"
        },
        {
          "paperId": "e901a9748feccb2eef7d9ff8959b060b94cb7d3a",
          "title": "Beyond the Transformer: A Novel Polynomial Inherent Attention (PIA) Model and Its Great Impact on Neural Machine Translation"
        },
        {
          "paperId": "36480a6136805d9f6a97d3eb8aefba316b87e080",
          "title": "ESPnet-ONNX: Bridging a Gap Between Research and Production"
        },
        {
          "paperId": "87297d4ae3aa78547c07b69ece13212338bffd41",
          "title": "Dynamic Graph Message Passing Networks for Visual Recognition"
        },
        {
          "paperId": "c3e1bc1b9fb2d92c3b76b22ff666af42a75491f1",
          "title": "WA-Transformer: Window Attention-based Transformer with Two-stage Strategy for Multi-task Audio Source Separation"
        },
        {
          "paperId": "d4d8db8bf134107e93b2d089a9d022e05066ce0c",
          "title": "Self-Attention Based Time-Rating-Aware Context Recommender System"
        },
        {
          "paperId": "79b7f6965242af82405ad47647450f6684c0d121",
          "title": "Parameter-Efficient Conformers via Sharing Sparsely-Gated Experts for End-to-End Speech Recognition"
        },
        {
          "paperId": "54e511310d40ae2ebb4448f92f27865df1b7bae1",
          "title": "LogGD: Detecting Anomalies from System Logs by Graph Neural Networks"
        },
        {
          "paperId": "fd17b2b6fdd9a6d67c8534bf0a2507c99242f62d",
          "title": "Graph-to-Text Generation with Dynamic Structure Pruning"
        },
        {
          "paperId": "c2737d1165399f3826d14ab10e40d23aaee73edc",
          "title": "Beat Transformer: Demixed Beat and Downbeat Tracking with Dilated Self-Attention"
        },
        {
          "paperId": "9acc94aed6b6a6bf4ec5f4579999f175d40e3089",
          "title": "Domain Adversarial Training on Conditional Variational Auto-Encoder for Controllable Music Generation"
        },
        {
          "paperId": "7d51e0d810d7b4567c0f377860428b06ab3a6a14",
          "title": "CAT-CPI: Combining CNN and transformer to learn compound image features for predicting compound-protein interactions"
        },
        {
          "paperId": "e420180e2b6da334ef2accf90b08087862cbff85",
          "title": "TRQ3DNet: A 3D Quasi-Recurrent and Transformer Based Network for Hyperspectral Image Denoising"
        },
        {
          "paperId": "387f9bd2412c2b1154aa6c9a2156e1162cc5f2c9",
          "title": "Semantic Representation Using Sub-Symbolic Knowledge in Commonsense Reasoning"
        },
        {
          "paperId": "4f84e798067ff8bc86a21dac6134f15a13bcc30e",
          "title": "SUN: Exploring Intrinsic Uncertainties in Text-to-SQL Parsers"
        },
        {
          "paperId": "e3a479c6d460bc8576a57011dae9b3d894eb75cc",
          "title": "Local-Aware Global Attention Network for Person Re-Identification"
        },
        {
          "paperId": "e7c835eeb56f28e043dbe5af866cd1e1f9b31b12",
          "title": "An End-to-End Mutually Interactive Emotion–Cause Pair Extractor via Soft Sharing"
        },
        {
          "paperId": "3ea15bb1e2956c54b141b07e0cda933d3c94ded2",
          "title": "DSE-GAN: Dynamic Semantic Evolution Generative Adversarial Network for Text-to-Image Generation"
        },
        {
          "paperId": "5ce9641a5caf7fa5377edb68b449185a281eddce",
          "title": "Structural Bias for Aspect Sentiment Triplet Extraction"
        },
        {
          "paperId": "35466623d3f0a8426e13d349d8260bfc83948d75",
          "title": "A generalized-template-based graph neural network for accurate organic reactivity prediction"
        },
        {
          "paperId": "e05aca5f3d24f250596fb7a7d8cc008f0478dfaf",
          "title": "ELMformer: Efficient Raw Image Restoration with a Locally Multiplicative Transformer"
        },
        {
          "paperId": "cdc027a961da229e870be3ac41a429f4c165788d",
          "title": "Modeling Spatial Trajectories using Coarse-Grained Smartphone Logs"
        },
        {
          "paperId": "1aac692ca061feb846cf32cd61a1d422f89593a1",
          "title": "A Survey on Text-to-SQL Parsing: Concepts, Methods, and Future Directions"
        },
        {
          "paperId": "c8483055b1dbef7e23bf31f544fde4a2b1de417e",
          "title": "Time-aware Self-Attention Meets Logic Reasoning in Recommender Systems"
        },
        {
          "paperId": "2a2fe9e852941b28454719257330d08bb252fe32",
          "title": "Progressive Transformer Machine for Natural Character Reenactment"
        },
        {
          "paperId": "623fe930267bdcbc50637298ea6a72969edd05a9",
          "title": "K-Order Graph-oriented Transformer with GraAttention for 3D Pose and Shape Estimation"
        },
        {
          "paperId": "9034123d8d5e2625e080589ec8223debf80dc7d1",
          "title": "Flat Multi-modal Interaction Transformer for Named Entity Recognition"
        },
        {
          "paperId": "39b1b104ff08aaea959a9a58dc8b4d158b4e9ab4",
          "title": "Efficient Attention-free Video Shift Transformers"
        },
        {
          "paperId": "1f621d02b124f0e2d8aa2d71c831deb5ea0ee834",
          "title": "PointTransformer: Encoding Human Local Features for Small Target Detection"
        },
        {
          "paperId": "2482e19a2e77d106deb7550144ff9d2cc910d5e2",
          "title": "Accelerating Vision Transformer Training via a Patch Sampling Schedule"
        },
        {
          "paperId": "5c092c7bb0f9deecebb365dc79e52cb1e24614cc",
          "title": "Position-aware Structure Learning for Graph Topology-imbalance by Relieving Under-reaching and Over-squashing"
        },
        {
          "paperId": "2940f734dc3880545a82c5f1c354c20fcd2c7bcd",
          "title": "TaxoTrans: Taxonomy-Guided Entity Translation"
        },
        {
          "paperId": "39ae1805a8d67bc833a104ba83edf7894ef968d8",
          "title": "Non-stationary Time-aware Kernelized Attention for Temporal Event Prediction"
        },
        {
          "paperId": "0f40c0957494c8d4bfdd6a9e2231a554b617fe28",
          "title": "Learning to Rotate: Quaternion Transformer for Complicated Periodical Time Series Forecasting"
        },
        {
          "paperId": "42bc0824d8ca35105d181aaa0183654535325f55",
          "title": "Structural Biases for Improving Transformers on Translation into Morphologically Rich Languages"
        },
        {
          "paperId": "de016a6cd875fb79084b68424031cd5848d01726",
          "title": "Txt2Img-MHN: Remote Sensing Image Generation from Text Using Modern Hopfield Networks"
        },
        {
          "paperId": "ba811d80e9f4173c19d1308da055750068a472b7",
          "title": "Dynamic Adaptive and Adversarial Graph Convolutional Network for Traffic Forecasting"
        },
        {
          "paperId": "0596cf28ae2fb579e9988697470c00b9ac0ca31b",
          "title": "PointConvFormer: Revenge of the Point-based Convolution"
        },
        {
          "paperId": "a60cb9c5ca4c4b437d4087f549ef12be12d4bdbd",
          "title": "Frequency Stability Prediction of Power Systems Using Vision Transformer and Copula Entropy"
        },
        {
          "paperId": "976bb85ba9e426f8faf0c85b37c96e2a9f95cc0d",
          "title": "Time Series Forecasting of Motor Bearing Vibration Based on Informer"
        },
        {
          "paperId": "076deb54a5d776cd21eabf2c40cdd839f53d6d77",
          "title": "giMLPs: Gate with Inhibition Mechanism in MLPs"
        },
        {
          "paperId": "6e4b798b39b42a6ecb738bf8e530f002064c52e8",
          "title": "Integration of Multiple Time Embedding and GLU for Sequential Recommendation"
        },
        {
          "paperId": "a6d0b96bdbb2267867826a15830c8a45faf852d6",
          "title": "Global-Local Self-Distillation for Visual Representation Learning"
        },
        {
          "paperId": "ec4c8d99eb1c028c43af6d8bbf727392d351cb59",
          "title": "Efficient Training of Language Models to Fill in the Middle"
        },
        {
          "paperId": "4f451ba06c4c9effd6c4ac0bae222495501a6200",
          "title": "Innovations in Neural Data-to-text Generation"
        },
        {
          "paperId": "437815d4f186b0a96f5464a6672c4b07eb8f18bf",
          "title": "Learning a Dual-Mode Speech Recognition Model via Self-Pruning"
        },
        {
          "paperId": "e9d1a851d4f7950db360d12fe7f96647aaf547da",
          "title": "Generalized Attention Mechanism and Relative Position for Transformer"
        },
        {
          "paperId": "3b5a08ab2faa5b1826c66e25b5efe167940c5356",
          "title": "Molormer: a lightweight self-attention-based method focused on spatial structure of molecular graph for drug-drug interactions prediction"
        },
        {
          "paperId": "52147112360c0cb80adc0a491ff8d2de30a754fc",
          "title": "Mobi-Trans: A Hybrid Network with Attention Mechanism for Myocardial Infarction Localization"
        },
        {
          "paperId": "9a5e384e032337abbbedae06fb0867eb725ee6b7",
          "title": "DGR: Decomposition Graph Reconstruction for Question Understanding"
        },
        {
          "paperId": "4299353235595391e2b4f7298baffd00b5acf9d1",
          "title": "LordBERT: Embedding Long Text by Segment Ordering with BERT"
        },
        {
          "paperId": "0a4d0f5a69336aee8e85a974dfdecdc9aa518ee2",
          "title": "CMSBERT-CLR: Context-driven Modality Shifting BERT with Contrastive Learning for linguistic, visual, acoustic Representations"
        },
        {
          "paperId": "b09336c21895c80966684f87c165c543ec4143a3",
          "title": "Multi-head Cascaded Swin Transformers with Attention to k-space Sampling Pattern for Accelerated MRI Reconstruction"
        },
        {
          "paperId": "2797abe9e6a59a3fc5554f1dc49e3c9c0009e553",
          "title": "Decision Making for Autonomous Driving Via Multimodal Transformer and Deep Reinforcement Learning*"
        },
        {
          "paperId": "a7b9d34bf98a56f11a6d6513a7535f0b42784c22",
          "title": "Boosting the Transformer with the BERT Supervision in Low-Resource Machine Translation"
        },
        {
          "paperId": "b9607608ed65a53326d64f773055f0b7e6ac4f88",
          "title": "CharFormer: A Glyph Fusion based Attentive Framework for High-precision Character Image Denoising"
        },
        {
          "paperId": "c27c68597065dad70cb0b4f882cd861826bf28fa",
          "title": "Automated Construction Contract Summarization Using Natural Language Processing and Deep Learning"
        },
        {
          "paperId": "4d80dae4039b4de6c1ba30227d5981fd1cbb4a1f",
          "title": "Parameterization of Cross-token Relations with Relative Positional Encoding for Vision MLP"
        },
        {
          "paperId": "eabfa0f53056ba243c6c426c7bc7f060e9ae0e13",
          "title": "Boosting Multi-Modal E-commerce Attribute Value Extraction via Unified Learning Scheme and Dynamic Range Minimization"
        },
        {
          "paperId": "c6bf37c47eabc93b552130bd052fe8d3d90c8a27",
          "title": "Position Prediction as an Effective Pretraining Strategy"
        },
        {
          "paperId": "eedf999f58b2fd888feb210e4ea5c5a6ce681880",
          "title": "Transformer-Based Global Zenith Tropospheric Delay Forecasting Model"
        },
        {
          "paperId": "9692886ba8e2c9d8990b0505e9c67a696d9f28a7",
          "title": "A Closer Look into Transformer-Based Code Intelligence Through Code Transformation: Challenges and Opportunities"
        },
        {
          "paperId": "5deef7fc161cbdc884aff15b9810f8a432c1489a",
          "title": "k-means Mask Transformer"
        },
        {
          "paperId": "e28adeb4db46469df9f9bd653501871ddc5f4318",
          "title": "MuchSUM: Multi-channel Graph Neural Network for Extractive Summarization"
        },
        {
          "paperId": "0b7b72e5325416250e5dd224395f7b78aedac634",
          "title": "Translation-Based Implicit Annotation Projection for Zero-Shot Cross-Lingual Event Argument Extraction"
        },
        {
          "paperId": "8bd623131b4a849a77e92aca6d9c113912b45646",
          "title": "Aspect-Based Sentiment Analysis using Local Context Focus Mechanism with DeBERTa"
        },
        {
          "paperId": "0b97eef60bccf79e76b7c50d5f11e44638b58bd1",
          "title": "Distance Matters in Human-Object Interaction Detection"
        },
        {
          "paperId": "1ed91a88cfb55aa8e1b2a2145c7fff7c644bb284",
          "title": "Syntax Controlled Knowledge Graph-to-Text Generation with Order and Semantic Consistency"
        },
        {
          "paperId": "f16656c5a8702523c3340938a7040817736b83f2",
          "title": "Unaligned Multimodal Sequences for Depression Assessment From Speech"
        },
        {
          "paperId": "3ee323d224391b610b18985237a633f64f55566a",
          "title": "Relational local electroencephalography representations for sleep scoring"
        },
        {
          "paperId": "1c3229a0a589467560f808b0234744181f051ea5",
          "title": "Table2Graph: Transforming Tabular Data to Unified Weighted Graph"
        },
        {
          "paperId": "5046d276dbaa4c5245e8427791fe7f30b26eef8e",
          "title": "Can we learn from developer mistakes? Learning to localize and repair real bugs from real bug fixes"
        },
        {
          "paperId": "ffd2a9c326b68e3feed96855367647eb32f7b1da",
          "title": "Guiding transformer to generate graph structure for AMR parsing"
        },
        {
          "paperId": "bccfcca7b28c4792bd59659c56e1d7843542dfe9",
          "title": "Attention Biasing and Context Augmentation for Zero-Shot Control of Encoder-Decoder Transformers for Natural Language Generation"
        },
        {
          "paperId": "1c85c2a214b3e6a64c121e1bee8c5ca46005c6ab",
          "title": "Capture Salient Historical Information: A Fast and Accurate Non-Autoregressive Model for Multi-turn Spoken Language Understanding"
        },
        {
          "paperId": "3ac674d9ea88fe1fb9dcd4031e4f50c3ecaa3ab9",
          "title": "SANE-TTS: Stable And Natural End-to-End Multilingual Text-to-Speech"
        },
        {
          "paperId": "2e6028f8b156c8b344e1a68d15d88403a978c71d",
          "title": "Learning Multiscale Transformer Models for Sequence Generation"
        },
        {
          "paperId": "1404b42b66d6f13af3aadf3d253ac531d260f38d",
          "title": "VReBERT: A Simple and Flexible Transformer for Visual Relationship Detection"
        },
        {
          "paperId": "0fda2f1ada85dda45cbb3aa926620bbbd9c476b9",
          "title": "SP-ViT: Learning 2D Spatial Priors for Vision Transformers"
        },
        {
          "paperId": "07e8fd26db7949b3d0b018f85ac360fec1e73492",
          "title": "A Survey : Neural Networks for AMR-to-Text"
        },
        {
          "paperId": "6fb05e3483d05f79e59e5ce957708d8ab8932fdc",
          "title": "Peripheral Vision Transformer"
        },
        {
          "paperId": "2fac281ccb4f52fe578e548aa62a75bf9e7fd2ad",
          "title": "Unsupervised method for video action segmentation through spatio-temporal and positional-encoded embeddings"
        },
        {
          "paperId": "9b1b92ee4f029a0fdb18c9effbc78fdb8b5f38e6",
          "title": "MetaTPTrans: A Meta Learning Approach for Multilingual Code Representation Learning"
        },
        {
          "paperId": "4c4347e19392ce37ce2161d8bad1a6823c3b054b",
          "title": "The YiTrans End-to-End Speech Translation System for IWSLT 2022 Offline Shared Task"
        },
        {
          "paperId": "014b2480e7f9487d571e9f5f56a6d6b27d1d706e",
          "title": "Cortical activity during naturalistic music listening reflects short-range predictions based on long-term experience"
        },
        {
          "paperId": "44d602e60ea9b228857ee13db971d1a4308cbeca",
          "title": "Position Labels for Self-Supervised Vision Transformer"
        },
        {
          "paperId": "eef43c8d4205d93258d9976379cffd768211506b",
          "title": "Revisiting End-to-End Speech-to-Text Translation From Scratch"
        },
        {
          "paperId": "728fff6344f9ce65fafcbf3b9aea4f0eb908d44d",
          "title": "How to Dissect a Muppet: The Structure of Transformer Embedding Spaces"
        },
        {
          "paperId": "2bc420d0f377c48fdaa7715715c655af4f43f526",
          "title": "Dict-TTS: Learning to Pronounce with Prior Dictionary Knowledge for Text-to-Speech"
        },
        {
          "paperId": "35941b8adb6cc3fec66292d355c504463a429e74",
          "title": "EAANet: Efficient Attention Augmented Convolutional Networks"
        },
        {
          "paperId": "ca285c906eac9b1b31483d2bc3cddc52c90ac564",
          "title": "KPGT: Knowledge-Guided Pre-training of Graph Transformer for Molecular Property Prediction"
        },
        {
          "paperId": "31a9744bd5421b3fbbad2ab38ce33bb2f352c77a",
          "title": "CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation"
        },
        {
          "paperId": "f5d292b97c0af02506c60c6615e1b58b0f4f421a",
          "title": "Exploring Structure-aware Transformer over Interaction Proposals for Human-Object Interaction Detection"
        },
        {
          "paperId": "45bf4b6468a3fa643bc1e2f9b7cf049d3d45c2fc",
          "title": "Graph Neural Networks with Precomputed Node Features"
        },
        {
          "paperId": "0dd163f1c10480128eeb25d77afec493c1c695eb",
          "title": "Transformer with Fourier Integral Attentions"
        },
        {
          "paperId": "4c5aa3e450b7c626beb7a27f65f1a238b13e9606",
          "title": "Voice activity detection using a local-global attention model"
        },
        {
          "paperId": "a1700c57759fe6482e972c30076d04529a08bf2d",
          "title": "Preparing an endangered language for the digital age: The Case of Judeo-Spanish"
        },
        {
          "paperId": "66b7836001407120ad0000369af8b30c44788a33",
          "title": "Transformer with Tree-order Encoding for Neural Program Generation"
        },
        {
          "paperId": "8e6ada0242dc9b3680ccf6ddeec9e7c3cd41766f",
          "title": "Dynamically Relative Position Encoding-Based Transformer for Automatic Code Edit"
        },
        {
          "paperId": "a3a125d28fed30b72e676914aa810b70644e98a1",
          "title": "Your Transformer May Not be as Powerful as You Expect"
        },
        {
          "paperId": "3884307cb95329275755baaf99600e7431be695d",
          "title": "Efficient U-Transformer with Boundary-Aware Loss for Action Segmentation"
        },
        {
          "paperId": "5fc1a0a05edaca418c27ef11d95a01bf6637dac4",
          "title": "Breaking the Chain of Gradient Leakage in Vision Transformers"
        },
        {
          "paperId": "6adf3504487b33cfa7fa6d1e0f1178c662ec835b",
          "title": "Flexible Diffusion Modeling of Long Videos"
        },
        {
          "paperId": "d6c5aab433d9871cabc01ffb1e5e1ea89141155b",
          "title": "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"
        },
        {
          "paperId": "3978fd4769d528b47d0233c2d86dcc8b54b9de3f",
          "title": "BabyNet: Residual Transformer Module for Birth Weight Prediction on Fetal Ultrasound Video"
        },
        {
          "paperId": "88b15679a7e60a99d5da1f1fde8aa332b368e1b8",
          "title": "Masked Autoencoders As Spatiotemporal Learners"
        },
        {
          "paperId": "6e98a35c439c5c7387de6bffd96dea9b2943a548",
          "title": "Trading Positional Complexity vs. Deepness in Coordinate Networks"
        },
        {
          "paperId": "38c8721a389324798b7d9628b80a43197d0cdc0b",
          "title": "AdaMCT: Adaptive Mixture of CNN-Transformer for Sequential Recommendation"
        },
        {
          "paperId": "77cb1ca1485f88f4061570dc999524da339863af",
          "title": "Hero-Gang Neural Model For Named Entity Recognition"
        },
        {
          "paperId": "14793aa93920cb8f748776cc45c3895de6df5fbf",
          "title": "RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL"
        },
        {
          "paperId": "137ec10dc0ca5f14deaa9c69ec6f6db3f63ac20d",
          "title": "Sentence model based subword embeddings for a dialog system"
        },
        {
          "paperId": "9d03a164759bb5cc2fa6b575254b58f790ab6785",
          "title": "Zero-shot Code-Mixed Offensive Span Identification through Rationale Extraction"
        },
        {
          "paperId": "7bd08a5d85260bb0a0a328cfe9a455c54c0793ad",
          "title": "CSI-based Indoor Localization via Attention-Augmented Residual Convolutional Neural Network"
        },
        {
          "paperId": "99eb7caba122464e0b977df8456887a02d2bc051",
          "title": "CSI-fingerprinting Indoor Localization via Attention-Augmented Residual Convolutional Neural Network"
        },
        {
          "paperId": "b4da9f3505e22d3e766ba21890285b822dc71599",
          "title": "EdgeViTs: Competing Light-weight CNNs on Mobile Devices with Vision Transformers"
        },
        {
          "paperId": "9b66c808f38f50d7f4e88e910caad015c23d118d",
          "title": "Capturing Time Dynamics From Speech Using Neural Networks for Surgical Mask Detection"
        },
        {
          "paperId": "a6dd1b1088a4ce34e0779cb0c6ae4547d0068b7e",
          "title": "Detecting Loaded Trajectories for Hazardous Chemicals Transportation"
        },
        {
          "paperId": "4990be2f9092f02df3560fca63d8aef26954bca6",
          "title": "AST-Trans: Code Summarization with Efficient Tree-Structured Attention"
        },
        {
          "paperId": "4d90e08472949c936d6f1c962f4d8b038327f76b",
          "title": "Recognition of Chinese Legal Elements Based on Transfer Learning and Semantic Relevance"
        },
        {
          "paperId": "4a850e47df73f75cc049baedaf988ce1a0cf3a35",
          "title": "DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation"
        },
        {
          "paperId": "b9a701c90f3d3df27366f5b29a97f798eb940ac7",
          "title": "ChapterBreak: A Challenge Dataset for Long-Range Language Models"
        },
        {
          "paperId": "0aac6adb54813cdf16b321dbee76dea82d7c1d09",
          "title": "Cross-stitched Multi-modal Encoders"
        },
        {
          "paperId": "2193d898e556ad2ac14f0ababf02f90e6fdfe663",
          "title": "DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks"
        },
        {
          "paperId": "c6dc862d4d105de77a39d30a7e79366ec891e486",
          "title": "Dynamic Position Encoding for Transformers"
        },
        {
          "paperId": "a99cbe3d7b2e201491a2fa1f4ba57b1e80cca3a3",
          "title": "LaMemo: Language Modeling with Look-Ahead Memory"
        },
        {
          "paperId": "2e03ee15c00561a891eaf197515da720d1566411",
          "title": "Causal Transformer for Estimating Counterfactual Outcomes"
        },
        {
          "paperId": "215fd67a51240194b816c695ac46e0d6d1fdbdc8",
          "title": "GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation"
        },
        {
          "paperId": "41481a39515fa843f7a582f6bb9b32a1c1f42c89",
          "title": "A collection of deep learning-based feature-free approaches for characterizing single-objective continuous fitness landscapes"
        },
        {
          "paperId": "fd472735d69fe44870fe6aad0133eb3388eed527",
          "title": "Accurate Portraits of Scientific Resources and Knowledge Service Components"
        },
        {
          "paperId": "01c93b745dac8d6d463fce3de4ac634c3048f065",
          "title": "TANet: Thread-Aware Pretraining for Abstractive Conversational Summarization"
        },
        {
          "paperId": "c04e3a0d4c3d5d734eadac8ca70dace565e34921",
          "title": "LSTM-RASA Based Agri Farm Assistant for Farmers"
        },
        {
          "paperId": "6a250b904965732840a75b6a13e35ac15f5cce4d",
          "title": "Compositional Generalization and Decomposition in Neural Program Synthesis"
        },
        {
          "paperId": "3b2a675bb617ae1a920e8e29d535cdf27826e999",
          "title": "Video Diffusion Models"
        },
        {
          "paperId": "fb3761d27765536b204191d2a8bca2898055cb95",
          "title": "Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection"
        },
        {
          "paperId": "1984b0f7605e2884616466c92126269ad3a1e426",
          "title": "A Complementary Joint Training Approach Using Unpaired Speech and Text for Low-Resource Automatic Speech Recognition"
        },
        {
          "paperId": "07b5755c833c240b1ee48629cf545442a16bd7d5",
          "title": "Micro-Behavior Encoding for Session-based Recommendation"
        },
        {
          "paperId": "2ad12a7be5eaf339a98c4defd8669e11fe726acc",
          "title": "MaxViT: Multi-Axis Vision Transformer"
        },
        {
          "paperId": "c99a0aac9a7166ff7659c7b0c6c4a65b3c5642b7",
          "title": "Pair-wise Aspect and Opinion Terms Extraction as Graph Parsing via a Novel Mutually-Aware Interaction Mechanism"
        },
        {
          "paperId": "652a892f36f576726a9fe6ea512e8cc540f5a144",
          "title": "ELECTRIcity: An Efficient Transformer for Non-Intrusive Load Monitoring"
        },
        {
          "paperId": "df1b3e55e8092a2aa42bb3eab8db778f2b94d66c",
          "title": "The auto segmentation for cardiac structures using a dual‐input deep learning network based on vision saliency and transformer"
        },
        {
          "paperId": "49995802ad0d6ef327647868868458d7619d430d",
          "title": "Pre-Training Transformer Decoder for End-to-End ASR Model with Unpaired Speech Data"
        },
        {
          "paperId": "043c1d66e77e24760f65b925b4ad48754bf87b76",
          "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information"
        },
        {
          "paperId": "35947a79360829331010011209927f8cb619f97a",
          "title": "VPTR: Efficient Transformers for Video Prediction"
        },
        {
          "paperId": "de4fa71618e56128286a7b6f8302fa78eb155fff",
          "title": "Seq-2-Seq based Refinement of ASR Output for Spoken Name Capture"
        },
        {
          "paperId": "2a04c7d083eec666b2a1edcade876d9be9dcdfe8",
          "title": "DPE-BoTNeT: Dual Position Encoding Bottleneck Transformer Network for Skin Lesion Classification"
        },
        {
          "paperId": "b90f8533cb6717121a741d60ac5f466afa71ee99",
          "title": "Coordinate Transformer Network for Prediction of Pseudomonas Aeruginosa’s Drug Resistance"
        },
        {
          "paperId": "336672adf959e5eb4a2a189c976a2847a68f85c5",
          "title": "ANNA”:\" Enhanced Language Representation for Question Answering"
        },
        {
          "paperId": "c52db31d0535ee38b7ca311350c6f490f0529924",
          "title": "Federated Learning with Position-Aware Neurons"
        },
        {
          "paperId": "cc1a8af59affa863aede0dcd601ccab920d99548",
          "title": "HELoC: Hierarchical Contrastive Learning of Source Code Representation"
        },
        {
          "paperId": "838a2297b94f7bad96c4f8370a5f58487f194f44",
          "title": "Visual Abductive Reasoning"
        },
        {
          "paperId": "0b568a599c7aac6dae52e5a7412ad5a7429ac344",
          "title": "Lane detection with position embedding"
        },
        {
          "paperId": "8e6a096447bd0af924cc5d853f37848bd9290524",
          "title": "Geometry-Aware Supertagging with Heterogeneous Dynamic Convolutions"
        },
        {
          "paperId": "fa118487bf333d20573a9dc10b5809d74b696e02",
          "title": "Speech Enhancement by Multiple Propagation through the Same Neural Network"
        },
        {
          "paperId": "9035098ce6131f63e892c77474c537161869a674",
          "title": "Scalable Multi-object Identification for Video Object Segmentation"
        },
        {
          "paperId": "15553a909ad77c2344e0e1c24e7f6a98ba293bd3",
          "title": "Associating Objects with Scalable Transformers for Video Object Segmentation"
        },
        {
          "paperId": "35c57673d8195035b86e6e7b8cd953dceeb6cfaa",
          "title": "Differentiable Duration Modeling for End-to-End Text-to-Speech"
        },
        {
          "paperId": "36d649496b91ce22f380502553a488ccaf3027dd",
          "title": "AAPFE: Aligned Assembly Pre-Training Function Embedding for Malware Analysis"
        },
        {
          "paperId": "2b7b7eb96fc30089d53e92ad97ac2db0035dc3cb",
          "title": "Generalized Classification of Satellite Image Time Series with Thermal Positional Encoding"
        },
        {
          "paperId": "04a061d959ff3a777c266c8975ca1e36f05528ac",
          "title": "ODE Transformer: An Ordinary Differential Equation-Inspired Model for Sequence Generation"
        },
        {
          "paperId": "e28ad66a8ee09278e00b8fa064b695eefca52d02",
          "title": "FormNet: Structural Encoding beyond Sequential Modeling in Form Document Information Extraction"
        },
        {
          "paperId": "d71915cf609dd0e785da481fcdab826c38611243",
          "title": "HIE-SQL: History Information Enhanced Network for Context-Dependent Text-to-SQL Semantic Parsing"
        },
        {
          "paperId": "acbe813244e07f32eb034d6c27547d772a995d1d",
          "title": "Uncertainty Estimation for Language Reward Models"
        },
        {
          "paperId": "a3c18ea5548acbc885836e8d0f202852db192268",
          "title": "S^2SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-SQL Parsers"
        },
        {
          "paperId": "93c1dffe2bae737da8f342fd749aa783df572a14",
          "title": "XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding"
        },
        {
          "paperId": "9f1b0e4c42a5a85d4c023030557ade4419f82ecf",
          "title": "Scaling Up Your Kernels to 31×31: Revisiting Large Kernel Design in CNNs"
        },
        {
          "paperId": "52c6e23efa0da8981f807b347b786e486d2b1c0b",
          "title": "Source-free Domain Adaptation for Multi-site and Lifespan Brain Skull Stripping"
        },
        {
          "paperId": "0239063c7a7782ead73fe72e4b2d21c3988be23d",
          "title": "Challenges of Neural Machine Translation for Short Texts"
        },
        {
          "paperId": "9aacdbc8b04fa63e6fe93f62a737a11c613f08fb",
          "title": "Recent Advances in Neural Text Generation: A Task-Agnostic Survey"
        },
        {
          "paperId": "63de5aacac3c29f7a3bb17ec65e50229a6a179ba",
          "title": "HDNet: High-resolution Dual-domain Learning for Spectral Compressive Imaging"
        },
        {
          "paperId": "4e9cd6be4a8fcad2ca562fcf41a1f882387a3167",
          "title": "LGT-Net: Indoor Panoramic Room Layout Estimation with Geometry-Aware Transformer Network"
        },
        {
          "paperId": "f8f3ba03d0350754fbb502192dc0a201a053024f",
          "title": "A Unified Query-based Paradigm for Point Cloud Understanding"
        },
        {
          "paperId": "83b455fd69b7cd1b9409c68fa262ae92335b8e61",
          "title": "KG-SQL: Hybrid Knowledge-Guided Semantic Understanding for Text-to-SQL"
        },
        {
          "paperId": "2e43501a14b831744999355c177321709659aff1",
          "title": "TableFormer: Robust Transformer Modeling for Table-Text Encoding"
        },
        {
          "paperId": "189e36ba5b50a854eb45f612bcad51a621d7a7f1",
          "title": "MS-Transformer: Introduce multiple structural priors into a unified transformer for encoding sentences"
        },
        {
          "paperId": "3921e7cd03c440c6d065a51b499c9f581d1d2f1f",
          "title": "A Survey of Automatic Source Code Summarization"
        },
        {
          "paperId": "7a9df88b2e10fd071d045cad1d4239116095ca99",
          "title": "Machine Reading Comprehension-Enabled Public Service Information System: A Large-Scale Dataset and Neural Network Models"
        },
        {
          "paperId": "50af83ea20201b51014358534650213e6133650c",
          "title": "FastRPB: a Scalable Relative Positional Encoding for Long Sequence Tasks"
        },
        {
          "paperId": "48df6b4c106bbb4735fcc9f1e2ac4e23bba10565",
          "title": "SFINet: Shuffle–and–Fusion Interaction Networks for Wind Power Forecasting"
        },
        {
          "paperId": "5f104a804ed245c79847ad0593e8f86196d697b1",
          "title": "Transformers in Time Series: A Survey"
        },
        {
          "paperId": "a996989466bb584602eb3027f308d78b2867893d",
          "title": "Source Code Summarization with Structural Relative Position Guided Transformer"
        },
        {
          "paperId": "2491a706049da76ab7fe17f74c6ea7f195502fdf",
          "title": "Multi-Resolution Attention for Personalized Item Search"
        },
        {
          "paperId": "10af057b73c206d5b999c200b4de4a7a67435dee",
          "title": "Personalized Long-distance Fuel-efficient Route Recommendation Through Historical Trajectories Mining"
        },
        {
          "paperId": "686e6558008c127849cb2f5197036db1591495df",
          "title": "Entroformer: A Transformer-based Entropy Model for Learned Image Compression"
        },
        {
          "paperId": "106ec729604861c1494f17fe7f277c130acef8dc",
          "title": "Improving the Sample-Complexity of Deep Classification Networks with Invariant Integration"
        },
        {
          "paperId": "ea0e4a9778e33b7f8e7b3246d63071330950995a",
          "title": "Structure-Aware Transformer for Graph Representation Learning"
        },
        {
          "paperId": "d0194169a4787cf2977e5574746f5466ed6b96cc",
          "title": "Curvature-Driven Deformable Convolutional Networks for End-To-End Object Detection"
        },
        {
          "paperId": "a81e30bbbf5422fbac75f6a017206ac97a508521",
          "title": "Efficient Self-attention with Relative Position Encoding for Electric Power Load Forecasting"
        },
        {
          "paperId": "4c75564731f564e78cafc76e18739bbcf4fceeb2",
          "title": "Knowledge Base Question Answering Based on Multi-head Attention Mechanism and Relative Position Coding"
        },
        {
          "paperId": "2b34ee1b7f4cf5e7dac38d8e66849a02d96fade3",
          "title": "Graph-based Neural Acceleration for Nonnegative Matrix Factorization"
        },
        {
          "paperId": "77365b30336ac46d620d958dc4c108a159c02834",
          "title": "WebFormer: The Web-page Transformer for Structure Information Extraction"
        },
        {
          "paperId": "b92898a28bfad42a053726c2707cc05686cd332a",
          "title": "GRPE: Relative Positional Encoding for Graph Transformer"
        },
        {
          "paperId": "218f550518a6961f6b136ce092d29cee88c63c07",
          "title": "Bellman Meets Hawkes: Model-Based Reinforcement Learning via Temporal Point Processes"
        },
        {
          "paperId": "83ebb3a6f8c81492456d024d5dfb9a9bc0221434",
          "title": "Generative Cooperative Networks for Natural Language Generation"
        },
        {
          "paperId": "076d06d66886dedf3cb6e5dc31393dac38fef300",
          "title": "Dual-Flattening Transformers through Decomposed Row and Column Queries for Semantic Segmentation"
        },
        {
          "paperId": "b3cd5f678394a37312b6a1af289de9731ae84210",
          "title": "MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term Video Recognition"
        },
        {
          "paperId": "a84d1dfb647297aa646930ad592ac2f440bd9ebb",
          "title": "Continual Transformers: Redundancy-Free Attention for Online Inference"
        },
        {
          "paperId": "93c53952714945b3900be66cc036acd4f6df2372",
          "title": "Video Transformers: A Survey"
        },
        {
          "paperId": "c5470d6ea548f084efcbeb57ab22e2522ff785a9",
          "title": "Reverse-Engineering Information Presentations: Recovering Hierarchical Grouping from Layouts of Visual Elements"
        },
        {
          "paperId": "49c3f85573a3204c5e66317289e4cecfed50f38a",
          "title": "Assemble Foundation Models for Automatic Code Summarization"
        },
        {
          "paperId": "47b97fb6398122ef41344b7e5301a2389b1a94bb",
          "title": "Feature Pyramid Multi-View Stereo Network Based on Self-Attention Mechanism"
        },
        {
          "paperId": "4f82bd927f6d79fd2e3ddf9d34bec0dc46b8e18c",
          "title": "Classification of Long Sequential Data using Circular Dilated Convolutional Neural Networks"
        },
        {
          "paperId": "063e410c2c52ccbaa22c62130ac2c58969bb3efa",
          "title": "SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations"
        },
        {
          "paperId": "ab00d5e3183cd8d49f62b1424c2503fd1e6edaee",
          "title": "PyramidTNT: Improved Transformer-in-Transformer Baselines with Pyramid Architecture"
        },
        {
          "paperId": "3d261c68c964fc2f9ba759e71fdaa9bf075d1c98",
          "title": "D-Former: A U-shaped Dilated Transformer for 3D Medical Image Segmentation"
        },
        {
          "paperId": "d3ff0a2e58e29d652fd15cd86f0a0f53df8e8af8",
          "title": "Transmorph: a transformer based morphological disambiguator for Turkish"
        },
        {
          "paperId": "59a8dbbbf0002f1f497fddc4d01e92c0b9645c10",
          "title": "Diformer: Directional Transformer for Neural Machine Translation"
        },
        {
          "paperId": "63b5ada6cbe9df123d18cd467b56fbcfbaf49acb",
          "title": "PointSCNet: Point Cloud Structure and Correlation Learning Based on Space-Filling Curve-Guided Sampling"
        },
        {
          "paperId": "9eb3726604cd436010c8f19089598b97fb1bf8a3",
          "title": "Improving Face-Based Age Estimation With Attention-Based Dynamic Patch Fusion"
        },
        {
          "paperId": "de309c9c35e06bb62f7d6ff482118bcd41453173",
          "title": "Masked Feature Prediction for Self-Supervised Visual Pre-Training"
        },
        {
          "paperId": "9137efc758f80dd22bb56f82cca5c94f78a5db3e",
          "title": "MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"
        },
        {
          "paperId": "2c11b720624dc669f78fd34557fe91b5b2097969",
          "title": "Deep attentive style transfer for images with wavelet decomposition"
        },
        {
          "paperId": "494c1f745dbb7625e86e9a222c480e40949b8dad",
          "title": "Zero-Shot Cross-Lingual Machine Reading Comprehension via Inter-Sentence Dependency Graph"
        },
        {
          "paperId": "e73d87e7c19457b2e79878638ea44ee66196743c",
          "title": "SANTM: Efficient Self-attention-driven Network for Text Matching"
        },
        {
          "paperId": "45f686be3b96302ede327645227134e1c304dbab",
          "title": "Attention Mechanisms in Computer Vision: A Survey"
        },
        {
          "paperId": "c1793d88db202bd416cab2776224f17e4da7457a",
          "title": "PESTO: Switching Point based Dynamic and Relative Positional Encoding for Code-Mixed Languages"
        },
        {
          "paperId": "23640b62ac800ecd1e6d57e5b765b45f21bfb6fb",
          "title": "Pseudo-Labeling for Massively Multilingual Speech Recognition"
        },
        {
          "paperId": "2f05f579f6c48f43e6ced36d96eff9190e57bd40",
          "title": "PatchFormer: An Efficient Point Transformer with Patch Attention"
        },
        {
          "paperId": "1df0f782b0d2264c455ba3a4332c26dbc014f2ee",
          "title": "Permutation invariant graph-to-sequence model for template-free retrosynthesis and reaction prediction"
        },
        {
          "paperId": "e528466e2aff981511d4ca6e063211297c0b4175",
          "title": "The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization"
        },
        {
          "paperId": "1067c44e473b6998f89e13f0d4c0de730def43f0",
          "title": "SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing"
        },
        {
          "paperId": "fa945b5f34f053020b0c87c7340bf3b1838bd598",
          "title": "Study of Positional Encoding Approaches for Audio Spectrogram Transformers"
        },
        {
          "paperId": "ab72bccf6f3981537389510ecc609109e79595c3",
          "title": "Disentangled Sequence to Sequence Learning for Compositional Generalization"
        },
        {
          "paperId": "12640af46eaf4c16125557b517a2d37fca70a82d",
          "title": "Ripple Attention for Visual Perception with Sub-quadratic Complexity"
        },
        {
          "paperId": "f837c066fcb77fc29ce64491ccabd4f54de85f58",
          "title": "Molformer: Motif-based Roto-Translation Invariant Transformer on 3D Heterogeneous Molecular Graphs"
        },
        {
          "paperId": "25dc7c6e808ffb097682941fe767d09709472480",
          "title": "Underwater image enhancement via LBP-based attention residual network"
        },
        {
          "paperId": "586cafd248d01c1a1c67a57b3cef9f807173110c",
          "title": "Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition"
        },
        {
          "paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd",
          "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"
        },
        {
          "paperId": "49e65b12d8d11f2ccb5ddd7be72a8f746b2d1bc2",
          "title": "Making Transformers Solve Compositional Tasks"
        },
        {
          "paperId": "a5c41f188b0eb0acb444cb4899bf6af378ee9ede",
          "title": "CrossFormer: A Versatile Vision Transformer Hinging on Cross-scale Attention"
        },
        {
          "paperId": "87e823d2cb58e741230c0fa3b83f3459c7e32241",
          "title": "PiSLTRc: Position-Informed Sign Language Transformer With Content-Aware Convolution"
        },
        {
          "paperId": "42ee424dd1aa07d8579b912bc8dc10719b47c643",
          "title": "From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers"
        },
        {
          "paperId": "0b036cd5dfc49d835d0c759c8ca31d89f2410e65",
          "title": "CMT: Convolutional Neural Networks Meet Vision Transformers"
        },
        {
          "paperId": "4cfaea33c6cd6e72d843e808cf7c69387e100e7b",
          "title": "Exploiting Positional Information for Session-Based Recommendation"
        },
        {
          "paperId": "800cfb3d23115cdcd4d114234b65bbdf2080f798",
          "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows"
        },
        {
          "paperId": "cf444d60da03f6a4dbabeb44186b80de1a717db1",
          "title": "Interpreting Depression From Question-Wise Long-Term Video Recording of SDS Evaluation"
        },
        {
          "paperId": "34ff62922161332cef417af9b470b77a7225a477",
          "title": "Exemplars-Guided Empathetic Response Generation Controlled by the Elements of Human Communication"
        },
        {
          "paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530",
          "title": "A Survey of Transformers"
        },
        {
          "paperId": "5ee4108f56e20398361d43420b5bee6223901d21",
          "title": "Image super-resolution via channel attention and spatial attention"
        },
        {
          "paperId": "2835951fabf12804e17d5a525b2be2bee70e7910",
          "title": "Uformer: A General U-Shaped Transformer for Image Restoration"
        },
        {
          "paperId": "bc528b7af23199c660481ee8d452b900705c3c8f",
          "title": "Complementary spatiotemporal network for video question answering"
        },
        {
          "paperId": "1079b5459291ec73fc0a2d0692d38eaa97cb41a6",
          "title": "StyTr2: Image Style Transfer with Transformers"
        },
        {
          "paperId": "b8cee43a51c44f8f4448e78e41ecf081987707cf",
          "title": "Towards Robust Vision Transformer"
        },
        {
          "paperId": "10d9c1f66742ac68ea8cd13f5c0a3968b3771c54",
          "title": "GCN-MIF: Graph Convolutional Network with Multi-Information Fusion for Low-dose CT Denoising (preprint)"
        },
        {
          "paperId": "b18261a2b718cf29a87cb0237592a4725d5e0a6f",
          "title": "AGMB-Transformer: Anatomy-Guided Multi-Branch Transformer Network for Automated Evaluation of Root Canal Therapy"
        },
        {
          "paperId": "a3c50f601a6b872751556a4324d29cd10db1704f",
          "title": "GasHis-Transformer: A multi-scale visual transformer approach for gastric histopathological image detection"
        },
        {
          "paperId": "32e6c2b2dda7b92d2425998e6e71a57c9bbaabec",
          "title": "MVS2D: Efficient Multiview Stereo via Attention-Driven 2D Convolutions"
        },
        {
          "paperId": "286d2e0f3d882a37f486623c716d8a54a4a58fdc",
          "title": "Dynamic Graph Neural Networks for Sequential Recommendation"
        },
        {
          "paperId": "dcd0cb3efe3a21c9b5240a5c4e1f1fdd98200b7c",
          "title": "Context-Self Contrastive Pretraining for Crop Type Semantic Segmentation"
        },
        {
          "paperId": "44930df2a3186edb58c4d6f6e5ed828c5d6a0089",
          "title": "Attention, please! A survey of Neural Attention Models in Deep Learning"
        },
        {
          "paperId": "cfb5eb84d6956de1207c18bf27137affbafb6141",
          "title": "Question-relationship guided graph attention network for visual question answer"
        },
        {
          "paperId": "41bc8ad57ae622a88de5611c4933a77a5cc5cdcc",
          "title": "Symbolic Integration by Integrating Learning Models with Different Strengths and Weaknesses"
        },
        {
          "paperId": "20251a6c246d203d663a3c4253f8fc32ccb42ed6",
          "title": "Compute and Memory Efficient Universal Sound Source Separation"
        },
        {
          "paperId": "7072db6eddb85ecd2c117365d91bd694760f726e",
          "title": "Position Information in Transformers: An Overview"
        },
        {
          "paperId": "3a906b77fa218adc171fecb28bb81c24c14dcc7b",
          "title": "Transformers in Vision: A Survey"
        },
        {
          "paperId": "93780d6c0e0d537bca3f24245618033ecb7ff4e3",
          "title": "A Survey on Vision Transformer"
        },
        {
          "paperId": "0aa2a93e8c8cdae408c3d76149ab758ad4075ded",
          "title": "Predicting Decisions in Language Based Persuasion Games"
        },
        {
          "paperId": "1dda7bbd03435ab1b794516785c4363f4dc95c62",
          "title": "Forecasting CPI inflation components with Hierarchical Recurrent Neural Networks"
        },
        {
          "paperId": "cc80244d69c2c5bb7fe79fc6ca2561f2bed830ca",
          "title": "Learning to Represent Programs with Heterogeneous Graphs"
        },
        {
          "paperId": "659597b1699ba5b73da9a8628bf7e4ad9bebd242",
          "title": "Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting"
        },
        {
          "paperId": "0f70e4983731d91675ddfa98ba0146d1f74bbc8e",
          "title": "SFINet: Shufﬂe–and–Fusion Interaction Networks for Wind Power Forecasting"
        },
        {
          "paperId": "74172ff20bf49b6a23bbfa9093253b9f9a50aaee",
          "title": "MMT4: Multi Modality To Text Transfer Transformer"
        },
        {
          "paperId": "25ed2c2bbfbd66c635ce027d57e900b7c60dfda7",
          "title": "A C OLLECTION OF D EEP L EARNING - BASED F EATURE -F REE A PPROACHES FOR C HARACTERIZING S INGLE -O BJECTIVE C ONTINUOUS F ITNESS L ANDSCAPES ∗"
        },
        {
          "paperId": "4f79726768fb9e7cd7fbd366a2777ea70bdd24c3",
          "title": "A Transition-based Method for Complex Question Understanding"
        },
        {
          "paperId": "cbf284fe85795eaeb94bfb3dc9e98276dcd33788",
          "title": "Neural Architecture Search for Transformers: A Survey"
        },
        {
          "paperId": "719febbe16f84c0b0865e37d8efbb5674da4cf05",
          "title": "Relation-Aware Attentive Neural Processes Model for Remaining Useful Life Prediction"
        },
        {
          "paperId": "363b476587afe486c0301ec1034e04a210fac552",
          "title": "Improved Transformer With Multi-Head Dense Collaboration"
        },
        {
          "paperId": "6ad4c07c485be9b2bcb4419aa2cf429743de6271",
          "title": "3M-CDNet-V2: An Efficient Medium-Weight Neural Network for Remote Sensing Image Change Detection"
        },
        {
          "paperId": "08013183713f427258d36332b1bee9d6ce37cf30",
          "title": "S TRUCTURE -A WARE T RANSFORMER P OLICY FOR I NHOMOGENEOUS M ULTI -T ASK R EINFORCEMENT L EARNING"
        },
        {
          "paperId": "6379c96627facae43b0b8d750a454509becf6b17",
          "title": "A Survey on Transformers for Point Cloud Processing: An Updated Overview"
        },
        {
          "paperId": "cc74ae8c906dbf404058223e304bfd70d46fbbaf",
          "title": "ur-iw-hnt at CheckThat!-2022: Cross-lingual Text Summarization for Fake News Detection"
        },
        {
          "paperId": "fc801b53900c591d32f8afedef79d8c4772b129d",
          "title": "Adding Context into Convolutional Neural Networks"
        },
        {
          "paperId": "2a0b08bc566212d864b250b5a8c0bb8eb7d61689",
          "title": "A. Ablations on Video Classification"
        },
        {
          "paperId": "d32a7a831c82892a276fca13ee5c5c53a0af5b10",
          "title": "Local Transformer Linear Linear Linear ��� Scaled dot-product attention"
        },
        {
          "paperId": "9c09f22107109b3bf328bacd490f8e2c27b1b4bf",
          "title": "Dependency Position Encoding for Relation Extraction"
        },
        {
          "paperId": "9fe29c834afbe1848d9df713ae6e0ca3bd053605",
          "title": "Probing the Role of Positional Information in Vision-Language Models"
        },
        {
          "paperId": "ebf35e312cd994bcc6654b2a5afae43ac64f5ef4",
          "title": "Recurrence and Self-attention vs the Transformer for Time-Series Classification: A Comparative Study"
        },
        {
          "paperId": "d595e49ed44e28599b82c7a99ecf6794f50d41ce",
          "title": "Lightweight Transformers for Conversational AI"
        },
        {
          "paperId": "f248c9811e2c8755438b3069c6fa7eb3f4746a97",
          "title": "Intent Discovery for Enterprise Virtual Assistants: Applications of Utterance Embedding and Clustering to Intent Mining"
        },
        {
          "paperId": "b6e80e715784aa7cdddf519ef91b130a17aa958f",
          "title": "SADLN: Self-Attention Based Deep Learning Network of Integrating Multi-omics Data for Cancer Subtype Recognition"
        },
        {
          "paperId": "c9a9441a68c0787da5d987d485531dd4af6d1a97",
          "title": "TransNet: Shift Invariant Transformer Network for Side Channel Analysis (extended version)"
        },
        {
          "paperId": "e2a4e0a60a36dad93b8f6157426fc8f59657cc04",
          "title": "Deps-SAN: Towards Better Syntax-Aware NMT via Dependency-Scaled Self-Attention Network"
        },
        {
          "paperId": "b8a919f4a2aaa97bef19aa43e01f8bc347693b73",
          "title": "NASV I T: N EURAL A RCHITECTURE S EARCH FOR E F FICIENT V ISION T RANSFORMERS WITH G RADIENT C ONFLICT - AWARE S UPERNET T RAINING"
        },
        {
          "paperId": "619d53bcebcf72d77387bfef56be7d628e438c26",
          "title": "The YiTrans Speech Translation System for IWSLT 2022 Offline Shared Task"
        },
        {
          "paperId": "c37cc9413abebda039379a6d2460d69580da6100",
          "title": "What Works and Doesn’t Work, A Deep Decoder for Neural Machine Translation"
        },
        {
          "paperId": "4594505cddf004eff3a9192dc8941483826301dc",
          "title": "Unsupervised Dependency Graph Network"
        },
        {
          "paperId": "7e8b4cfdc03b59ece2d6b33a217f0abd47f708d9",
          "title": "Variational Graph Autoencoding as Cheap Supervision for AMR Coreference Resolution"
        },
        {
          "paperId": "9a2ca811882ed7513f83014b9de4fb3b4ab218c4",
          "title": "DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"
        },
        {
          "paperId": "06539d3bd22a3de187acbdda5705e13a143b4f13",
          "title": "TADSAM:A Time-Aware Dynamic Self-Attention Model for Next Point-of-Interest Recommendation"
        },
        {
          "paperId": "b70363498cb912b31c4647f92e848047f01c2754",
          "title": "Changing the Narrative Perspective: From Ranking to Prompt-Based Generation of Entity Mentions"
        },
        {
          "paperId": "cbad1c5fdc942bb92b090741ba975a0fec91f97e",
          "title": "QAN-et al.: Exploring Extensions on QANet"
        },
        {
          "paperId": "976d46a8be48b81e88209294f491b2f7478d971f",
          "title": "PortaSpeech: Portable and High-Quality Generative Text-to-Speech"
        },
        {
          "paperId": "3054a690aafdc770a2441003150b4222350900e6",
          "title": "Advancing Packet-Level Traffic Predictions with Transformers"
        },
        {
          "paperId": "b89f6d3087fd1bd6836d5c92ef3134f41854a3a6",
          "title": "An Object Detection Algorithm for Rotary-Wing UAV Based on AWin Transformer"
        },
        {
          "paperId": "1907ac07a22ef5c78231227eeb68e040a2f92b9b",
          "title": "Multi-criteria assessment of user trust in Social Reviewing Systems with subjective logic fusion"
        },
        {
          "paperId": "c6eef27b7ecefe7b8efe15ba228e70137761da10",
          "title": "SinTra: Learning an inspiration model from a single multi-track music segment"
        },
        {
          "paperId": "c817dbd4aa035b3e3d8d244f5e07c319f0e6fb06",
          "title": "Position-Enhanced Multi-Head Self-Attention Based Bidirectional Gated Recurrent Unit for Aspect-Level Sentiment Classification"
        },
        {
          "paperId": "c59a685133c8b0969fb8c5490e3c741b6e89541f",
          "title": "Semi-MsST-GAN: A Semi-Supervised Segmentation Method for Corneal Ulcer Segmentation in Slit-Lamp Images"
        },
        {
          "paperId": "ca1d71b52b3b81a7e1cca297f19d47f353cf3340",
          "title": "Relation-Aware Graph Transformer for SQL-to-Text Generation"
        },
        {
          "paperId": "6449f3ea73692d3a8cac8c51cefe608aa4b22a02",
          "title": "S+PAGE: A Speaker and Position-Aware Graph Neural Network Model for Emotion Recognition in Conversation"
        },
        {
          "paperId": "d81ec6393db9d67741b90915911180a52e42413d",
          "title": "ViT-based VQ-VAE Generative Network for Accompaniment Generation"
        },
        {
          "paperId": "122ac5e90051fe41919884fe274a1cb66528f326",
          "title": "Semantics-Recovering Decompilation through Neural Machine Translation"
        },
        {
          "paperId": "a1031489ac747006b0ecb1a1a6d74b2e76ada4d4",
          "title": "Joint-training on Symbiosis Networks for Deep Nueral Machine Translation models"
        },
        {
          "paperId": "c44da47301bae65f687650997293ba1933ec9015",
          "title": "Learning Positional Embeddings for Coordinate-MLPs"
        },
        {
          "paperId": "0366d4355cbce731054714a96dbd3bbdb7399a0d",
          "title": "Protein-Ligand Binding Affinity Prediction Using Deep Learning"
        },
        {
          "paperId": "94c5ab1786266cf01d2a3f9dafd498b49d2b46a2",
          "title": "Seizure prediction in scalp EEG based channel attention dual-input convolutional neural network"
        },
        {
          "paperId": "c390111d8df8593bb7c2fbb522457d6376d5ed70",
          "title": "Towards More Efficient Insertion Transformer with Fractional Positional Encoding"
        },
        {
          "paperId": "79d9422ff0761c12f5b13b7199c08ee4e4fdfb29",
          "title": "A Convolutional Neural Network Based on Self-Attention Mechanism for Molecular Property Prediction Using Molecular Hidden Fingerprints: An efficient molecular property prediction method"
        },
        {
          "paperId": "fdfa5490dd47e7dc8556bcfafe7a4f6ea07b4dbb",
          "title": "UNITER-Based Situated Coreference Resolution with Rich Multimodal Input"
        },
        {
          "paperId": "91a4cbae6553e975ddc3b2f6850ed725ff475307",
          "title": "SwinTrack: A Simple and Strong Baseline for Transformer Tracking"
        },
        {
          "paperId": "fcf25e1affc2f8ee5bb49d156f174e9769234deb",
          "title": "Systematic Generalization with Edge Transformers"
        },
        {
          "paperId": "4c93661c3ad6fe4e3e092975afd9a451c49319a6",
          "title": "KARL-Trans-NER: Knowledge Aware Representation Learning for Named Entity Recognition using Transformers"
        },
        {
          "paperId": "366766cbf762ba8f9d50fd77bfcf1e6956b275a4",
          "title": "Dual-axial self-attention network for text classification"
        },
        {
          "paperId": "c1324e412472ec9b475ae1b4e8229481251053b2",
          "title": "Boosting Neural Machine Translation with Dependency-Scaled Self-Attention Network"
        },
        {
          "paperId": "becb855b7cf635316ec634bdbb851bda13399fc0",
          "title": "Deps-SAN: Neural Machine Translation with Dependency-Scaled Self-Attention Network"
        },
        {
          "paperId": "7f631586a368f1762866b01ff9f43c265851d52e",
          "title": "DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing"
        },
        {
          "paperId": "a782b1d91d90e3d3946cf019703162f7f9661cdf",
          "title": "GN-Transformer: Fusing Sequence and Graph Representation for Improved Code Summarization"
        },
        {
          "paperId": "9da405a29af85b16fff126db1e52158677ed8e05",
          "title": "Remote Sensing Image Scene Classification Based on Global Self-Attention Module"
        },
        {
          "paperId": "8e4a113b669dd293cac30d7dce35a452911b6fc1",
          "title": "A Survey of Visual Transformers"
        },
        {
          "paperId": "9418741ce0f9d1d0a00d7631adb17fdbfd06e4e5",
          "title": "A Chinese Multi-type Complex Questions Answering Dataset over Wikidata"
        },
        {
          "paperId": "36d1aeff3f1e57f2f2bf3cd4f596d7797862bc86",
          "title": "A Contextual Latent Space Model: Subsequence Modulation in Melodic Sequence"
        },
        {
          "paperId": "5ef4e013ce7bf0c19873c83ffd6898ce33ffd542",
          "title": "AST-Transformer: Encoding Abstract Syntax Trees Efficiently for Code Summarization"
        },
        {
          "paperId": "8e488694a0f9675eab577d45e9bbee7d6982d741",
          "title": "With a Little Help from my Temporal Context: Multimodal Egocentric Action Recognition"
        },
        {
          "paperId": "0a22b5df6c1c25b776d1015e9490974ed704c044",
          "title": "ER-SQL: Learning enhanced representation for Text-to-SQL using table contents"
        },
        {
          "paperId": "ff653651ddc57081f0a2b82ebcc2126905089182",
          "title": "Two Heads are Better than One: Geometric-Latent Attention for Point Cloud Classification and Segmentation"
        },
        {
          "paperId": "61a1a2cd42d24700f17934f6f74e1f400f625e69",
          "title": "PatchFormer: A Versatile 3D Transformer Based on Patch Attention"
        },
        {
          "paperId": "c6481af1c5ebba68a8f59ade7d07eab1265d8e15",
          "title": "Dispensed Transformer Network for Unsupervised Domain Adaptation"
        },
        {
          "paperId": "063eee315e864f0842d3074629dccc4bb36d19e7",
          "title": "Discovering Non-monotonic Autoregressive Orderings with Variational Inference"
        },
        {
          "paperId": "a26d810b78fa39b23253ee072aeedac88c4c87c4",
          "title": "Operation Diagnosis on Procedure Graph: The Task and Dataset"
        },
        {
          "paperId": "1dc2edea1b328c846bd48c59541f7b7c49b9d59e",
          "title": "Cache-based GNN System for Dynamic Graphs"
        },
        {
          "paperId": "0eb024e6591f071f832be7a18657e9d6eb85fa1c",
          "title": "Positional Mask Attention for Video Sequence Modeling"
        },
        {
          "paperId": "df473ca1b625f63335cd20524b16ddeb9c45e8b9",
          "title": "Hierarchical Semantic Enhanced Directional Graph Network for Visual Commonsense Reasoning"
        },
        {
          "paperId": "eb36750a31af99baf22ed01d0b8c24d9ff550b40",
          "title": "EAPT: Efficient Attention Pyramid Transformer for Image Processing"
        },
        {
          "paperId": "b67ff4b6fca84d57f951a3ce1cc9829f6c902dce",
          "title": "Multimodal Video Summarization via Time-Aware Transformers"
        },
        {
          "paperId": "03da17fc6d1868efc0b91872f1823e8ff4612824",
          "title": "Direction Relation Transformer for Image Captioning"
        },
        {
          "paperId": "46c8b4b133a359dec97a03d54b36de054384d5be",
          "title": "Image Search with Text Feedback by Deep Hierarchical Attention Mutual Information Maximization"
        },
        {
          "paperId": "42c248f36f8fe9b1cd530ca9b40ecc16fb29afbd",
          "title": "Multi-View Stereo Network with attention thin volume"
        },
        {
          "paperId": "b34e7babf9443d792a765b65fd9f5dc3b9b3ddee",
          "title": "RTJTN: Relational Triplet Joint Tagging Network for Joint Entity and Relation Extraction"
        },
        {
          "paperId": "bc605d228cc2536ec13d3a1a00cbb334f9350f8a",
          "title": "A Speaker-Aware Learning Framework for Improving Multi-turn Dialogue Coherence"
        },
        {
          "paperId": "701f5eb3337c7ea3731168615886bba810aa1ff6",
          "title": "A Speaker-aware Parallel Hierarchical Attentive Encoder-Decoder Model for Multi-turn Dialogue Generation"
        },
        {
          "paperId": "c501d6f1eecf3b0fdae7d428e1829bb6607a6a37",
          "title": "Relative Molecule Self-Attention Transformer"
        },
        {
          "paperId": "281fc1783a91481ebbd22e01f85e17ac67990a6f",
          "title": "A Systematic Review of Deep Learning Approaches for Natural Language Processing in Battery Materials Domain"
        },
        {
          "paperId": "347a313eb34790b221233de0b32b41df2a749f5c",
          "title": "Learning Efficient Multi-Agent Cooperative Visual Exploration"
        },
        {
          "paperId": "3356254badd0652bbb75515eae688517dc8d260f",
          "title": "Revitalizing CNN Attentions via Transformers in Self-Supervised Visual Representation Learning"
        },
        {
          "paperId": "3d5699e7f7e085ad72102859b06fa4884d207e77",
          "title": "Iterative Decoding for Compositional Generalization in Transformers"
        },
        {
          "paperId": "68f14f333dad84ec35fc0eb9fcfd2c41fdc02596",
          "title": "ATISS: Autoregressive Transformers for Indoor Scene Synthesis"
        },
        {
          "paperId": "2b63f69c6eff99c06d0ff5d7fd3aafce1d3b6bdc",
          "title": "Learning to Iteratively Solve Routing Problems with Dual-Aspect Collaborative Transformer"
        },
        {
          "paperId": "7d34244c64fec4808503e21d648eaaefe5d85208",
          "title": "SPaR.txt, a Cheap Shallow Parsing Approach for Regulatory Texts"
        },
        {
          "paperId": "77b12e430b2238bf22a0dbc1d38f0891f1e42d57",
          "title": "Molformer: Motif-based Transformer on 3D Heterogeneous Molecular Graphs"
        },
        {
          "paperId": "8b2d357347b57217966863a36c95f4b52e6be3a6",
          "title": "Gaze estimation via self-attention augmented convolutions"
        },
        {
          "paperId": "74fc2b3999897b0a09934224100212531335055a",
          "title": "ResSaNet: A Hybrid Backbone of Residual Block and Self-Attention Module for Masked Face Recognition"
        },
        {
          "paperId": "7dbeb92e7c234b3887cc484ce406ad86b5743add",
          "title": "Detecting Persuasive Atypicality by Modeling Contextual Compatibility"
        },
        {
          "paperId": "98eafdb2a4d6242cc916f1278a0113b102de6391",
          "title": "PortaSpeech: Portable and High-Quality Generative Text-to-Speech"
        },
        {
          "paperId": "cdeb1f6188666809bcdeec2e3b916671fc012634",
          "title": "GT U-Net: A U-Net Like Group Transformer Network for Tooth Root Segmentation"
        },
        {
          "paperId": "232acef2483a26fe95c70b619f88fa0b82c1a105",
          "title": "Multiplicative Position-aware Transformer Models for Language Understanding"
        },
        {
          "paperId": "27c4102b00a04681e5adbde4493006fd41628b35",
          "title": "Modeling Dynamic Attributes for Next Basket Recommendation"
        },
        {
          "paperId": "8b6b9d426776c3c7c82f5dadee0e654d5088c14e",
          "title": "Audio-Visual Speech Recognition is Worth $32\\times 32\\times 8$ Voxels"
        },
        {
          "paperId": "4a8964ea0de47010fb458021b68fa3ef5c4b77b2",
          "title": "Primer: Searching for Efficient Transformers for Language Modeling"
        },
        {
          "paperId": "df6f3c607ae3956db722f76f3f81e672c3dfa803",
          "title": "CodeQA: A Question Answering Dataset for Source Code Comprehension"
        },
        {
          "paperId": "a299947b4d588dee748d0a3b3c6a4dec55c8b212",
          "title": "A Two-Stage Short-Term Load Forecasting Method Using Long Short-Term Memory and Multilayer Perceptron"
        },
        {
          "paperId": "8a3882fd35847d78997a8491f06ebbb3019d8775",
          "title": "Generating Music Transition by Using a Transformer-Based Model"
        },
        {
          "paperId": "0a35064caba35a80f21fb9798ada7abb1377ead9",
          "title": "The NiuTrans System for the WMT21 Efficiency Task"
        },
        {
          "paperId": "255c526f78bbfec35d4ed73b6dd8eacd9ef2c0b3",
          "title": "RankNAS: Efficient Neural Architecture Search by Pairwise Ranking"
        },
        {
          "paperId": "9c5a5e932139621da37c93d48f8a8df40a9c61d4",
          "title": "Dialogue State Tracking with a Language Model using Schema-Driven Prompting"
        },
        {
          "paperId": "64a0a4f357be12aaf30cc6e4964d1c3a9d927aac",
          "title": "Sequence Length is a Domain: Length-based Overfitting in Transformer Models"
        },
        {
          "paperId": "64522a5b3476e9f201f6a5b3e312ef0005c562f1",
          "title": "SHAPE: Shifted Absolute Position Embedding for Transformers"
        },
        {
          "paperId": "32d7233aab1a5020f688333007f70bce3bf0a97a",
          "title": "Leveraging Multi-Faceted User Preferences for Improving Click-Through Rate Predictions"
        },
        {
          "paperId": "bd7907735793f122c400d3afc4e73187dc57f376",
          "title": "SPARQLing Database Queries from Intermediate Question Decompositions"
        },
        {
          "paperId": "33c831326bb47b2ba2031fd7213b6918d23eb01e",
          "title": "The Impact of Positional Encodings on Multilingual Compression"
        },
        {
          "paperId": "23d11338be48471b3979b13eb172ec67fc22244b",
          "title": "Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model"
        },
        {
          "paperId": "64f3a18921f7f3a384dca073cd6d2476b9af47f2",
          "title": "Zero-Shot Dialogue State Tracking via Cross-Task Transfer"
        },
        {
          "paperId": "bb363c8c5bc1c473f0801c647c88d0c071792858",
          "title": "PermuteFormer: Efficient Relative Position Encoding for Long Sequences"
        },
        {
          "paperId": "e8f39c0bde3a3f624d2198576f5a0b99a87f862f",
          "title": "Improving neural machine translation using gated state network and focal adaptive attention networtk"
        },
        {
          "paperId": "5b95ef4fea818f08897f0dddb56e5d3a8e5cb9d3",
          "title": "ShopTalk: A System for Conversational Faceted Search"
        },
        {
          "paperId": "8e9d44502f5b6de37d5a7181e169a2899a91f7b3",
          "title": "Self-Attention-Based Temporary Curiosity in Reinforcement Learning Exploration"
        },
        {
          "paperId": "51b5db5c679be0ce9a39a2ee21def42bca165efe",
          "title": "Shatter: An Efficient Transformer Encoder with Single-Headed Self-Attention and Relative Sequence Partitioning"
        },
        {
          "paperId": "1b45714103c276ebadb12cfe0a0d08f0667d98e5",
          "title": "LinearSpeech: Parallel Text-to-Speech with Linear Complexity"
        },
        {
          "paperId": "053ac76f7b6e12f406eaf6e953b3bd5313fd69c2",
          "title": "Ultra Fast Speech Separation Model with Teacher Student Learning"
        },
        {
          "paperId": "35e31785d64574f6f5a89be81ae934c616d3753a",
          "title": "Lightweight Causal Transformer with Local Self-Attention for Real-Time Speech Enhancement"
        },
        {
          "paperId": "ed535e93d5b5a8b689e861e9c6083a806d1535c2",
          "title": "The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"
        },
        {
          "paperId": "015b2fb3a12bd0134b91af272b29b62371ef9022",
          "title": "Recurrent multiple shared layers in Depth for Neural Machine Translation"
        },
        {
          "paperId": "5882f98d17780c6c0b2638f57cd1764dd5267c18",
          "title": "Deep Contrast Learning Approach for Address Semantic Matching"
        },
        {
          "paperId": "2eaf36dc40fd9a61f8f0fad14a6d3c9594b1c2be",
          "title": "An Effective Non-Autoregressive Model for Spoken Language Understanding"
        },
        {
          "paperId": "e3d06054af531ee2f42270d43100b309c28546ef",
          "title": "MUSIQ: Multi-scale Image Quality Transformer"
        },
        {
          "paperId": "564c7e51f930b208cf05f68a8c478d1195ccad05",
          "title": "Learning Fair Face Representation With Progressive Cross Transformer"
        },
        {
          "paperId": "79678d2f10bddf14b2aedf3427f8a4c39908931f",
          "title": "Variable-Length Music Score Infilling via XLNet and Musically Specialized Positional Encoding"
        },
        {
          "paperId": "5019fc4c08177fb70045aa0ce1edd08099cbe1ec",
          "title": "An end-to-end CNN with attentional mechanism applied to raw EEG in a BCI classification task"
        },
        {
          "paperId": "2a52e0bb16638df4216cf63ea9708891741e565e",
          "title": "Multi-Branch with Attention Network for Hand-Based Person Recognition"
        },
        {
          "paperId": "9f7f81b1c82828a45a52df8f0c6a92636af76c7e",
          "title": "CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention"
        },
        {
          "paperId": "17b4a75b432b9f58de143918608de9234a4da988",
          "title": "Towards Continual Entity Learning in Language Models for Conversational Agents"
        },
        {
          "paperId": "a9c214e846188adb645021cd7b1964b8ea1fef6f",
          "title": "Rethinking and Improving Relative Position Encoding for Vision Transformer"
        },
        {
          "paperId": "ae30c7199df1991de6a90508d40c593bfee760e0",
          "title": "CI-Net: Contextual Information for Joint Semantic Segmentation and Depth Estimation"
        },
        {
          "paperId": "4159a98819c10c2710f68d239fcc03bdb7ece472",
          "title": "Neural Rule-Execution Tracking Machine For Transformer-Based Text Generation"
        },
        {
          "paperId": "a514cc9796034bcc01450ac968b2f576fa35c70f",
          "title": "Multi-Scale Local-Temporal Similarity Fusion for Continuous Sign Language Recognition"
        },
        {
          "paperId": "fcf0cf8510e19fec46256c0c472665766fcdea7f",
          "title": "Residual Tree Aggregation of Layers for Neural Machine Translation"
        },
        {
          "paperId": "a4cc58555e2459168a8472b35d159151f6d90f30",
          "title": "A Structural Transformer with Relative Positions in Trees for Code-to-Sequence Tasks"
        },
        {
          "paperId": "2cac91a6022af7ba5fd6cb1b5ff6ebb815880bb4",
          "title": "Text classification using improved bidirectional transformer"
        },
        {
          "paperId": "a5896974ae46a3232419e93b5f7522b92995003e",
          "title": "Semantically Constrained Document-Level Chinese-Mongolian Neural Machine Translation"
        },
        {
          "paperId": "485fb51d8e2e579d42a1f9be45b1806519a6ad8c",
          "title": "DeepMutants: Training neural bug detectors with contextual mutations"
        },
        {
          "paperId": "00a92e580cb4801fc32f2ed8cdb215bfb9c1a575",
          "title": "Conformer-based End-to-end Speech Recognition With Rotary Position Embedding"
        },
        {
          "paperId": "1306680402070eecd37da6bb0d531017166c5f80",
          "title": "The Piano Inpainting Application"
        },
        {
          "paperId": "059783a60339601faabf8f9ec739fc2da89dff56",
          "title": "Tool Wear Prediction Based on Multidomain Feature Fusion by Attention-Based Depth-Wise Separable Convolutional Neural Network in Manufacturing"
        },
        {
          "paperId": "cca5a070dac2f434a10bcc12bd1377b8c7356e21",
          "title": "Locally Enhanced Self-Attention: Rethinking Self-Attention as Local and Context Terms"
        },
        {
          "paperId": "fc19e94109d4c2f05f3639a67327c708543def98",
          "title": "Locally Enhanced Self-Attention: Combining Self-Attention and Convolution as Local and Context Terms"
        },
        {
          "paperId": "3d6315f836c6ab33320df965c5504c95bd7c4a35",
          "title": "ReconVAT: A Semi-Supervised Automatic Music Transcription Framework for Low-Resource Real-World Data"
        },
        {
          "paperId": "d3f51870f4da5dd9c2a08a55cfa8a380b8d49208",
          "title": "Lighter and Better: Low-Rank Decomposed Self-Attention Networks for Next-Item Recommendation"
        },
        {
          "paperId": "fbac64d617914ab8dac0682639fbd6012faed771",
          "title": "Rethinking Positional Encoding"
        },
        {
          "paperId": "8dfa41ada085940f3e4cb2af3eb7ba6fe1082a41",
          "title": "The NiuTrans End-to-End Speech Translation System for IWSLT 2021 Offline Task"
        },
        {
          "paperId": "fbc34e0bac913acf81bebfc1be2909465543f49b",
          "title": "Relative Position Representation over Interaction Space for Natural Language Inference"
        },
        {
          "paperId": "8c7aa01a5d57082474ed9188376eeea82608a3c0",
          "title": "Investigation of Practical Aspects of Single Channel Speech Separation for ASR"
        },
        {
          "paperId": "1bed382373aed687c045bb65bc7541b16fc7a6be",
          "title": "Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from SCAN"
        },
        {
          "paperId": "6d3558a70490ffa25393996a7c09cd439aaaede2",
          "title": "Polarized Self-Attention: Towards High-quality Pixel-wise Regression"
        },
        {
          "paperId": "d1919dc2e2fad6c32caa85da4a3a40372c1d9d33",
          "title": "MNet-Sim: A Multi-layered Semantic Similarity Network to Evaluate Sentence Similarity"
        },
        {
          "paperId": "a255b4674ecac416eb37d9441e1cda2fed147b0c",
          "title": "StableEmit: Selection Probability Discount for Reducing Emission Latency of Streaming Monotonic Attention ASR"
        },
        {
          "paperId": "034869f2f55b01f240b30923983ea197ff9fc32c",
          "title": "FastPitchFormant: Source-filter based Decomposed Modeling for Speech Synthesis"
        },
        {
          "paperId": "01b5412f3d17e90e09226d7c40ad4d4468a1414d",
          "title": "Multimodal Few-Shot Learning with Frozen Language Models"
        },
        {
          "paperId": "36de6b3b18ea763619bdca2d39035a8adf1582d2",
          "title": "Language Models are Good Translators"
        },
        {
          "paperId": "0d46cbaf914da31a06ef2753e00b7f47e055e70d",
          "title": "Probabilistic Attention for Interactive Segmentation"
        },
        {
          "paperId": "c6dad3f9c9f29602b8c89585c23c73377ef00601",
          "title": "Bootstrap Representation Learning for Segmentation on Medical Volumes and Sequences"
        },
        {
          "paperId": "0d508600d77d8a7e6a655cdb6d139779732f649f",
          "title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding"
        },
        {
          "paperId": "a76d5f07caad631c196e109cc1052b85d45fb83d",
          "title": "Learning Explainable Representations of Malware Behavior"
        },
        {
          "paperId": "7cf872dbad1cd780a2ebf56f8bee76fb9497c018",
          "title": "ROPE: Reading Order Equivariant Positional Encoding for Graph-based Document Information Extraction"
        },
        {
          "paperId": "523745e29f6cb1890f18352d449fd3597910c485",
          "title": "Improving Compositional Generalization in Classification Tasks via Structure Annotations"
        },
        {
          "paperId": "b99c61f6957c1b04ec1376b74f82dd1e83559695",
          "title": "JointGT: Graph-Text Joint Representation Learning for Text Generation from Knowledge Graphs"
        },
        {
          "paperId": "f0993e68c76d940763ef7f8106db86cdc97b3a49",
          "title": "Multi-head or Single-head? An Empirical Comparison for Transformer Training"
        },
        {
          "paperId": "600a4066e6264a9f604ee03d49b246c8a17da645",
          "title": "Do Large Scale Molecular Language Representations Capture Important Structural Information?"
        },
        {
          "paperId": "946179bd263d46d70422cdff7e6657b97b81230c",
          "title": "Learning to Combine Per-Example Solutions for Neural Program Synthesis"
        },
        {
          "paperId": "bcd53292bb2999a0e0a8aa7307f760a827d7f296",
          "title": "Structure-Regularized Attention for Deformable Object Representation"
        },
        {
          "paperId": "0a12ea86c9193bf2496d65d25d1f234c1fac923b",
          "title": "Zero-Shot Controlled Generation with Encoder-Decoder Transformers"
        },
        {
          "paperId": "5d7d34abbc14739e40b53ec3c33a3c698a37e70e",
          "title": "To Beam Or Not To Beam: That is a Question of Cooperation for Language GANs"
        },
        {
          "paperId": "92bf1c069747374fbc3efb55e7a916a3e2d736da",
          "title": "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech"
        },
        {
          "paperId": "1dbb523a6555d6e0c5727620e2b57daaa5b79dc0",
          "title": "Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models"
        },
        {
          "paperId": "de282eff05d89f9fb53d1585c1377c4f0499ee12",
          "title": "Transformed CNNs: recasting pre-trained convolutional layers with self-attention"
        },
        {
          "paperId": "a6337d9ebb0b7de84588806110157806f9c0383b",
          "title": "GraphiT: Encoding Graph Structure in Transformers"
        },
        {
          "paperId": "47ae807cd511b35e78a2cd4e198283dea6dafd41",
          "title": "Do Transformers Really Perform Bad for Graph Representation?"
        },
        {
          "paperId": "9f4b69762ffb1ba42b573fd4ced996f3153e21c0",
          "title": "CoAtNet: Marrying Convolution and Attention for All Data Sizes"
        },
        {
          "paperId": "b50815251c948f00baedccaf5f56c281ffa7650f",
          "title": "Staircase Attention for Recurrent Processing of Sequences"
        },
        {
          "paperId": "95f5bafba97beb9b4f8c1fe607f04ec28efab7f9",
          "title": "Learning to Efficiently Sample from Diffusion Probabilistic Models"
        },
        {
          "paperId": "acb2d427a3669b0bec85d0d1c8d8a9a09340967c",
          "title": "Efficient Training of Visual Transformers with Small-Size Datasets"
        },
        {
          "paperId": "2d98048c2d2fcd3f6b989d2a54003808906ab4b7",
          "title": "Efficient Training of Visual Transformers with Small Datasets"
        },
        {
          "paperId": "576c462dbc1f3d732b919ef1daac37a817123e52",
          "title": "ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias"
        },
        {
          "paperId": "7509c66a666e2e3f14bc8676b969b945ee6e136f",
          "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings"
        },
        {
          "paperId": "94576783bc73bf55a0091203a3d45a0a4665a1ae",
          "title": "Learnable Fourier Features for Multi-Dimensional Spatial Positional Encoding"
        },
        {
          "paperId": "e7e626e379e67b8ae59689fa1c055ab83a5b2313",
          "title": "Dual Attentive Sequential Learning for Cross-Domain Click-Through Rate Prediction"
        },
        {
          "paperId": "ab0ef8e705fa88f079a055842ba20b4f413a69db",
          "title": "Associating Objects with Transformers for Video Object Segmentation"
        },
        {
          "paperId": "27c09d07f1a7dba51889fef9ee8f925c87fb6b00",
          "title": "X-volution: On the unification of convolution and self-attention"
        },
        {
          "paperId": "93892c5bbc489ee7bccb1b97eed92465013ea826",
          "title": "Scalable Transformers for Neural Machine Translation"
        },
        {
          "paperId": "d8e7bad2681ce70277c900c77a22181d4b03d705",
          "title": "The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models"
        },
        {
          "paperId": "4dc2c552bfb0abcf15db9cfe795da9e97551ee42",
          "title": "An Improved Model for Voicing Silent Speech"
        },
        {
          "paperId": "806dc5c64d3a65f89e0f26ff9f51bb029c6908b2",
          "title": "A Multi-Level Attention Model for Evidence-Based Fact Checking"
        },
        {
          "paperId": "50db74aa7e662b640ccbf37788af62cd8af3e930",
          "title": "LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations"
        },
        {
          "paperId": "f4b20bd6a7ec7c57f8afc2228bcfd00a7346197c",
          "title": "Smart-Start Decoding for Neural Machine Translation"
        },
        {
          "paperId": "0e09c6b34b34a2ca3eb16e8ebfbe0c2472666206",
          "title": "Enhancing Transformer with Horizontal and Vertical Guiding Mechanisms for Neural Language Modeling"
        },
        {
          "paperId": "7af513e8a395b82628dbf627fe9269a44cea14ae",
          "title": "Does Structure Matter? Encoding Documents for Machine Reading Comprehension"
        },
        {
          "paperId": "1b9b096216ce7b694e594389bd74c5b7bce6f1e6",
          "title": "StyTr^2: Unbiased Image Style Transfer with Transformers"
        },
        {
          "paperId": "8c4f89a9ac30cf94186916be1bfaa02dbfb3600d",
          "title": "CoDesc: A Large Code–Description Parallel Dataset"
        },
        {
          "paperId": "685ca8238989d93a4c196abc6627bb960967d786",
          "title": "Learning to Extend Program Graphs to Work-in-Progress Code"
        },
        {
          "paperId": "957424a1f3319a2aab1a91539a00e712477b4b4a",
          "title": "Self-Attention Networks Can Process Bounded Hierarchical Languages"
        },
        {
          "paperId": "ca5e5a98ebe6497a43b1868f4f2a86851b09fee8",
          "title": "Explainable Enterprise Credit Rating via Deep Feature Crossing Network"
        },
        {
          "paperId": "9f863fcb229e762a95b8bb0e4a89d7aeeb3d8640",
          "title": "Link Prediction on N-ary Relational Facts: A Graph-based Approach"
        },
        {
          "paperId": "08ffdec40291a2ccb5f8a6cc048b01247fb34b96",
          "title": "Relative Positional Encoding for Transformers with Linear Complexity"
        },
        {
          "paperId": "d82a03951796a352dc4442387782c21d2b761480",
          "title": "GTA: Graph Truncated Attention for Retrosynthesis"
        },
        {
          "paperId": "a3196e65467b80f4755968923b382e40c02ccb51",
          "title": "Two-Stream Convolution Augmented Transformer for Human Activity Recognition"
        },
        {
          "paperId": "f07c5c540233b22f0ca154c80c713e2aed3c9606",
          "title": "Symbolic Music Generation with Transformer-GANs"
        },
        {
          "paperId": "50a9ff8b1a3f49220baa0950bc4645ad6f88f013",
          "title": "HCRF-Flow: Scene Flow from Point Clouds with Continuous High-order CRFs and Position-aware Flow Embedding"
        },
        {
          "paperId": "64a29bee2e1ad29547d590a3cc26274f4c537145",
          "title": "Not All Memories are Created Equal: Learning to Forget by Expiring"
        },
        {
          "paperId": "95080f0ee0a2d4fcb9fd330bf4060870849a8b47",
          "title": "Global Structure-Aware Drum Transcription Based on Self-Attention Mechanisms"
        },
        {
          "paperId": "e563ba6db52146dd002e3f1527f92dde9a1d3cfa",
          "title": "MuseMorphose: Full-Song and Fine-Grained Music Style Transfer with Just One Transformer VAE"
        },
        {
          "paperId": "aecc84aa9ff29a99f44556689592cd0fff84cb87",
          "title": "Leveraging Slot Descriptions for Zero-Shot Cross-Domain Dialogue StateTracking"
        },
        {
          "paperId": "6a3e13d7926a4aaa0ddcd3acc7c08e8d24c330e5",
          "title": "ReadTwice: Reading Very Large Documents with Memories"
        },
        {
          "paperId": "61cce75554a6d1bb802f26758c3b0ba97de6918d",
          "title": "Graph Attention Networks with Positional Embeddings"
        },
        {
          "paperId": "7f066a5fb06c0abe09ce67e9e420868d49ea47ad",
          "title": "Duplex Sequence-to-Sequence Learning for Reversible Machine Translation"
        },
        {
          "paperId": "a83902f8b3aadfda633968a840ca1738bedef837",
          "title": "Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs"
        },
        {
          "paperId": "5213e56878c0df2f6f8f59c1d412daf98de1d8bf",
          "title": "COUnty aggRegation mixup AuGmEntation (COURAGE) COVID-19 prediction"
        },
        {
          "paperId": "01fe50c8023f15bce3628d4581f59bfe33bcdf16",
          "title": "Incorporating Transformer and LSTM to Kalman Filter with EM algorithm for state estimation"
        },
        {
          "paperId": "6dd0d7749691ea5f9d826fd7e8b3680d16b07427",
          "title": "Score-Transformer: A Deep Learning Aid for Music Composition"
        },
        {
          "paperId": "2c9fdba6bf846e0986cbbf30d56b467d9e334333",
          "title": "ConTNet: Why not use convolution and transformer at the same time?"
        },
        {
          "paperId": "1841d235095ba022f1463f0ac2436e29601829e4",
          "title": "Neural Machine Translation for Harmonized System Codes prediction"
        },
        {
          "paperId": "52f79cb91dd3149960dd5d436d8746441fbffce4",
          "title": "Optimizing small BERTs trained for German NER"
        },
        {
          "paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
          "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
        },
        {
          "paperId": "ceb1a7e4bccb14b048e81e023b3219edca08862b",
          "title": "Deep drug-target binding affinity prediction with multiple attention blocks"
        },
        {
          "paperId": "db46b0de44c5113c47f0ec5392eb91d0726497bf",
          "title": "A Simple and Effective Positional Encoding for Transformers"
        },
        {
          "paperId": "a1ad15d2333cf9d9b55bbc97a3aacd244a8b9fdf",
          "title": "Demystifying the Better Performance of Position Encoding Variants for Transformer"
        },
        {
          "paperId": "eaa9fde5756852745e7e14383794ba75f797cc8b",
          "title": "Question Decomposition with Dependency Graphs"
        },
        {
          "paperId": "1abd71fb70b8c3639af1d087cd179eaef8c718b0",
          "title": "MIMO Self-attentive RNN Beamformer for Multi-speaker Speech Separation"
        },
        {
          "paperId": "4c1307807b72eb8bd8f4026462e71b195b5cb621",
          "title": "Syntax-Aware Graph-to-Graph Transformer for Semantic Role Labelling"
        },
        {
          "paperId": "5db0b92418dfe1f44a9440c682fae05b77bc6587",
          "title": "Nonparametric analysis of inter‐individual relations using an attention‐based neural network"
        },
        {
          "paperId": "d27eac86c86a953a5b1ad13f7c7bc9d5fb127837",
          "title": "Simultaneous Face Hallucination and Translation for Thermal to Visible Face Verification using Axial-GAN"
        },
        {
          "paperId": "5b68522f58b61e7235b852677337ef3725075fd9",
          "title": "Co-Scale Conv-Attentional Image Transformers"
        },
        {
          "paperId": "19be83b770b96905288fa47dd882665a4c64bb45",
          "title": "Estimating articulatory movements in speech production with transformer networks"
        },
        {
          "paperId": "c114db5f1c38cbe6797bc74ef98072cac71f6cc6",
          "title": "ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser"
        },
        {
          "paperId": "b43b69a6d303fe3c349d5ecf5920ff1dd47ec650",
          "title": "Dual self-attention with co-attention networks for visual question answering"
        },
        {
          "paperId": "e12e837cb2e9baeaefdcab06fe1c75add8f46389",
          "title": "Effective gene expression prediction from sequence by integrating long-range interactions"
        },
        {
          "paperId": "319d6c88152780f2835687100921393457668cc0",
          "title": "ODE Transformer: An Ordinary Differential Equation-Inspired Model for Neural Machine Translation"
        },
        {
          "paperId": "17a2834c6540703fd35edde4e5f7cc50da4ab41f",
          "title": "MuSLCAT: Multi-Scale Multi-Level Convolutional Attention Transformer for Discriminative Music Modeling on Raw Waveforms"
        },
        {
          "paperId": "5b81580712f6e16abd05824c162537674e99b095",
          "title": "Learning Neural Representation of Camera Pose with Matrix Representation of Pose Shift via View Synthesis"
        },
        {
          "paperId": "003326a15fc4a8833785a47a741d7712474fa256",
          "title": "LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference"
        },
        {
          "paperId": "c478ad9eb6ffd16f3c63cfc942fd77081fbb6c3f",
          "title": "TDJEE: A Document-Level Joint Model for Financial Event Extraction"
        },
        {
          "paperId": "92255f5be9f254057f809943398c156808ef47eb",
          "title": "Transformer-based end-to-end speech recognition with residual Gaussian-based self-attention"
        },
        {
          "paperId": "2012bcf3b172c2888dc46cceeb419c3c26010e6c",
          "title": "A Practical Survey on Faster and Lighter Transformers"
        },
        {
          "paperId": "712bf9c7202b8dec3d06491a380bbac9c9600fbc",
          "title": "Mask Attention Networks: Rethinking and Strengthen Transformer"
        },
        {
          "paperId": "6b9564d94aa4415b90561b430ac85b0ee11a5833",
          "title": "NL-EDIT: Correcting Semantic Parse Errors through Natural Language Interaction"
        },
        {
          "paperId": "91e8117e7ebc966bc76de2cb52ec717d2acdb1a4",
          "title": "Scaling Local Self-Attention for Parameter Efficient Visual Backbones"
        },
        {
          "paperId": "05e396e79a2f88f0b8f8d99f5ab36ab3efa95c14",
          "title": "Language-Agnostic Representation Learning of Source Code from Structure and Context"
        },
        {
          "paperId": "5dc1e65fdad52b1847c49fae1dbc6a1ba8ac232f",
          "title": "Neural Machine Translating from XML to RDF"
        },
        {
          "paperId": "c9963e89add5e02bf6e5b340586ee6406e087089",
          "title": "API2Com: On the Improvement of Automatically Generated Code Comments Using API Documentations"
        },
        {
          "paperId": "395221cd3ff22539f261ef1fc305fa3e928fca35",
          "title": "Unsupervised Transfer Learning in Multilingual Neural Machine Translation with Cross-Lingual Word Embeddings"
        },
        {
          "paperId": "483b7857df6104d5c8899c61c64ab0397f87f7fa",
          "title": "I2Net: Mining intra-video and inter-video attention for temporal action localization"
        },
        {
          "paperId": "6a66cf7dd6d1f70e99018518e765917fb76491ad",
          "title": "Variable-rate discrete representation learning"
        },
        {
          "paperId": "f77d9e0b14598df68d6cfb64c5dd70916ee29aea",
          "title": "IOT: Instance-wise Layer Reordering for Transformer Structures"
        },
        {
          "paperId": "e369e6a871eb9ca85cc41ff309631f6da2a79c2b",
          "title": "A Survey on Document-level Neural Machine Translation"
        },
        {
          "paperId": "3bdc2e6b22b2b7567eb6fc02a4a4348737cb8526",
          "title": "Advances in Deep Learning Methods for Visual Tracking: Literature Review and Fundamentals"
        },
        {
          "paperId": "3cbce50dcd213b10190a9d27ddafa700752380e5",
          "title": "An End-to-End Network for Emotion-Cause Pair Extraction"
        },
        {
          "paperId": "eaa88d697f92739f3569564329e9d037aabbe2d7",
          "title": "A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics"
        },
        {
          "paperId": "88a8061c17de49527c05e14b593671913fb32117",
          "title": "Nested-block self-attention for robust radiotherapy planning segmentation"
        },
        {
          "paperId": "daaa6d5a00adad83ca2e00820d9fe9e6d78f40c7",
          "title": "Accelerating Transformer for Neural Machine Translation"
        },
        {
          "paperId": "5776b9e8146bb7f6b3df1321cc7f121670699c90",
          "title": "Iterative SE(3)-Transformers"
        },
        {
          "paperId": "2fd10e095b146f99da8cdc6ff58720e2e8fca36d",
          "title": "When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute"
        },
        {
          "paperId": "79b4ec1aaf67a04a9afa0d8138f84b7be66c00cb",
          "title": "Do Transformer Modifications Transfer Across Implementations and Applications?"
        },
        {
          "paperId": "654247d5b184495fca18c6aa7e840e4f4559fef0",
          "title": "Do We Really Need Explicit Position Encodings for Vision Transformers?"
        },
        {
          "paperId": "1efb035c38baa3a3a282d3d8184679b2146b0671",
          "title": "Few Shot Learning for Information Verification"
        },
        {
          "paperId": "1e5e8106700c8dbdfa036a5a9be5e61e06c0ed02",
          "title": "Medical Transformer: Gated Axial-Attention for Medical Image Segmentation"
        },
        {
          "paperId": "0fe8b49369d70a2be473435a82b01544704b3c9f",
          "title": "Evolving Attention with Residual Convolutions"
        },
        {
          "paperId": "07129116591119162bf873e7dafc9ab44aae323e",
          "title": "Celltrack R-CNN: A Novel End-To-End Deep Neural Network For Cell Segmentation And Tracking In Microscopy Images"
        },
        {
          "paperId": "92a21e9ce3f702d0a2b0d619c4c974bdc8ff23cd",
          "title": "Entity Structure Within and Throughout: Modeling Mention Dependencies for Document-Level Relation Extraction"
        },
        {
          "paperId": "36906613dcef29263afe711f128da1fc916cbbee",
          "title": "Gaussian Kernelized Self-Attention for Long Sequence Data and its Application to CTC-Based Speech Recognition"
        },
        {
          "paperId": "cec7872b194aadf54140578b9be52939eb1112e9",
          "title": "LambdaNetworks: Modeling Long-Range Interactions Without Attention"
        },
        {
          "paperId": "594db112d65891fbaf45b27f17d2f9de88ddcd82",
          "title": "Revisiting Language Encoding in Learning Multilingual Representations"
        },
        {
          "paperId": "7095f65f655468951b7e1edb97db4ede9365064a",
          "title": "A multi-omics-based serial deep learning approach to predict clinical outcomes of single-agent anti-PD-1/PD-L1 immunotherapy in advanced stage non-small-cell lung cancer."
        },
        {
          "paperId": "868d4d9c9d8afee78f21a0113cff762ff5eb4961",
          "title": "Translational Equivariance in Kernelizable Attention"
        },
        {
          "paperId": "6e8f35c6d54acb14109c9b792a62609eac8a7b5e",
          "title": "TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up"
        },
        {
          "paperId": "1f479fe113b0a4a5d6da08e0632395d3273fd3dd",
          "title": "InsNet: An Efficient, Flexible, and Performant Insertion-based Text Generation Model"
        },
        {
          "paperId": "6f22b0da3447eebff003d0f6d4b9ed0864b85d2a",
          "title": "Transformer Language Models with LSTM-Based Cross-Utterance Information Representation"
        },
        {
          "paperId": "57c4035f49e6063dd9e8498418f05135d931235a",
          "title": "On the application of BERT models for nanopore methylation detection"
        },
        {
          "paperId": "1a73f486d35ad7c20e140f30ab7148c6804f8aaa",
          "title": "Attentive Gaussian processes for probabilistic time-series generation"
        },
        {
          "paperId": "ca4b945ad7d109c3cbc2170a942ca3b0ecf6fcf5",
          "title": "Wake Word Detection with Streaming Transformers"
        },
        {
          "paperId": "e175d23d33ccef7df511c1d299851ba763ca6ea4",
          "title": "Structural attention network for graph"
        },
        {
          "paperId": "292d8fabc74dc5cbb42df821afb724535220fceb",
          "title": "Syntax-Informed Self-Attention Network for Span-Based Joint Entity and Relation Extraction"
        },
        {
          "paperId": "1dd30a5e56b39486369f27aa5ea8e3f282670c85",
          "title": "NRTSI: Non-Recurrent Time Series Imputation"
        },
        {
          "paperId": "f5a2ce064ea0efc3d7f26acd91041824c3254cd8",
          "title": "NRTSI: Non-Recurrent Time Series Imputation for Irregularly-sampled Data"
        },
        {
          "paperId": "a6ca91afe845ef5294c40c2029e0c1cba19ba40b",
          "title": "Unifying Vision-and-Language Tasks via Text Generation"
        },
        {
          "paperId": "dc030407206fc26fda7df35c79f9a5a96419edb1",
          "title": "Enquire One’s Parent and Child Before Decision: Fully Exploit Hierarchical Structure for Self-Supervised Taxonomy Expansion"
        },
        {
          "paperId": "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78",
          "title": "Bottleneck Transformers for Visual Recognition"
        },
        {
          "paperId": "f05126c1a792ea64a7af0c8c68b03bcddec5b297",
          "title": "VisualMRC: Machine Reading Comprehension on Document Images"
        },
        {
          "paperId": "0477ef522fa3ec861a9ff5ba35c96cc08ba3697f",
          "title": "Representations for Question Answering from Documents with Tables and Text"
        },
        {
          "paperId": "4f889a6b76fb77cbd9ed239ba9ba38748bceb89a",
          "title": "An Investigation of Positional Encoding in Transformer-based End-to-end Speech Recognition"
        },
        {
          "paperId": "bf8241fff03fa98c8e57d858c125110a218c0a35",
          "title": "Mitigating the Position Bias of Transformer Models in Passage Re-Ranking"
        },
        {
          "paperId": "12300cc0acb5b29ca5ed57c03f1276e2c2da0d96",
          "title": "To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph"
        },
        {
          "paperId": "1f465b77c7d774da1fa18827e197a6372bbaa63b",
          "title": "A Zero Attentive Relevance Matching Networkfor Review Modeling in Recommendation System"
        },
        {
          "paperId": "e08eed9608382beea1febca49119c665fbabd031",
          "title": "The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained Sequence-to-Sequence Models"
        },
        {
          "paperId": "5af128253aa1b8d21821ab10f2aa446b5143fabd",
          "title": "A Chinese named entity recognition method combined with relative position information"
        },
        {
          "paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
          "title": "Shortformer: Better Language Modeling using Shorter Inputs"
        },
        {
          "paperId": "7434fb6c79a50bfedd4ad1c872f9e4fb1f25571a",
          "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer"
        },
        {
          "paperId": "b086b812c867b1d07eb65bcdd206dd0891733f9d",
          "title": "Vocabulary Learning via Optimal Transport for Neural Machine Translation"
        },
        {
          "paperId": "cd02e0a094953077217e2e62f3557b36a365acff",
          "title": "Optimizing Deeper Transformers on Small Datasets"
        },
        {
          "paperId": "0197abda042e6de87b5f716caa708a6a459f078c",
          "title": "LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding"
        },
        {
          "paperId": "77f9b54da444f49b2436712abe932627f16cbf95",
          "title": "Code Summarization with Structure-induced Transformer"
        },
        {
          "paperId": "a817d740f64dcc04734ece08e20e136ccff240e7",
          "title": "Learning Light-Weight Translation Models from Deep Transformer"
        },
        {
          "paperId": "37f4cecf9cf83cb6de2ffaecc864226eff1f9918",
          "title": "Document Graph for Neural Machine Translation"
        },
        {
          "paperId": "3a77f0699c036ee09ab5805213ca1cb5baa12a7a",
          "title": "Investigating Typed Syntactic Dependencies for Targeted Sentiment Classification Using Graph Attention Neural Network"
        },
        {
          "paperId": "787119e3c3f819244c82b7d97779473773e60696",
          "title": "MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers"
        },
        {
          "paperId": "f556bdc969127ecd8c9f3faef20f200beff7826f",
          "title": "A Light Transformer For Speech-To-Intent Applications"
        },
        {
          "paperId": "bb77ed615f4b9e5f55a54e1f69cb3f887667147c",
          "title": "Deep learning pan‐specific model for interpretable MHC‐I peptide binding prediction with improved attention mechanism"
        },
        {
          "paperId": "54e6dc08b468b740bcdfdd83b2514be410994400",
          "title": "Persuasive Dialogue Understanding: the Baselines and Negative Results"
        },
        {
          "paperId": "d2405a1cb8b41701fa8177e1c2438c730051ac01",
          "title": "Context-aware Decoder for Neural Machine Translation using a Target-side Document-Level Language Model"
        },
        {
          "paperId": "11c2f3e323161b90d600fb9e0c56de06caea5dde",
          "title": "Blind Deinterleaving of Signals in Time Series with Self-Attention Based Soft Min-Cost Flow Learning"
        },
        {
          "paperId": "556983a574e322fd7d01c6a8193b3785c97a8905",
          "title": "A Simple Approach for Handling Out-of-Vocabulary Identifiers in Deep Learning for Source Code"
        },
        {
          "paperId": "5868a7bfe6a4590d332ca66b8097dbe5490c8a73",
          "title": "SmBoP: Semi-autoregressive Bottom-up Semantic Parsing"
        },
        {
          "paperId": "f879bd75a10acbb4223d6feb9652ca3a8d35581f",
          "title": "Don’t Shoot Butterfly with Rifles: Multi-Channel Continuous Speech Separation with Early Exit Transformer"
        },
        {
          "paperId": "131ee42e4839d153333e17f46facdb6806e98c73",
          "title": "Developing Real-Time Streaming Transformer Transducer for Speech Recognition on Large-Scale Dataset"
        },
        {
          "paperId": "a2462ff5546b45b1cc7cb50ffc50c7ddececca65",
          "title": "DuoRAT: Towards Simpler Text-to-SQL Models"
        },
        {
          "paperId": "1d00f609e8cbac2f7a67a04e13adf82a096ea689",
          "title": "Predicting Chemical Properties using Self-Attention Multi-task Learning based on SMILES Representation"
        },
        {
          "paperId": "2b2711b23dd2503933b2f02a41574fc72d21aabf",
          "title": "Empirical study of transformers for source code"
        },
        {
          "paperId": "cf469e6fffbc5344572959a3f71d03b8037f284b",
          "title": "DA-Transformer: Distance-aware Transformer"
        },
        {
          "paperId": "9af8f2bba51c76fe7fcc1eec5b57b5445747b5ad",
          "title": "GATE: Graph Attention Transformer Encoder for Cross-lingual Relation and Event Extraction"
        },
        {
          "paperId": "1be28ce9a1145c2cf4f78e6c494a4c15397fbac3",
          "title": "SumGNN: Multi-typed Drug Interaction Prediction via Efficient Knowledge Graph Summarization"
        },
        {
          "paperId": "7a5d1a7646ce1884ad76a0e177f956ae4d77c722",
          "title": "Group Equivariant Stand-Alone Self-Attention For Vision"
        },
        {
          "paperId": "53aaff9d979084d76daf4048345e0ccf4bacb386",
          "title": "TRU-NET: A Deep Learning Approach to High Resolution Prediction of Rainfall"
        },
        {
          "paperId": "2abd377ae35740a28a38fde4f84aa6534e39ff37",
          "title": "Continuous Speech Separation with Conformer"
        },
        {
          "paperId": "8256f48f759cf85044db251cc512f965834945b3",
          "title": "Rethinking Positional Encoding in Language Pre-training"
        },
        {
          "paperId": "fa22ef1881de15c75ad7e65936bdad7dad43ebad",
          "title": "Syntax-based Transformer for Neural Machine Translation"
        },
        {
          "paperId": "59eccbe007ccf92172499db8420c12ea0933e24b",
          "title": "Explicitly Modeled Attention Maps for Image Classification"
        },
        {
          "paperId": "05f5f8b2065a520846d89771ebaea2bb1534e9c6",
          "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
        },
        {
          "paperId": "7c5c149699a0ba54b52cd5b9e291077f4a1f9d13",
          "title": "Synthesizer: Rethinking Self-Attention in Transformer Models"
        },
        {
          "paperId": "904b7802d2c07161c11351fdc246f48a4a3da29b",
          "title": "Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement"
        },
        {
          "paperId": "657329c633709dd1ac34a30d57341b186b1a47c2",
          "title": "Efficient Content-Based Sparse Attention with Routing Transformers"
        },
        {
          "paperId": "b15f06c8c50d2e8ac0f2dc782e2ecf501c2c7e52",
          "title": "MANet: Multimodal Attention Network based Point-View Fusion for 3D Shape Recognition"
        },
        {
          "paperId": "b3254a5e2866b52b644962b4ad1917922d903938",
          "title": "LAMBERT: Layout-Aware Language Modeling for Information Extraction"
        },
        {
          "paperId": "12cf222b05755a59655a5846f990c2aaf6065086",
          "title": "On the Importance of Word Order Information in Cross-lingual Sequence Labeling"
        },
        {
          "paperId": "8e5b2c00444f517ff12765ae2597108f544b00d7",
          "title": "Attention! A Lightweight 2D Hand Pose Estimation Approach"
        },
        {
          "paperId": "8a44ede59c7fec2418839747336ac7b1ec3e49cb",
          "title": "Position-aware self-attention based neural sequence labeling"
        },
        {
          "paperId": "48bf1d216339ee13ed8e518eb67bf5fef9b001df",
          "title": "Enlivening Redundant Heads in Multi-head Self-attention for Machine Translation"
        },
        {
          "paperId": "c687acfeaafb86ab045f9fcbb6e2ff691827a0c2",
          "title": "On Position Embeddings in BERT O N P OSITION E MBEDDINGS IN BERT"
        },
        {
          "paperId": "78fd56819e9963b13432d4c01e9afda1ae2884d7",
          "title": "Position-aware Graph Attention Networks for Emotion Recognition in Conversations"
        },
        {
          "paperId": "2a405483796dfedf5d95483aa8880c57626e0e9f",
          "title": "Integrating Tree Path in Transformer for Code Representation"
        },
        {
          "paperId": "4be2a1bcd4a972ca2030d1624f942e7a391aa08c",
          "title": "A Unified Encoding of Structures in Transition Systems"
        },
        {
          "paperId": "3297bc6c604b81c84a979ea3ee1a3768761264d6",
          "title": "Incorporating Relative Position Information in Transformer-Based Sign Language Recognition and Translation"
        },
        {
          "paperId": "74fe28fe925da3a2d34846960f8e58d7cdf87eb6",
          "title": "Medical Term and Status Generation From Chinese Clinical Dialogue With Multi-Granularity Transformer"
        },
        {
          "paperId": "dc35daba3fb34b2e6a5b12530badb7b799262bbf",
          "title": "On Position Embeddings in BERT"
        },
        {
          "paperId": "3dc2556f89e02e180e29e63b1f9e9fea0c369835",
          "title": "Preordering Encoding on Transformer for Translation"
        },
        {
          "paperId": "621ae436951d97249bbb5fb22c6c4786c787eaed",
          "title": "Global Context and Geometric Priors for Effective Non-Local Self-Attention"
        },
        {
          "paperId": "8d6b1929b92211ad0eb3e14b2c2b41789ccf053a",
          "title": "Explore Better Relative Position Embeddings from Encoding Perspective for Transformer Models"
        },
        {
          "paperId": "cc4ce024b58e9ce81eb9291262898e1f7019dc35",
          "title": "Translating Headers of Tabular Data: A Pilot Study of Schema Translation"
        },
        {
          "paperId": "3e487ed5f2df3b8d120bcdbf2812422d2f7ae2db",
          "title": "Searching Effective Transformer for Seq2Seq Keyphrase Generation"
        },
        {
          "paperId": "2d9ec5a86c0525da54ce8dee7b9ac38718567e03",
          "title": "Comparing Span Extraction Methods for Semantic Role Labeling"
        },
        {
          "paperId": "8babeaffc8413747412765803a50d2c3adbbbbdd",
          "title": "DEBERTA: DECODING-ENHANCED BERT"
        },
        {
          "paperId": "c8e3df02135aac8ccfa12f962c6c856e9c0fa600",
          "title": "Clause Attention based on Signal Words Division"
        },
        {
          "paperId": "0a488f7c299327d9eb8f089d1bb9ede62197aef4",
          "title": "Data and Text Mining SumGNN: Multi-typed Drug Interaction Prediction via Efﬁcient Knowledge Graph Summarization"
        },
        {
          "paperId": "202c27a5239e232562a32e3d1892b8a2e62834f6",
          "title": "The NiuTrans System for the WMT 2021 Efficiency Task"
        },
        {
          "paperId": "70bb772805e0560ff1a061a6903ea92a0f416cba",
          "title": "HW-TSC’s Participation in the WMT 2021 Efficiency Shared Task"
        },
        {
          "paperId": "594414401010930a4153bca75bc14323f69e1e43",
          "title": "The NiuTrans Machine Translation Systems for WMT21"
        },
        {
          "paperId": "79159305945229a16f1c02bde93e8015ebd7dc55",
          "title": "MiSS@WMT21: Contrastive Learning-reinforced Domain Adaptation in Neural Machine Translation"
        },
        {
          "paperId": "6a2d6237927a076d3da7100a2fcc65710ae443a8",
          "title": "Syntactic Knowledge-Infused Transformer and BERT Models"
        },
        {
          "paperId": "8e7bb46419d4c96053a87827c960f68674b2dccb",
          "title": "How Does it Sound? Generation of Rhythmic Soundtracks for Human Movement Videos"
        },
        {
          "paperId": "6945ed07fcfcc3597f7dc15a67a9d8602601fde3",
          "title": "PortaSpeech: Portable and High-Quality Generative Text-to-Speech"
        },
        {
          "paperId": "57cde90173c7baabfbf40742cd7f3d2e4e30351c",
          "title": "MuseBERT: Pre-training Music Representation for Music Understanding and Controllable Generation"
        },
        {
          "paperId": "22894675216595f4651f300d03d1a05d04a0e0ac",
          "title": "POAT-Net: Parallel Offset-attention Assisted Transformer for 3D Object Detection for Autonomous Driving"
        },
        {
          "paperId": "28eec83187e67a676fa443ae2bfb79f0ef232b46",
          "title": "AutoFormer: Searching Transformers for Visual Recognition —— Supplementary Material ——"
        },
        {
          "paperId": "cb799406cc00258a51936c5ea73199df71164727",
          "title": "Event Prominence Extraction Combining a Knowledge-Based Syntactic Parser and a BERT Classifier for Dutch"
        },
        {
          "paperId": "3544532e9cfdf386b82cd999650dec54f4457e1f",
          "title": "Learn to Copy from the Copying History: Correlational Copy Network for Abstractive Summarization"
        },
        {
          "paperId": "cc53d235f7757983b17d9d2a1b63b0a94c9b695a",
          "title": "A Language Model-based Generative Classifier for Sentence-level Discourse Parsing"
        },
        {
          "paperId": "df4f0c97d4ee2e28bb8a88fcc9a1c865a861fc69",
          "title": "Cross Attention Augmented Transducer Networks for Simultaneous Translation"
        },
        {
          "paperId": "1cedb352a10828b3ec263a4da907794bea052282",
          "title": "PLUR: A Unifying, Graph-Based View of Program Learning, Understanding, and Repair"
        },
        {
          "paperId": "6fe61d77b8a4a090899867b79e32efd658f848e7",
          "title": "Explainable Natural Language to Bash Translation using Abstract Syntax Tree"
        },
        {
          "paperId": "678f0cdb6b8e59aa2ef6c409b97fecc529059703",
          "title": "An Enhanced Visual Attention Siamese Network That Updates Template Features Online"
        },
        {
          "paperId": "0fff3ad88f5f63be5dc3b3f6fdff4720ecba2c7d",
          "title": "Bering Lab’s Submissions on WAT 2021 Shared Task"
        },
        {
          "paperId": "bf0d69c053653cc7a0de852343391c19653f8a5d",
          "title": "SmBoP: Semi-autoregressive Bottom-up Semantic Parsing"
        },
        {
          "paperId": "d4eb2ca9694f34d63abe6d27bd2d958992431017",
          "title": "Theano: A Greek-speaking conversational agent for COVID-19"
        },
        {
          "paperId": "3071e5ca83f2c4fe4a4c62fd48a8abd3342a0dc1",
          "title": "Multilingual Speech Translation KIT @ IWSLT2021"
        },
        {
          "paperId": "0a53f5ef9769b2f08ceee72e23312974a7cdf7e7",
          "title": "FINDINGS OF THE IWSLT 2021 EVALUATION CAMPAIGN"
        },
        {
          "paperId": "35a3237dae11d0269c721aa1acd101c281e470a8",
          "title": "Adversary-Aware Rumor Detection"
        },
        {
          "paperId": "b8cba3c5880a8ad29259408ef67fbdc6b41d2027",
          "title": "Insertion-based Tree Decoding"
        },
        {
          "paperId": "a4242e9077e9674d0e3896b1a4d651ff742cb5ca",
          "title": "Mention Flags (MF): Constraining Transformer-based Text Generators"
        },
        {
          "paperId": "b029346bfa85fa2fc7a12498ae5f018922ce55f9",
          "title": "Code Generation from Natural Language with Less Prior Knowledge and More Monolingual Data"
        },
        {
          "paperId": "312fd02d69bb5f95761bd627f51fc5157940dfa6",
          "title": "Multi-Head Highly Parallelized LSTM Decoder for Neural Machine Translation"
        },
        {
          "paperId": "d4d26ccbf1e64e725b5bffc08ab28a72e271facb",
          "title": "Chase: A Large-Scale and Pragmatic Chinese Dataset for Cross-Database Context-Dependent Text-to-SQL"
        },
        {
          "paperId": "676dc1bfb68c969a1ce8b135fbf215421df5f0a0",
          "title": "HumorHunter at SemEval-2021 Task 7: Humor and Offense Recognition with Disentangled Attention"
        },
        {
          "paperId": "8bbf0b6c928acfceb2ab4ded56d8cc7f3245fe42",
          "title": "Improving Breadth-Wise Backpropagation in Graph Neural Networks Helps Learning Long-Range Dependencies"
        },
        {
          "paperId": "764c400ac71c73eb533223723520b6b8a44cd5bf",
          "title": "AutoAttend: Automated Attention Representation Search"
        },
        {
          "paperId": "46419f5a084d743a08b55342e1ca23267bca0da3",
          "title": "Learning Deep Attention Network from Incremental and Decremental Features for Evolving Features"
        },
        {
          "paperId": "53433afb53bb01f25660d3eb0f5f5f88db66094b",
          "title": "TransNet: Shift Invariant Transformer Network for Power Attack"
        },
        {
          "paperId": "f012c1e29cc5732fa4c1b21f7e9c6227cd2816ea",
          "title": "Impromptune: Symbolic Music Generation with Relative Attention Mechanisms"
        },
        {
          "paperId": "a93a08eedf4aa4c1912fd24ecf8b4590aa1b2117",
          "title": "Anomaly Detection Methods for Log Files"
        },
        {
          "paperId": "44c4bfa8548205fec5a82a19974faab93134a9d7",
          "title": "A Modified Generative Adversarial Network Using Spatial and Channel-Wise Attention for CS-MRI Reconstruction"
        },
        {
          "paperId": "b59d64ba042afbc198a806954517007074213b86",
          "title": "Big Bird: Transformers for Longer Sequences – Appendix"
        },
        {
          "paperId": "52d01d9f71caf0d021fccb75b75b7d3dfc7460f5",
          "title": "On Scalar Embedding of Relative Positions in Attention Models"
        },
        {
          "paperId": "31ec5d12752c171a9331250ae7ef05e536816c24",
          "title": "Probability-Mixing : Semi-Supervised Learning in Question-Answering with Data Augmentation"
        },
        {
          "paperId": "578db2aa1902f9f5861d60fe2a917d2f9be85056",
          "title": "Pretraining of Transformers on Question Answering without External Data"
        },
        {
          "paperId": "976a609cf540d1ded373b872d34779f7164d840a",
          "title": "Rethinking the Design Principles of Robust Vision Transformer"
        },
        {
          "paperId": "743838b675cd79dd8b669bb988847ab28771e2bf",
          "title": "GasHis-Transformer: A Multi-scale Visual Transformer Approach for Gastric Histopathology Image Classification"
        },
        {
          "paperId": "c4cb90a67f45e7cbacb5286e934b309e89843922",
          "title": "Attention is Turing-Complete"
        },
        {
          "paperId": "84ab506e49351abec11223700dc6065785855d0a",
          "title": "A Graph Convolutional Network-Based Sensitive Information Detection Algorithm"
        },
        {
          "paperId": "ecf0f4f94ab215c2da098b0ea2cb3a49d124e27c",
          "title": "Deep Graph-Based Character-Level Chinese Dependency Parsing"
        },
        {
          "paperId": "3c1535cd1e97e3c37a51e9734b4965e25787461a",
          "title": "A template for the arxiv style"
        },
        {
          "paperId": "0502d45c3e0759602f51f79bea1044319c58678f",
          "title": "Chord Conditioned Melody Generation With Transformer Based Decoders"
        },
        {
          "paperId": "eeddeab9b2383b49b1f453e27d22a76b73286049",
          "title": "Attend to Chords: Improving Harmonic Analysis of Symbolic Music Using Transformer-Based Models"
        },
        {
          "paperId": "b7a697b7cba927898643b81bad92aaf70a9a03f7",
          "title": "A Transformer Self-attention Model for Time Series Forecasting"
        },
        {
          "paperId": "ceeac1741f8c8dee5a1365828f79e8d3e84b0623",
          "title": "Semantic Parsing with Less Prior and More Monolingual Data"
        },
        {
          "paperId": "66bb4a1ea9e901c51a2de8fbd44cb6b141cb489a",
          "title": "AISE: Attending to Intent and Slots Explicitly for better spoken language understanding"
        },
        {
          "paperId": "11dcf910b4221e4c7a5440962371585cecf46502",
          "title": "The NiuTrans Machine Translation Systems for WMT20"
        },
        {
          "paperId": "6dc710b46bc510a42af487a3ac220e4fabf4d518",
          "title": "VOLT: Improving Vocabularization via Optimal Transport for Machine Translation"
        },
        {
          "paperId": "32580b1814416e7dfaf6f569302441046c1ac39e",
          "title": "Optimizing Deeper Transformers on Small Datasets: An Application on Text-to-SQL Semantic Parsing"
        },
        {
          "paperId": "23b38bf1e43329791828cc64796cd77ca3b0e0c1",
          "title": "SIT3: Code Summarization with Structure-Induced Transformer"
        },
        {
          "paperId": "5d04a41b2b8ecf75aa6631abebe7a00bb25a3cdf",
          "title": "Hybrid Interest Modeling for Long-tailed Users"
        },
        {
          "paperId": "72bc6534a83a8a8bc51c3d7185d7820a37728e98",
          "title": "Multiple Structural Priors Guided Self Attention Network for Language Understanding"
        },
        {
          "paperId": "15e82f4d369c73320e83fa02e9cb782239838cab",
          "title": "Attention-based Image Upsampling"
        },
        {
          "paperId": "95b0fc5bde87f7c4f513cf8532ca873823971d2f",
          "title": "Portfolio Optimization with 2D Relative-Attentional Gated Transformer"
        },
        {
          "paperId": "f51bac51dc4f9f42942966b13f62df355ac9af1f",
          "title": "Weighted Cluster-Range Loss and Criticality-Enhancement Loss for Speaker Recognition"
        },
        {
          "paperId": "4c5d4601a3a19c31da6588d2a34adfb161f68c0e",
          "title": "Imitating Interactive Intelligence"
        },
        {
          "paperId": "bd6788cb13952d67cf668cc08f62d20ea090b646",
          "title": "Enhancing Attention Models via Multi-head Collaboration"
        },
        {
          "paperId": "28bd12dfc7414b39023ff3cc9c17cb92d1a849c2",
          "title": "ODIANLP’s Participation in WAT2020"
        },
        {
          "paperId": "5a84bf724ee79c507e66e5dc564c98c76d553a53",
          "title": "Porous Lattice Transformer Encoder for Chinese NER"
        },
        {
          "paperId": "ac4ae352e2434d4a71c6a79bf5f93df5f600b058",
          "title": "Increasing Learning Efficiency of Self-Attention Networks through Direct Position Interactions, Learnable Temperature, and Convoluted Attention"
        },
        {
          "paperId": "cc12cea62725035d5428888117e0d58359c539e3",
          "title": "Generalized Shortest-Paths Encoders for AMR-to-Text Generation"
        },
        {
          "paperId": "d5052e267f059e8ce78deb477f368b84636e6b3f",
          "title": "Joint Aspect Extraction and Sentiment Analysis with Directional Graph Convolutional Networks"
        },
        {
          "paperId": "076b78ac25f0f9c0de968ec1374cc791bb403e5f",
          "title": "Improving Performance of a Resonant String-Based Pulsation Attenuator in Hydraulic Systems"
        },
        {
          "paperId": "d568d183750f3dfd338fd702cb188009457c4a5d",
          "title": "A Hierarchical Self-attentive Convolution Network for Review Modeling in Recommendation Systems"
        },
        {
          "paperId": "8188ae74363ab55bf02e4204517314db0112795e",
          "title": "Attention Aware Cost Volume Pyramid Based Multi-view Stereo Network for 3D Reconstruction"
        },
        {
          "paperId": "c2b4d96db34bd472e84c9234838cc4e808eb1ba9",
          "title": "Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language"
        },
        {
          "paperId": "920294817417d9c2a4fbed86bd595450d58d60dc",
          "title": "The Ubiqus English-Inuktitut System for WMT20"
        },
        {
          "paperId": "ee15b0d8b7de63bdfc31532e866dfeb675442964",
          "title": "s-Transformer: Segment-Transformer for Robust Neural Speech Synthesis"
        },
        {
          "paperId": "ce7fd7cd09b0fc82969d374b438d14bcaa9c6ee2",
          "title": "Transformer Based Molecule Encoding for Property Prediction"
        },
        {
          "paperId": "4b5a931aea987e01a659a3bf67075d89130e7bbe",
          "title": "I Know What You Asked: Graph Path Learning using AMR for Commonsense Reasoning"
        },
        {
          "paperId": "56bf3402b25e75faafff96f80fe43168a7e93b97",
          "title": "Relation-aware Graph Attention Networks with Relational Position Encodings for Emotion Recognition in Conversations"
        },
        {
          "paperId": "f7f0cbd289ee9f17aad546d78c14830c99e3d37d",
          "title": "Bi-Directional Self-Attention with Relative Positional Encoding for Video Summarization"
        },
        {
          "paperId": "a45ee25414948e454bf14bf40d5d03dbaf42e9bd",
          "title": "The University of Helsinki and Aalto University submissions to the WMT 2020 news and low-resource translation tasks"
        },
        {
          "paperId": "c565ab25591ade03f89277cfbfba507335f5356c",
          "title": "An Empirical Exploration of Local Ordering Pre-training for Structured Prediction"
        },
        {
          "paperId": "232b40980acb55afa89ec50dd9806a5e551f699b",
          "title": "Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic Parsing"
        },
        {
          "paperId": "00b677e971ded11ac4a7da1b80ffda95b4f1ed78",
          "title": "Pre-Training Transformers as Energy-Based Cloze Models"
        },
        {
          "paperId": "39a991a80df0d6586d838dab97d5e17040ad93be",
          "title": "Discourse Self-Attention for Discourse Element Identification in Argumentative Student Essays"
        },
        {
          "paperId": "6d4babd73cb98c452ff10d8bbc13d540fc1626b5",
          "title": "Local-Aggregation Graph Networks"
        },
        {
          "paperId": "3c661d139bf560716db7f4b1939faf087317b509",
          "title": "Every Layer Counts: Multi-Layer Multi-Head Attention for Neural Machine Translation"
        },
        {
          "paperId": "bef4548a43fca8a7410734e4200157d50e257a29",
          "title": "End-to-End ASR with Adaptive Span Self-Attention"
        },
        {
          "paperId": "4d51dce0873d8df2aeeb4c4f2b000a9b2993eae4",
          "title": "Stronger Transformers for Neural Multi-Hop Question Generation"
        },
        {
          "paperId": "2a5c611d6a4cfc0ed66562bd754f1cbb5426f702",
          "title": "Multi-Unit Transformer for Neural Machine Translation"
        },
        {
          "paperId": "8f8701e17f1b14f5a0a69fa2b24e7d8515f6d711",
          "title": "Multi-Unit Transformers for Neural Machine Translation"
        },
        {
          "paperId": "8452f4248eecc9396bd8146e3a8a8a68d1838bb8",
          "title": "Personalized Flight Itinerary Ranking at Fliggy"
        },
        {
          "paperId": "91b84b91002dc9ff5ee8ef45854c6847ba7f8ce4",
          "title": "DiDi’s Machine Translation System for WMT2020"
        },
        {
          "paperId": "707ee137373a9f3e650a032f7627053aa59bea71",
          "title": "Non-autoregressive Neural Machine Translation with Distortion Model"
        },
        {
          "paperId": "b2d849e2c5a87b812289db8ae555b20d299ca809",
          "title": "Information Extraction from Swedish Medical Prescriptions with Sig-Transformer Encoder"
        },
        {
          "paperId": "4889ba5a8ae8b2169dd44d1d3a605bf9820bae8d",
          "title": "What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding"
        },
        {
          "paperId": "2a0417e641233cd9aa77c58f00f944d3c5e84d62",
          "title": "AutoQA: From Databases to Q&A Semantic Parsers with Only Synthetic Training Data"
        },
        {
          "paperId": "53af14897ccca4f8d6c465ee133e859f99ddd7e7",
          "title": "Shallow-to-Deep Training for Neural Machine Translation"
        },
        {
          "paperId": "c05c39a01a418b2b32bbf9d543293a2665cd3d6b",
          "title": "SSACBKM: An Integration Model for Biomedical Relationship Extraction"
        },
        {
          "paperId": "46e7383e6fb8da77479d0a828c7a24d924302169",
          "title": "Stepwise Extractive Summarization and Planning with Structured Transformers"
        },
        {
          "paperId": "64b9be00f4eecd465b4e8e46e2ab7624d7eaeb2b",
          "title": "Global Self-Attention Networks for Image Recognition"
        },
        {
          "paperId": "efb315c39ee035ef5fad8a66378b96b653f4a96d",
          "title": "SlotRefine: A Fast Non-Autoregressive Model for Joint Intent Detection and Slot Filling"
        },
        {
          "paperId": "90d6719d1b898a0d35e2502ca0f3724eb1e7245d",
          "title": "MIA-Prognosis: A Deep Learning Framework to Predict Therapy Response"
        },
        {
          "paperId": "f8a77d8c43e0fab50aef6c05eafbe653039ac447",
          "title": "Lexicon-Enhanced Transformer with Pointing for Domains Specific Generative Question Answering"
        },
        {
          "paperId": "f43bc763593e9b2fa48199274ce79eeaf4c7e238",
          "title": "Information Extraction Method based on Dilated Convolution and Character-Enhanced Word Embedding"
        },
        {
          "paperId": "6c46bd6de48214a1e81ebef369070713febaeced",
          "title": "Generating Vietnamese Language Caption Automatically for Scene Images"
        },
        {
          "paperId": "3214fa2979603057aabd2c82e58c92da9d2fcf25",
          "title": "Entity Relative Position Representation based Multi-head Selection for Joint Entity and Relation Extraction"
        },
        {
          "paperId": "1f1ba4520e646ab27b2978a1d76f18c5e387ba7b",
          "title": "MQTransformer: Multi-Horizon Forecasts with Context Dependent and Feedback-Aware Attention"
        },
        {
          "paperId": "84476fdf6ead3553f4493dff8e02308439d6222b",
          "title": "Improve Transformer Models with Better Relative Position Embeddings"
        },
        {
          "paperId": "28858688894da173dbdcc49899be6e22ea97bc63",
          "title": "MinTL: Minimalist Transfer Learning for Task-Oriented Dialogue Systems"
        },
        {
          "paperId": "517902025f4de7b4c89540ab057516ad357c8214",
          "title": "PURS: Personalized Unexpected Recommender System for Improving User Satisfaction"
        },
        {
          "paperId": "daa00c142fdf25aa454d8ca52e4d20f5d0605964",
          "title": "Temporally Guided Music-to-Body-Movement Generation"
        },
        {
          "paperId": "8189b5580e8080d3d9c800d9855d2f72f867b3f5",
          "title": "Building a Personally Identifiable Information Recognizer in a Privacy Preserved Manner Using Automated Annotation and Federated Learning"
        },
        {
          "paperId": "0fe9f97efe4afd806b30be75a4b698ad2db93fc5",
          "title": "Graph-to-Sequence Neural Machine Translation"
        },
        {
          "paperId": "0e1610392cb0cd91b8261b7fbaed78178e304885",
          "title": "Cascaded Semantic and Positional Self-Attention Network for Document Classification"
        },
        {
          "paperId": "24a151d2811e831992e840bd30f5de5f75aca72d",
          "title": "Multi-domain sentiment analysis with mimicked and polarized word embeddings for human-robot interaction"
        },
        {
          "paperId": "d6d8b2bd23cb3feea0e37714564e8a849e105442",
          "title": "Speech Emotion Recognition UsingConvolutional Neural Network and Long-Short TermMemory"
        },
        {
          "paperId": "6f3ad944bb56c6e930eac3d95cff1521ac03101e",
          "title": "On estimating gaze by self-attention augmented convolutions"
        },
        {
          "paperId": "5bc8f2fd72b3ffa0432a971c4097dc370f6d6222",
          "title": "SoDA: Multi-Object Tracking with Soft Data Association"
        },
        {
          "paperId": "95210c74f28664d1481be29af411220d27cf63d7",
          "title": "POP909: A Pop-song Dataset for Music Arrangement Generation"
        },
        {
          "paperId": "eb6309dd0e70fe68ba2c97629f9091eae3cbf8b4",
          "title": "Conv-Transformer Transducer: Low Latency, Low Frame Rate, Streamable End-to-End Speech Recognition"
        },
        {
          "paperId": "97eccd93834d48e1eb3063b6f51288e1421a54b2",
          "title": "EXTENDED CONVOLUTIONAL NEURAL NETWORKS POST-TRAINED WITH FACTORED STATISTICAL MACHINE"
        },
        {
          "paperId": "1da75743ad30c9c9b23a534888be2dc99e52d087",
          "title": "Select, Extract and Generate: Neural Keyphrase Generation with Syntactic Guidance"
        },
        {
          "paperId": "41374b84f9bffda7862bb6103301017d3af2b2e4",
          "title": "Integrating Bi-Dynamic Routing Capsule Network with Label-Constraint for Text classification"
        },
        {
          "paperId": "81c5c35ad1311fb1ebae00f6d87631021fc7d956",
          "title": "Self-Attention and Dynamic Convolution Hybrid Model for Neural Machine Translation"
        },
        {
          "paperId": "6181c5fb6b82a267dae129351cce8477a1fb463d",
          "title": "Relation Aware Attention Model for Uncertainty Detection in Text"
        },
        {
          "paperId": "5fb52197928290d3020b2256ccab22d5bf93c366",
          "title": "Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos"
        },
        {
          "paperId": "044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3",
          "title": "Big Bird: Transformers for Longer Sequences"
        },
        {
          "paperId": "e7a4e1a5d933ca482ef215784085eaf19b8d5e16",
          "title": "Spatially Aware Multimodal Transformers for TextVQA"
        },
        {
          "paperId": "43971a0a2593f660427e016032b983b52f8dd8eb",
          "title": "Foley Music: Learning to Generate Music from Videos"
        },
        {
          "paperId": "566a026da7d2b1eec034017d67c6bb901f5acd85",
          "title": "RobustScanner: Dynamically Enhancing Positional Clues for Robust Text Recognition"
        },
        {
          "paperId": "9a2b7ca4d98352f57785115862a81ff06666ef8a",
          "title": "Self-Attentive Hawkes Process"
        },
        {
          "paperId": "20aec946cdc97135677afaa2373fe379cc3d33f7",
          "title": "Transformer-XL Based Music Generation with Multiple Sequences of Time-valued Notes"
        },
        {
          "paperId": "1f1e38015e6f9d072c2b7bb2af1212d1c71b5b06",
          "title": "Learning Graph Structure With A Finite-State Automaton Layer"
        },
        {
          "paperId": "1a67761027bd89b078445bef7040ded5d20dd2c8",
          "title": "Software Engineering Event Modeling using Relative Time in Temporal Knowledge Graphs"
        },
        {
          "paperId": "d358f8939992a8c2c649338810aa5e66ed665568",
          "title": "Polar Relative Positional Encoding for Video-Language Segmentation"
        },
        {
          "paperId": "40f003d5784d069a4c901edc5438b11f75496d51",
          "title": "LIT Team’s System Description for Japanese-Chinese Machine Translation Task in IWSLT 2020"
        },
        {
          "paperId": "8b2aae249e0a6961fb4790644561247620c40791",
          "title": "CASIA’s System for IWSLT 2020 Open Domain Translation"
        },
        {
          "paperId": "ee0b27c32603a352ddebe27f98f89fc8d7795cb6",
          "title": "TimeSAN: A Time-Modulated Self-Attentive Network for Next Point-of-Interest Recommendation"
        },
        {
          "paperId": "18f86146a095a30d57569197c04f807bb10064e8",
          "title": "The NiuTrans System for WNGT 2020 Efficiency Task"
        },
        {
          "paperId": "df6d1e461dc8245aa441eb87267ec5d2c28ad1d2",
          "title": "Heterogeneous Graph Transformer for Graph-to-Sequence Learning"
        },
        {
          "paperId": "7d8c9587ec051ed1612a686d93c6d1a92b7cbda4",
          "title": "Generating Medical Reports from Patient-Doctor Conversations Using Sequence-to-Sequence Models"
        },
        {
          "paperId": "a4886702f5c92f0d643a61578a58684d403e8f5e",
          "title": "PA-GGAN: Session-Based Recommendation with Position-Aware Gated Graph Attention Network"
        },
        {
          "paperId": "01db8a5e8d3d868517eb3b2b50aabcabf7b66d50",
          "title": "Hybrid Models for Learning to Branch"
        },
        {
          "paperId": "5156381d63bb3e873533b08f203cb56c2d79b6c9",
          "title": "Object-Centric Learning with Slot Attention"
        },
        {
          "paperId": "cf1eb488136995d76b5e64410fffa63ed7236702",
          "title": "Differentiable Window for Dynamic Local Attention"
        },
        {
          "paperId": "82b5ee0ae468cd2e0b62df96f42b5b5480c75510",
          "title": "Infinite attention: NNGP and NTK for deep attention networks"
        },
        {
          "paperId": "4e99406c10b61004826a0428634ad5c7b0f2f731",
          "title": "SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks"
        },
        {
          "paperId": "6c5fb4717f2a402a6a6ba60b896b6ccdfa715981",
          "title": "I-BERT: Inductive Generalization of Transformer to Arbitrary Context Lengths"
        },
        {
          "paperId": "856b12bc3b99db662e93def08e49db73c1df5e79",
          "title": "Modeling Graph Structure via Relative Position for Better Text Generation from Knowledge Graphs"
        },
        {
          "paperId": "75e17a2dc0a2b2c1cce5ab89d8c703fb8bddbef1",
          "title": "On the Computational Power of Transformers and Its Implications in Sequence Modeling"
        },
        {
          "paperId": "575e4b980654773c40101659fe8d63ba325a2431",
          "title": "Multi-head enhanced self-attention network for novelty detection"
        },
        {
          "paperId": "d457157eab0df47daca66d5c81321f0a9c41e30b",
          "title": "Positional Context Aggregation Network for Remote Sensing Scene Classification"
        },
        {
          "paperId": "5d8cbed235f6aed824e515885a5c8c04161c7761",
          "title": "A Survey of the Model Transfer Approaches to Cross-Lingual Dependency Parsing"
        },
        {
          "paperId": "3c33cc56dfb1d53f568ec80ebaa133d9c91b5ea7",
          "title": "BG-SAC: Entity relationship classification model based on Self-Attention supported Capsule Networks"
        },
        {
          "paperId": "86d70b862b37402b42848c544813c7b920fc2e84",
          "title": "Improving Self-Attention Networks With Sequential Relations"
        },
        {
          "paperId": "12867c63f39cbed83713dd0d68a56cda66554790",
          "title": "Comparative Study of Machine Learning Models and BERT on SQuAD"
        },
        {
          "paperId": "2ed0d4931d89528bd53482a0b6587ebcbba6d096",
          "title": "Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search"
        },
        {
          "paperId": "852733cd68848d1f5b4e940bc94d50a412744aa1",
          "title": "Sentiment analysis with deep neural networks: comparative study and performance assessment"
        },
        {
          "paperId": "96e24bd4bf212cf8e8c66d2f78081cdabcb5913f",
          "title": "Joint Model of Entity Recognition and Relation Extraction with Self-attention Mechanism"
        },
        {
          "paperId": "74c84c391930224f7e1e150525132b3ff8aca8a0",
          "title": "Towards More Diverse Input Representation for Neural Machine Translation"
        },
        {
          "paperId": "0a82b81fbc0bc25bf4d60f9a18c8ee3571e80d7d",
          "title": "Relative Positional Encoding for Speech Recognition and Direct Translation"
        },
        {
          "paperId": "27338d00461eda47ea931f7214cb2910f67bfba6",
          "title": "BiQGEMM: Matrix Multiplication with Lookup Table for Binary-Coding-Based Quantized DNNs"
        },
        {
          "paperId": "46e7ec6a63e0e29be7d1988025d6bf7f50351f81",
          "title": "Searching Better Architectures for Neural Machine Translation"
        },
        {
          "paperId": "38b40ae531ddca434de07015637b78413370d15a",
          "title": "The Roles of Language Models and Hierarchical Models in Neural Sequence-to-Sequence Prediction"
        },
        {
          "paperId": "b53c742835ad4fac842d415d08aa2519fcd3e4ad",
          "title": "Recent Advances in SQL Query Generation: A Survey"
        },
        {
          "paperId": "d97cd476bef990352ae921e4c7c5ae1222e9b8da",
          "title": "A Mixture of h - 1 Heads is Better than h Heads"
        },
        {
          "paperId": "9ae116139ae77ca6d78e162e9639681adef5761c",
          "title": "The Unstoppable Rise of Computational Linguistics in Deep Learning"
        },
        {
          "paperId": "63ec033f8bf1ab0d859d1b98f6d075baacb156c6",
          "title": "Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension"
        },
        {
          "paperId": "812aed3f4032bd28a14d4cd3a40c77cb82066b9c",
          "title": "How Does Selective Mechanism Improve Self-Attention Networks?"
        },
        {
          "paperId": "07a9f47885cae97efb7b4aa109392128532433da",
          "title": "Hard-Coded Gaussian Attention for Neural Machine Translation"
        },
        {
          "paperId": "805a6d1df9f460abfcea3d51d181cf1e80680be4",
          "title": "A Transformer-based Approach for Source Code Summarization"
        },
        {
          "paperId": "03e1f4c1f06b4be13fb567831914acd3591f7a03",
          "title": "Mixup Multi-Attention Multi-Tasking Model for Early-Stage Leukemia Identification"
        },
        {
          "paperId": "39206ad9fe0859a0043bd8caea2e3f8202b67533",
          "title": "Improving End-to-End Speech Synthesis with Local Recurrent Neural Network Enhanced Transformer"
        },
        {
          "paperId": "876be9b226601821eeade310013506a03f023824",
          "title": "Capsule-Transformer for Neural Machine Translation"
        },
        {
          "paperId": "08066c80919620397e8e4e5372ff84caf401e675",
          "title": "Global Relational Models of Source Code"
        },
        {
          "paperId": "f02bb4bc0b711fd90d32bd7dadd505465d6b45e1",
          "title": "Probabilistically Masked Language Model Capable of Autoregressive Generation in Arbitrary Word Order"
        },
        {
          "paperId": "8474155e4a0085c44b12817d5da5a20853b56285",
          "title": "Lite Transformer with Long-Short Range Attention"
        },
        {
          "paperId": "de7032ee638659b93e0b5e3684ab0327f3ba16fb",
          "title": "Vector Quantized Contrastive Predictive Coding for Template-based Music Generation"
        },
        {
          "paperId": "c51b99f18f0ca9e1e2fea7132bf5654299817d6c",
          "title": "DIET: Lightweight Language Understanding for Dialogue Systems"
        },
        {
          "paperId": "128df7097379767d872841722eab54138bd6192f",
          "title": "Exploiting deep representations for natural language processing"
        },
        {
          "paperId": "0e002114cd379efaca0ec5cda6d262b5fe0be104",
          "title": "MPNet: Masked and Permuted Pre-training for Language Understanding"
        },
        {
          "paperId": "2d143c1f0289dbcc9cbf3ea0bfb18dbc6e0b2815",
          "title": "Detecting Uncertainty in Text using Multi-Channel CNN-TreeBiLSTM Network"
        },
        {
          "paperId": "56676aef356ebb13cba77fc9e4d70760fbc151f5",
          "title": "ETC: Encoding Long and Structured Inputs in Transformers"
        },
        {
          "paperId": "baed71eed57ad462f3ab138d4b1700a738cd5414",
          "title": "ETC: Encoding Long and Structured Data in Transformers"
        },
        {
          "paperId": "bccfdbf542bc1d0d7172906db36dce542805d101",
          "title": "Highway Transformer: Self-Gating Enhanced Self-Attentive Networks"
        },
        {
          "paperId": "57dc1eec85b7c0490ce00d4f02c660b7c0b06f05",
          "title": "Improving Scholarly Knowledge Representation: Evaluating BERT-based Models for Scientific Relation Classification"
        },
        {
          "paperId": "d95c7683cdc908e53af496eb5d3ce6bd6c8f5053",
          "title": "HopGAT: Hop-aware Supervision Graph Attention Networks for Sparsely Labeled Graphs"
        },
        {
          "paperId": "951747a8d4f516cb011b4f8d19c764f14f155723",
          "title": "Explicit Reordering for Neural Machine Translation"
        },
        {
          "paperId": "a2061fa43759830136dc158250c6e20ffcbd5e9b",
          "title": "Evaluating Machines by their Real-World Language Use"
        },
        {
          "paperId": "d0d5625ae371fc1b42ea4d499fe8b89b8260cd49",
          "title": "Multiple Positional Self-Attention Network for Text Classification"
        },
        {
          "paperId": "39283c3d6262b24bd61c88038353f3ed0145b6e4",
          "title": "Self-Attention with Cross-Lingual Position Representation"
        },
        {
          "paperId": "f723df370dfd2f7dd5fb7fc8d92dfd83dc0092cc",
          "title": "Group Recommendation with Latent Voting Mechanism"
        },
        {
          "paperId": "9954f3207f74bef08345096a3ff70311d9ea2391",
          "title": "Non-Parametric Analysis of Inter-Individual Relations Using an Attention-Based Neural Network"
        },
        {
          "paperId": "70ea5d98141cd845d1c8131f25ddbc77f962f3d8",
          "title": "Video Object Grounding Using Semantic Roles in Language Description"
        },
        {
          "paperId": "016a3ba7adcae71f5a23ed2663d8062ae1da63e6",
          "title": "SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection"
        },
        {
          "paperId": "b990e1fd1e0e100530add646d8321d44cc65f5e9",
          "title": "Toward Tag-free Aspect Based Sentiment Analysis: A Multiple Attention Network Approach"
        },
        {
          "paperId": "833560cd68a3e3d1be1bc650756dd6c679798551",
          "title": "Normalized and Geometry-Aware Self-Attention Network for Image Captioning"
        },
        {
          "paperId": "8eba733040b016e9c7ec5c3dc87cc1b28a5c2000",
          "title": "Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation"
        },
        {
          "paperId": "e8984c6e6c24aab26c332728a5fff616dfb3adbb",
          "title": "Learning to Encode Position for Transformer with Continuous Dynamical Model"
        },
        {
          "paperId": "987678b10eccaa192abf36ed6dc274a0587e1981",
          "title": "Local Contextual Attention with Hierarchical Structure for Dialogue Act Recognition"
        },
        {
          "paperId": "65ba73d0b4f76f480fe5c5ef76441583563e2ba8",
          "title": "Sentiment Analysis with Contextual Embeddings and Self-Attention"
        },
        {
          "paperId": "dd79f0f99de86a1ce71008660ab4fd5194e79940",
          "title": "Comprehensive Document Summarization with Refined Self-Matching Mechanism"
        },
        {
          "paperId": "0ca7d8c3250d43d14fdde46bf6fc299654d861ef",
          "title": "Heterogeneous Graph Transformer"
        },
        {
          "paperId": "962950c12a5ac59807c358a074a922f117d4b6f3",
          "title": "Meta-Embeddings Based On Self-Attention"
        },
        {
          "paperId": "aea8e43c643d0393c692465e6609efe8d115c1e2",
          "title": "Hyperspectral Band Selection Using Attention-Based Convolutional Neural Networks"
        },
        {
          "paperId": "77b91d7607518994d04f75119db4138b23e2eb87",
          "title": "Natural Language Processing Advancements By Deep Learning: A Survey"
        },
        {
          "paperId": "57f123c95ecf9d901be3a53291f53302740451e2",
          "title": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation"
        },
        {
          "paperId": "1bd93fdbb9f371ba81bb12e14c4af83f52b8701b",
          "title": "GRET: Global Representation Enhanced Transformer"
        },
        {
          "paperId": "c08f1e9f6a62306ca746189fecd7b9b75886e648",
          "title": "Exploiting Typed Syntactic Dependencies for Targeted Sentiment Classification Using Graph Attention Neural Network"
        },
        {
          "paperId": "c6dd45c1986f3577336bc9aa596cbd23d610d904",
          "title": "Transformer Hawkes Process"
        },
        {
          "paperId": "8890eeda67d02117a589b0ba41c69419c40c7d5e",
          "title": "Accessing Higher-level Representations in Sequential Transformers with Feedback Memory"
        },
        {
          "paperId": "26963fda30da20ee342b8e5250667ef702c5aed2",
          "title": "Towards Making the Most of Context in Neural Machine Translation"
        },
        {
          "paperId": "c5bae6a19d8fb1f5b9b823d6c671376dcee27789",
          "title": "Toward Making the Most of Context in Neural Machine Translation"
        },
        {
          "paperId": "355601688c5c567bc5900c2ed09941538bfed960",
          "title": "Molecule Attention Transformer"
        },
        {
          "paperId": "4ecf4356c3b451b16780788a3f94e422d4deeda5",
          "title": "Tree-structured Attention with Hierarchical Accumulation"
        },
        {
          "paperId": "3fa8d2a9e9a9cf3ee9626424a157888580dcfaba",
          "title": "A Survey of Deep Learning Techniques for Neural Machine Translation"
        },
        {
          "paperId": "ac0e0e6cfb5bac417c7769867b1366bd64d1b8bd",
          "title": "On the localness modeling for the self-attention based end-to-end speech synthesis"
        },
        {
          "paperId": "d02d402cabe6d19649d823c4582028faf5667459",
          "title": "Non-Autoregressive Neural Dialogue Generation"
        },
        {
          "paperId": "525e78a4368d2ca702a324367e01091bf9d4f812",
          "title": "Bottom-Up Scene Text Detection with Markov Clustering Networks"
        },
        {
          "paperId": "af34ea4242ca8725ea739ec1bef674bec10c1fa9",
          "title": "Time-aware Large Kernel Convolutions"
        },
        {
          "paperId": "2c3859564617e910651b56373e6c0941656f32ff",
          "title": "LAVA NAT: A Non-Autoregressive Translation Model with Look-Around Decoding and Vocabulary Attention"
        },
        {
          "paperId": "76f67dbc08e083c9e9bda0bf58e7f900fe371339",
          "title": "Pop Music Transformer: Beat-based Modeling and Generation of Expressive Pop Piano Compositions"
        },
        {
          "paperId": "7a79099447bef9a3ea13b1dc409d04b3dff57320",
          "title": "Pop Music Transformer: Generating Music with Rhythm and Harmony"
        },
        {
          "paperId": "b2a8c682683bbe0362d13f378d4f20002433a479",
          "title": "Do We Need Word Order Information for Cross-lingual Sequence Labeling"
        },
        {
          "paperId": "57da7772bff7b15df6ad71757a5a41e941ebd84c",
          "title": "Interpretable Rumor Detection in Microblogs by Attending to User Interactions"
        },
        {
          "paperId": "247e0a4455da68eb4500a36dbb51233fcbab7812",
          "title": "SANST: A Self-Attentive Network for Next Point-of-Interest Recommendation"
        },
        {
          "paperId": "9c7a455b1e48d01a99e58e884a6c1acb75074ad0",
          "title": "Time Interval Aware Self-Attention for Sequential Recommendation"
        },
        {
          "paperId": "36ff7927f9049d37b314c2a114769b517b3f5f7a",
          "title": "Joint Recognition of Names and Publications in Academic Homepages"
        },
        {
          "paperId": "777f9d1f23027da0acc65d6f529beb2e7830bafe",
          "title": "Two-Level Transformer and Auxiliary Coherence Modeling for Improved Text Segmentation"
        },
        {
          "paperId": "72aa8d7f6f3a36d5e208cd1f41e90613ed6aee75",
          "title": "Encoding word order in complex embeddings"
        },
        {
          "paperId": "ad3222db224444e0998bcc9aa66341bfe5d81c51",
          "title": "Learning and Evaluating Contextual Embedding of Source Code"
        },
        {
          "paperId": "199918446424c4cb25146979840a20df93b37168",
          "title": "Encoding Musical Style with Transformer Autoencoders"
        },
        {
          "paperId": "4d08dcd2cc1e9691defe664a10f021424a896a1e",
          "title": "Neural Machine Translation: A Review"
        },
        {
          "paperId": "988236ed9defc9d040a5cc3844849d846c9dbd85",
          "title": "Multi-Scale Self-Attention for Text Classification"
        },
        {
          "paperId": "ebc10cb78438c8ef3f160f6e1e44fa7ecf925a13",
          "title": "Autoencoding undirected molecular graphs with neural networks"
        },
        {
          "paperId": "bb98bc96e02396d199fc899287d9b84393c86e79",
          "title": "Graph Transformer for Graph-to-Sequence Learning"
        },
        {
          "paperId": "0c5bc409e62e65f86838968a2a7cdae5fa0b288b",
          "title": "RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers"
        },
        {
          "paperId": "d715b4a9282562b9d84fb66e04ee70e66b12e86d",
          "title": "Location Attention for Extrapolation to Longer Sequences"
        },
        {
          "paperId": "3f55c542de1e5a19e33cba481dd5460b2a92a266",
          "title": "ConveRT: Efficient and Accurate Conversational Representations from Transformers"
        },
        {
          "paperId": "094a58e8c07395be13d6c92d35533d787906913b",
          "title": "PoD: Positional Dependency-Based Word Embedding for Aspect Term Extraction"
        },
        {
          "paperId": "7b2debe256134ae8bccf94535a33fc220a97ef85",
          "title": "Graph-to-Graph Transformer for Transition-based Dependency Parsing"
        },
        {
          "paperId": "2c7d213b2230dea4ff88f0e50631089d513e9528",
          "title": "Data Diversification: A Simple Strategy For Neural Machine Translation"
        },
        {
          "paperId": "8752d24336f79569a461ad44eb05748a3055f479",
          "title": "Attention Is All You Need for Chinese Word Segmentation"
        },
        {
          "paperId": "3cfb319689f06bf04c2e28399361f414ca32c4b3",
          "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
          "paperId": "463fefdbd81a4a0a32cf59bc58a9545757c8cf2e",
          "title": "Pre-trained Contextual Embedding of Source Code"
        },
        {
          "paperId": "834fb308ca1a0177d303691d6003e3b0fded021e",
          "title": "Reasoning Over Semantic-Level Graph for Fact Checking"
        },
        {
          "paperId": "49f1525c78d42037ffe4cd5a0c451bb7f5eb27b2",
          "title": "Enhancing Machine Translation with Dependency-Aware Self-Attention"
        },
        {
          "paperId": "3827ecf84bc0429b73d9c57a6b2b55e723d4cfba",
          "title": "Dynamic Graph Message Passing Networks"
        },
        {
          "paperId": "eb606d9ce65139754232cee62f6ab77f3e0c665f",
          "title": "Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"
        },
        {
          "paperId": "4ea2a426d2b3c87ea9632c7f017f6883ae238c36",
          "title": "Investigating Self-Attention Network for Chinese Word Segmentation"
        },
        {
          "paperId": "8a8e4fa580a81ba2fcb86965f323709cafcab275",
          "title": "Representation Learning for Dynamic Graphs: A Survey"
        },
        {
          "paperId": "0b2267913519ffe3cdff8681d3c43ac1bb0aa35d",
          "title": "Synchronous Bidirectional Inference for Neural Sequence Generation"
        },
        {
          "paperId": "daae969477adfcd65827192592284a429a0dfaf6",
          "title": "Meta-Amortized Variational Inference and Learning"
        },
        {
          "paperId": "1e21d0480f2c03859461dc7abd1c11b137bd5dcf",
          "title": "THUMT: An Open-Source Toolkit for Neural Machine Translation"
        },
        {
          "paperId": "96424fde14891eb15709fab65577b2b8b146c4ce",
          "title": "Global-and-Local Relative Position Embedding for Unsupervised Video Summarization"
        },
        {
          "paperId": "bfc6ac0cf8d75e758f6c7c6fed735e4a6ebd1e92",
          "title": "L ANGUAGE -A GNOSTIC R EPRESENTATION L EARNING OF S OURCE C ODE FROM S TRUCTURE AND C ONTEXT"
        },
        {
          "paperId": "da1c6421b678c4b9290eb28ad0d36aaf000a19df",
          "title": "融合目标端句法的AMR-to-Text生成(AMR-to-Text Generation with Target Syntax)"
        },
        {
          "paperId": "539268757e7d29201a13e5ca90454eb151d3d022",
          "title": "Abstraction, Generalization, and Embodiment in Neural Program Synthesis"
        },
        {
          "paperId": "e33b3f5011b6745b5a39576445cc1ab3ddfc0bc2",
          "title": "Traduire des corpus pour construire des modèles de traduction neuronaux : une solution pour toutes les langues peu dotées ? (Corpus Translation to Build Translation Models : a Solution for all Low-Resource Languages ?)"
        },
        {
          "paperId": "caf9705291315a86f820bce4fac451a38c993e0a",
          "title": "MQTRANSFORMER: MULTI-HORIZON FORECASTS"
        },
        {
          "paperId": "9ea5313ed74150d3683eb52ea2c4197778b7fdcf",
          "title": "WAVEQ: GRADIENT-BASED DEEP QUANTIZATION"
        },
        {
          "paperId": "6d1d37e716e936b30cb8a51e1489d2b15064295c",
          "title": "Composing Creative Music Via Automatic Note Sequence Generation"
        },
        {
          "paperId": "eae5981cc94639a97b209b21ed39eefa82396e31",
          "title": "Supplementary Material for Object-Centric Learning with Slot Attention"
        },
        {
          "paperId": "13a0bd788e666491b4eca0010d038f95746cdbb3",
          "title": "Supplement: Hybrid Models for Learning to Branch"
        },
        {
          "paperId": "242ea389f6ba7beaef0d99210d016aaa1925e099",
          "title": "NJUNLP’s Machine Translation System for CCMT-2020 Uighur → Chinese Translation Task"
        },
        {
          "paperId": "0b24fee14c814a783fbb911ec1e44f69ff9254c9",
          "title": "NJUNLP’s Machine Translation System for CCMT-2020 Uighur $$\\rightarrow $$ Chinese Translation Task"
        },
        {
          "paperId": "648b8e669139ef5ec285caf5eec08a721c919d7e",
          "title": "Audio frame reconstruction from incomplete observations using Deep Learning techniques"
        },
        {
          "paperId": "4b91247cc0692f34eb3e39485aa3d2d4e8ac9dc5",
          "title": "H2oloo at TREC 2020: When all you got is a hammer... Deep Learning, Health Misinformation, and Precision Medicine"
        },
        {
          "paperId": "40e50558fc995f6fc04fc36353d763275a34bc1a",
          "title": "IMPROVING GRAPH NEURAL NETWORK EXPRESSIV-"
        },
        {
          "paperId": "6533d1a7fb4dfe9241d7e6cc6ff7ee6751e0cc03",
          "title": "TransformerFTC: Scaling Low-Dimensional Transformers for Higher Performance"
        },
        {
          "paperId": "9d7774da269c5193120b7c1c86cd9f3d56fafdfc",
          "title": "Discovering Music Relations with Sequential Attention"
        },
        {
          "paperId": "0dcab5960490df9a6891c01026c23ff622395eeb",
          "title": "Neural Machine Translation with Soft Reordering Knowledge"
        },
        {
          "paperId": "13ba74e528edc8600ff0bc4ab3dd71d892df14c8",
          "title": "An Empirical Exploration of Local Ordering Pre-training for Structured Learning"
        },
        {
          "paperId": "beec5d92f7deec04d82b0af3180863cb0f0d39ef",
          "title": "Automatic Segregation and Classification of Inclusion and Exclusion Criteria of Clinical Trials to Improve Patient Eligibility Matching"
        },
        {
          "paperId": "12c4a31d7d5f618ee7742d22e8777ee77a22a7cc",
          "title": "anarchy ! I want their \" Symbolic Incarceration \" to include a \" Mock Tribunal \" at Gitmo"
        },
        {
          "paperId": "e76c4fb795ba073eb454ca8007b412fe9f33e822",
          "title": "Factored Phrase-based Statistical Machine Pre-training with Extended Transformers"
        },
        {
          "paperId": "d370d25ba348f33bb5a5ff10d78b5dc9630694ec",
          "title": "Stochastic Fine-Grained Labeling of Multi-state Sign Glosses for Continuous Sign Language Recognition"
        },
        {
          "paperId": "83f6f11e3335662ab57efe83844791fab1142053",
          "title": "Research Statement: Context Aware Sequential Model"
        },
        {
          "paperId": "b4a0783965da5b4aab82efe76d6de685dcd017ef",
          "title": "Attention-Based Dynamic Preference Model for Next Point-of-Interest Recommendation"
        },
        {
          "paperId": "a05a614dc4c39a15f708149aa653fa1b696e928d",
          "title": "What do you mean, BERT?"
        },
        {
          "paperId": "fe5e6d7063664f4de2e5a4f0ee6cd00874dc9265",
          "title": "Fusion of discourse structural position encoding for neural machine translation"
        },
        {
          "paperId": "9222fd8b08c64c9bf677b4ce51d22444bd509fd3",
          "title": "Advancing neural language modeling in automatic speech recognition"
        },
        {
          "paperId": "6c13ccaaac8a7a0bf5ccaa5f86ad89ccd2000f81",
          "title": "Table-to-Dialog: Building Dialog Assistants to Chat With People on Behalf of You"
        },
        {
          "paperId": "8af925f4edf45131b5b6fed8aa655089d58692fa",
          "title": "LONG-SHORT RANGE ATTENTION"
        },
        {
          "paperId": "329d1e32d4fbb587ccb3762c866ac63e49ac3c0b",
          "title": "A Hybrid Architecture of Jointly Learning Image Compression and Quality Enhancement with Improved Entropy Minimization"
        },
        {
          "paperId": "29ca1cd39bbc0146eea2616dce0bf1cc95e128bb",
          "title": "An End-to-End Joint Learning Scheme of Image Compression and Quality Enhancement with Improved Entropy Minimization."
        },
        {
          "paperId": "b03cf6324ecf7a295a4aeae5970c88d1a1c3f336",
          "title": "Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection"
        },
        {
          "paperId": "45f0003613cc570b498a3196e5f3abefe82d6378",
          "title": "Transformer-DW: A Transformer Network with Dynamic and Weighted Head"
        },
        {
          "paperId": "5c55e60ffc892ee231952ae514e3dcea86ffb375",
          "title": "Mixtape: Breaking the Softmax Bottleneck Efficiently"
        },
        {
          "paperId": "201fae97e51fb6aea7ed8120147e806e43834de6",
          "title": "Neural Machine Translation: A Review and Survey"
        },
        {
          "paperId": "2a02c967dd9848064bca0aa69ea6c75b3765d0ee",
          "title": "Low-Rank and Locality Constrained Self-Attention for Sequence Modeling"
        },
        {
          "paperId": "3f7bce8bc23ea08739f9bb5a83723b96a33c5cb6",
          "title": "Improving Mandarin End-to-End Speech Synthesis by Self-Attention and Learnable Gaussian Bias"
        },
        {
          "paperId": "3bc53c49ae68adacf2d5be2fa795bcb879e2717a",
          "title": "MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning"
        },
        {
          "paperId": "4bff291cf7fa02a0dbac767aba55d43ad8c59055",
          "title": "What do you mean, BERT? Assessing BERT as a Distributional Semantics Model"
        },
        {
          "paperId": "2e14e84ccec924ed770b58108ad1d9de6f0ca295",
          "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning"
        },
        {
          "paperId": "cd41c23387824ab2a41c85f0309606f8004bc26f",
          "title": "TENER: Adapting Transformer Encoder for Named Entity Recognition"
        },
        {
          "paperId": "beb91a773677872fc21f08722bdcc737bf5917b5",
          "title": "Syntax-Infused Transformer and BERT models for Machine Translation and Natural Language Understanding"
        },
        {
          "paperId": "bafc0d4f9c6f4d05b89e5e12cd9e88b93c70f408",
          "title": "Porous Lattice-based Transformer Encoder for Chinese NER"
        },
        {
          "paperId": "ee1e75dd122ecd6b955145c9e50fac8fd3f23a5f",
          "title": "Data Diversification: An Elegant Strategy For Neural Machine Translation"
        },
        {
          "paperId": "e1e12ee297c9e059e63a5cee520922cd96f70de6",
          "title": "Attention mechanism-based deep learning pan-specific model for interpretable MHC-I peptide binding prediction"
        },
        {
          "paperId": "f93bdf5cd5b63a429f1893e0d4a5b63713554d80",
          "title": "Deep convolutional neural network and attention mechanism based pan-specific model for interpretable MHC-I peptide binding prediction"
        },
        {
          "paperId": "f3f90b45f4d9916d65731a1b11e0c2483605c79f",
          "title": "On the Relation between Position Information and Sentence Length in Neural Machine Translation"
        },
        {
          "paperId": "59e3f0fcd07a44fd1293681528209d0b0d78e75e",
          "title": "Mixed Multi-Head Self-Attention for Neural Machine Translation"
        },
        {
          "paperId": "114745b95c7029c5163b745a37829d1e85fc3083",
          "title": "Recurrent Positional Embedding for Neural Machine Translation"
        },
        {
          "paperId": "ce8cce68366f178ade7e4885fa59c497a0569092",
          "title": "KNU-HYUNDAI’s NMT system for Scientific Paper and Patent Tasks onWAT 2019"
        },
        {
          "paperId": "44c99a42c80518690a54bf2d46d7c1e71f0ab2f8",
          "title": "Our Neural Machine Translation Systems for WAT 2019"
        },
        {
          "paperId": "b134ce39a3a195f5b0c277a1d2c0d776e5ce6a52",
          "title": "Syntax-aware Transformer Encoder for Neural Machine Translation"
        },
        {
          "paperId": "7c60348a8e16f8f73f5dcc0e2a021c99dd089954",
          "title": "Question Mark Prediction By Bert"
        },
        {
          "paperId": "b149f7bf834be85019defb4c96825c0f6861ece7",
          "title": "Inspecting Unification of Encoding and Matching with Transformer: A Case Study of Machine Reading Comprehension"
        },
        {
          "paperId": "dc28420a0f61113d336cc19c07388ba720f96f2f",
          "title": "Improving Generalization of Transformer for Speech Recognition with Parallel Schedule Sampling and Relative Positional Embedding"
        },
        {
          "paperId": "4d1316798f575b564d0bd3da96a8b02be760e21c",
          "title": "An Augmented Transformer Architecture for Natural Language Generation Tasks"
        },
        {
          "paperId": "37a23c43ddf09ea97b82b38e2827a2229cfae545",
          "title": "Novel positional encodings to enable tree-based transformers"
        },
        {
          "paperId": "725ba44104f2fd6a681235c503580f4640ed07fc",
          "title": "Syntax-Enhanced Self-Attention-Based Semantic Role Labeling"
        },
        {
          "paperId": "0708d8eb3c0b328fcfeccd2f0e3d6af1722c4825",
          "title": "Dependency-Based Relative Positional Encoding for Transformer NMT"
        },
        {
          "paperId": "e17c7daa6f3bbc8ce7812e8dffdf61e3aea2edb3",
          "title": "You May Not Need Order in Time Series Forecasting"
        },
        {
          "paperId": "4d68c1b4167f858979c6a8e8b9ad0b484cd48c63",
          "title": "Cross Aggregation of Multi-head Attention for Neural Machine Translation"
        },
        {
          "paperId": "a3a76d6b77f2b44ffd9dd0c953345aa71b419cfb",
          "title": "Improving Transformer with Sequential Context Representations for Abstractive Text Summarization"
        },
        {
          "paperId": "7455f5f3a524fea54be74fc002266eca0f003a91",
          "title": "Neural Language Priors"
        },
        {
          "paperId": "a5a9b1ee9e4b3db4a3237b3cddd2630d3333abfb",
          "title": "Towards Controllable and Personalized Review Generation"
        },
        {
          "paperId": "d492084e932e7ca004f05c97dc8434300cff5ced",
          "title": "NiuTrans Submission for CCMT19 Quality Estimation Task"
        },
        {
          "paperId": "24433d65fb2b3df2bfc500861fa801ba04622028",
          "title": "Improving Semantic Parsing with Neural Generator-Reranker Architecture"
        },
        {
          "paperId": "34e75b1eb986ceab79e5c463cf1a82bcf4e87944",
          "title": "Non-autoregressive Transformer by Position Learning"
        },
        {
          "paperId": "933f118f35875e97f708f570f77462a11de38e42",
          "title": "Cross-Lingual Dependency Parsing with Unlabeled Auxiliary Languages"
        },
        {
          "paperId": "b26bfc6fa8aff11181cf1231e36752e8c33e080c",
          "title": "Improving Neural Machine Translation with Parent-Scaled Self-Attention"
        },
        {
          "paperId": "429236d6daee722fd2528d7dd85fc646796c813e",
          "title": "Multi-agent Learning for Neural Machine Translation"
        },
        {
          "paperId": "4ca4e7ed290617aab8b4d99294d10b0ac8372967",
          "title": "Self-Attention with Structural Position Representations"
        },
        {
          "paperId": "6c1beae31b92c70b42ebeb99e5598d73bff6eea5",
          "title": "NEZHA: Neural Contextualized Representation for Chinese Language Understanding"
        },
        {
          "paperId": "a99855603e7689a4529a1573f0bc4716adb246c1",
          "title": "Improving Multi-Head Attention with Capsule Networks"
        },
        {
          "paperId": "8cef9900c04d7f661c08f4b5b1ed4337ace042a3",
          "title": "Transformer Dissection: An Unified Understanding for Transformer’s Attention via the Lens of Kernel"
        },
        {
          "paperId": "e0f41a30fe692c76e9a27396b9494f2e017dd333",
          "title": "Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension"
        },
        {
          "paperId": "a20931dd89bb3d77f7f0e577ba4126bb756fe76d",
          "title": "Multiresolution Transformer Networks: Recurrence is Not Essential for Modeling Hierarchical Structure"
        },
        {
          "paperId": "19f79feb82070362eb0956f6378719a9113de5c3",
          "title": "Enhancing Neural Sequence Labeling with Position-Aware Self-Attention"
        },
        {
          "paperId": "ddbe99d66a112dd58de835462a60ebfaef401ad8",
          "title": "Recurrent Graph Syntax Encoder for Neural Machine Translation"
        },
        {
          "paperId": "c5db8d349238d918eafecf61641c81a2fe6f21c3",
          "title": "Interactive Variance Attention based Online Spoiler Detection for Time-Sync Comments"
        },
        {
          "paperId": "3cdaa387b08306012a28a1746d59f09941b7ed82",
          "title": "Universal Transforming Geometric Network"
        },
        {
          "paperId": "afc1824a051f686e18ad87e1244bb0926a361021",
          "title": "Modeling Graph Structure in Transformer for Better AMR-to-Text Generation"
        },
        {
          "paperId": "4c4ecd0385f638e069186b1073a6a0ae22c15d61",
          "title": "Spatio-Temporal Attentive RNN for Node Classification in Temporal Attributed Graphs"
        },
        {
          "paperId": "ba298699c4661a1810d4c27b8b911c52c5d088ff",
          "title": "Modeling Source Syntax and Semantics for Neural AMR Parsing"
        },
        {
          "paperId": "efe4902a39c8ef332058ae7d156a6bffcd3c1341",
          "title": "Token-level Dynamic Self-Attention Network for Multi-Passage Reading Comprehension"
        },
        {
          "paperId": "7897ac005ed9fd26fa0f737afef8d8b7517bf2b0",
          "title": "Self-Attentive Hawkes Processes"
        },
        {
          "paperId": "d6ce0773fb5a9447f2646fe4d275d1834d0924cf",
          "title": "A Human-Like Semantic Cognition Network for Aspect-Level Sentiment Classification"
        },
        {
          "paperId": "dc02301e84404ea69eba49fc0028e230a24750b8",
          "title": "An External Knowledge Enhanced Multi-label Charge Prediction Approach with Label Number Learning"
        },
        {
          "paperId": "830995ef17cc291c13f42dfd9f462137de1d2179",
          "title": "Augmenting Self-attention with Persistent Memory"
        },
        {
          "paperId": "7d67237398986a6088c696df0bf57646c714508f",
          "title": "Look Harder: A Neural Machine Translation Model with Hard Attention"
        },
        {
          "paperId": "865f5167c4353d2b120f0469ed1c298bc92794fa",
          "title": "Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions"
        },
        {
          "paperId": "957f53525b386d6f31077449c8ec23b83c19f506",
          "title": "Neural Machine Translation with Reordering Embeddings"
        },
        {
          "paperId": "332a3387559384d588e5ceddd64c7bd50c6c39ca",
          "title": "Encoding Database Schemas with Relation-Aware Self-Attention for Text-to-SQL Parsers"
        },
        {
          "paperId": "c4cb9f42bda982974edad239d1fc292d68efa18b",
          "title": "A Study on Self-attention Mechanism for AMR-to-text Generation"
        },
        {
          "paperId": "735ce0447e459e13f89ae751b19323d76a2af786",
          "title": "Program Synthesis and Semantic Parsing with Learned Code Idioms"
        },
        {
          "paperId": "a4d29e912ef6014e355649c1f9f822b9bf3d14e3",
          "title": "Lattice Transformer for Speech Translation"
        },
        {
          "paperId": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d",
          "title": "Stand-Alone Self-Attention in Vision Models"
        },
        {
          "paperId": "81e1d123a85562555befb0243256b1a0d9fca014",
          "title": "Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View"
        },
        {
          "paperId": "a39398f68ae7e042f2ef5009e31b4e6a20fd5736",
          "title": "Learning Deep Transformer Models for Machine Translation"
        },
        {
          "paperId": "0ab0fda8774c303be8f8f8c8f684a890dcf5d455",
          "title": "Lattice-Based Transformer Encoder for Neural Machine Translation"
        },
        {
          "paperId": "f89d2da991935549b109d780be3351e0dda92a8f",
          "title": "Assessing the Ability of Self-Attention Networks to Learn Word Order"
        },
        {
          "paperId": "949fef650da4c41afe6049a183b504b3cc91f4bd",
          "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences"
        },
        {
          "paperId": "2796a197926854a6316e51d598db86f87516c796",
          "title": "Attentional Policies for Cross-Context Multi-Agent Reinforcement Learning"
        },
        {
          "paperId": "f89ecf4e7ef689db10edd082eea3fea93604e1f3",
          "title": "Relational Representation Learning for Dynamic (Knowledge) Graphs: A Survey"
        },
        {
          "paperId": "ec578b5bba19c604116794c374a3f5df9a7aa261",
          "title": "Deep Learning for Music Composition: Generation, Recommendation and Control"
        },
        {
          "paperId": "8b53d0d64a118ac80936750dc77c9f633fe7a1a5",
          "title": "Generating Logical Forms from Graph Representations of Text and Entities"
        },
        {
          "paperId": "f4238bd2385a52413ccbacfd9e409a650235bd13",
          "title": "Adaptive Attention Span in Transformers"
        },
        {
          "paperId": "b0f0a5a21619d70748a4dc007983cc111f1b301e",
          "title": "Joint Source-Target Self Attention with Locality Constraints"
        },
        {
          "paperId": "3928b2177086532775fbf607ae3e05a0375a5061",
          "title": "Language Modeling with Deep Transformers"
        },
        {
          "paperId": "454bf7bf8217498c1e840e53090f0bb00a99bdb8",
          "title": "Neural-Attentional Architectures for Deep Multi-Agent Reinforcement Learning in Varying Environments"
        },
        {
          "paperId": "fb507ada871d1e8c29e376dbf7b7879689aa89f9",
          "title": "Music Transformer: Generating Music with Long-Term Structure"
        },
        {
          "paperId": "56605486faa2281fd24a076d15505b5791d9b7f0",
          "title": "Enhancing Hybrid Self-attention Structure with Relative-position-aware Bias for Speech Synthesis"
        },
        {
          "paperId": "27ac832ee83d8b5386917998a171a0257e2151e2",
          "title": "Attention Augmented Convolutional Networks"
        },
        {
          "paperId": "690edf44e8739fd80bdfb76f40c9a4a222f3bba8",
          "title": "BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer"
        },
        {
          "paperId": "18d41e3bd94cf38736e37580912c3b4ba56f08d5",
          "title": "An Empirical Study of Spatial Attention Mechanisms in Deep Networks"
        }
      ],
      "references": [
        {
          "paperId": "33998aff64ce51df8dee45989cdca4b6b1329ec4",
          "title": "Graph Attention Networks"
        },
        {
          "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
          "title": "Attention is All you Need"
        },
        {
          "paperId": "43428880d75b3a14257c3ee9bda054e61eb869c0",
          "title": "Convolutional Sequence to Sequence Learning"
        },
        {
          "paperId": "98445f4172659ec5e891e031d8202c102135c644",
          "title": "Neural Machine Translation in Linear Time"
        },
        {
          "paperId": "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd",
          "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
        },
        {
          "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
          "title": "Layer Normalization"
        },
        {
          "paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321",
          "title": "A Decomposable Attention Model for Natural Language Inference"
        },
        {
          "paperId": "23ffaa0fe06eae05817f527a47ac3291077f9e58",
          "title": "Rethinking the Inception Architecture for Computer Vision"
        },
        {
          "paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0",
          "title": "Effective Approaches to Attention-based Neural Machine Translation"
        },
        {
          "paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e",
          "title": "End-To-End Memory Networks"
        },
        {
          "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
          "title": "Adam: A Method for Stochastic Optimization"
        },
        {
          "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
          "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
          "paperId": null,
          "title": "End-to-end memory networks. In Advances in neural information processing systems"
        },
        { "paperId": null, "title": "End - toend memory networks" },
        {
          "paperId": null,
          "title": "Effective approaches to attentionbased neural machine translation"
        },
        {
          "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
          "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
          "paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e",
          "title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
        }
      ],
      "summary": "This work presents an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements, on the WMT 2014 English-to-German and English- to-French translation tasks.",
      "title": "Self-Attention with Relative Position Representations",
      "venue": "NAACL",
      "year": 2018,
      "authors": "Peter Shaw,Jakob Uszkoreit,Ashish Vaswani"
    },
    {
      "id": "b626754a0fd7de12c87e88165b2484ac5d98212a",
      "url": "https://www.semanticscholar.org/paper/b626754a0fd7de12c87e88165b2484ac5d98212a",
      "citationCount": 77,
      "influentialCitationCount": 24,
      "referenceCount": 33,
      "citations": [
        {
          "paperId": "c44176020bc7034f5ea788cb8de7fcdda5f6a91d",
          "title": "Social Influence Dialogue Systems: A Scoping Survey of the Efforts Towards Influence Capabilities of Dialogue Systems"
        },
        {
          "paperId": "4580b0e3ede1b7ad97fb742fa28cd3ebe3bed7a9",
          "title": "Improving Policy Learning via Language Dynamics Distillation"
        },
        {
          "paperId": "b1708d989527a0c63649e223760da12b399da147",
          "title": "Multimodal Persuasive Dialogue Corpus using Teleoperated Android"
        },
        {
          "paperId": "43e558c408fef659631573c1cbbf81f58decf5df",
          "title": "Intelligent Negotiation Bot using Machine Learning Techniques"
        },
        {
          "paperId": "d348561af0832b83652dfa31fed41f93a41ff9b0",
          "title": "Persuasion Strategies in Advertisements: Dataset, Modeling, and Baselines"
        },
        {
          "paperId": "42ffe1139f97ce3809c9140e2d92e837fe5eb1e7",
          "title": "Structured and Natural Responses Co-generation for Conversational Search"
        },
        {
          "paperId": "ca1d78a0e30a554540ad5c1c7c84d87d780f9ade",
          "title": "How to Ask for Donations? Learning User-Specific Persuasive Dialogue Policies through Online Interactions"
        },
        {
          "paperId": "680536428d734b28f23c86a7d4e53dd03144a752",
          "title": "Do You Know My Emotion? Emotion-Aware Strategy Recognition towards a Persuasive Dialogue System"
        },
        {
          "paperId": "d1f58b960a4beafa0f7c803bfbe19c3572988e27",
          "title": "Unsupervised Learning of Hierarchical Conversation Structure"
        },
        {
          "paperId": "a1948d9322c7a8d88168fdffac0800940da5e7e8",
          "title": "Towards a Progression-Aware Autonomous Dialogue Agent"
        },
        {
          "paperId": "304c860cd33f6630fc2fb0d14b6f3ca62b0fa7dc",
          "title": "Opponent Modeling in Negotiation Dialogues by Related Data Adaptation"
        },
        {
          "paperId": "88a9924ba302f9c5c8065d9e38798e9506c5b171",
          "title": "CHAI: A CHatbot AI for Task-Oriented Dialogue with Offline Reinforcement Learning"
        },
        {
          "paperId": "81ebda8a734ff917681c6d73c179f6614b281818",
          "title": "Context-Aware Language Modeling for Goal-Oriented Dialogue Systems"
        },
        {
          "paperId": "02b31611637db09856fa56f4e799d0f3b5674563",
          "title": "Interacting with Non-Cooperative User: A New Paradigm for Proactive Dialogue Policy"
        },
        {
          "paperId": "237f5ca6fcccef2b77a2212b34fb06a1dbd09b72",
          "title": "Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting"
        },
        {
          "paperId": "9a1c65b395974ee4c8f073f26d2a47b8492989e7",
          "title": "Seamlessly Integrating Factual Information and Social Content with Persuasive Dialogue"
        },
        {
          "paperId": "3517bed8f1fa9ccdd075cbc4ded1467b09e4c199",
          "title": "A Wizard of Oz Study Simulating API Usage Dialogues With a Virtual Assistant"
        },
        {
          "paperId": "98cb29c03a0882fde368db28ade214c57c3239d6",
          "title": "Multi-agent deep reinforcement learning: a survey"
        },
        {
          "paperId": "ae4da8cb36e40fa167f565cbd4df3526f01322c7",
          "title": "Conversational AI for Positive-sum Retailing under Falsehood Control"
        },
        {
          "paperId": "b657861d3b96ff0e5a502ddc2cb865a5ea845f53",
          "title": "Predicting Success of a Persuasion through Joint Modeling of Utterance Categorization"
        },
        {
          "paperId": "3451d237f29243d63c846e99f74558aa441d649c",
          "title": "Improved Goal Oriented Dialogue via Utterance Generation and Look Ahead"
        },
        {
          "paperId": "5931c8ac145baf17cec9effc25c051049b7dfd4c",
          "title": "Reference-Centric Models for Grounded Collaborative Dialogue"
        },
        {
          "paperId": "bab1b89c4b69b9661037114d45af68d26d3cdd70",
          "title": "WeaSuL: Weakly Supervised Dialogue Policy Learning: Reward Estimation for Multi-turn Dialogue"
        },
        {
          "paperId": "7f3fb456319181ee092b4e335302fb953523aaba",
          "title": "Towards Emotion-Aware Agents For Negotiation Dialogues"
        },
        {
          "paperId": "c1361b0fe1140f02051eeb68d86f07993af9a6a9",
          "title": "Dialogue Management for Interactive API Search"
        },
        {
          "paperId": "b89c361f37657fb4fab246e9d55b3c6f32e29713",
          "title": "Targeted Data Acquisition for Evolving Negotiation Agents"
        },
        {
          "paperId": "751966cbac5707e42264cba590eed735a3c57f14",
          "title": "DeliData: A dataset for deliberation in multi-party problem solving"
        },
        {
          "paperId": "2995b6175e6ad666c33273722f33a86d9a08e46b",
          "title": "Semantic-Enhanced Explainable Finetuning for Open-Domain Dialogues"
        },
        {
          "paperId": "a5a853fbcca89ec308511527e1614fac701681e8",
          "title": "Conversational AI Systems for Social Good: Opportunities and Challenges"
        },
        {
          "paperId": "8c0ec929aac2eb67d36f383f3175f22a1b285264",
          "title": "Action-Based Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented Dialogue Systems"
        },
        {
          "paperId": "37607828cb9b8ae5011bbcc8ecc2e159f719347c",
          "title": "CaSiNo: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems"
        },
        {
          "paperId": "17fdf700dcd6e8621ef4d9aa4ae69d1ad7a2043e",
          "title": "ResPer: Computationally Modelling Resisting Strategies in Persuasive Conversations"
        },
        {
          "paperId": "e77932feb60d4c91bcb3d862d2b30e4ec8fb7821",
          "title": "A Neural Question Answering System for Basic Questions about Subroutines"
        },
        {
          "paperId": "774f067f645a481b3c36b4bfe26ae707b61ce84f",
          "title": "Refine and Imitate: Reducing Repetition and Inconsistency in Persuasion Dialogues via Reinforcement Learning and Human Demonstration"
        },
        {
          "paperId": "7bf03c12f74a83852fc831c468aa754e0008a6d7",
          "title": "Improving Dialog Systems for Negotiation with Personality Modeling"
        },
        {
          "paperId": "4fd961ec9a6af0c0d15e0a85471d0ff596342ac5",
          "title": "Modelling Hierarchical Structure between Dialogue Policy and Natural Language Generator with Option Framework for Task-oriented Dialogue System"
        },
        {
          "paperId": "8a9a798c56fc83858d7ace0352606d73aeaa204d",
          "title": "Adversarial Language Games for Advanced Natural Language Intelligence"
        },
        {
          "paperId": "2785ec102c3d2888ae9d29c8d3fba2a666b5b3af",
          "title": "Dialogue Context Encoder Structure Encoder Graph Encoding ( GAT ) Structure Encoder u 1 u 2 u 3 u 4 Graph Pooling Graph Pooling Graph Encoding ( GAT ) GCN-ASAPGCN-ASAP Utterance Embedding Utterance Generation"
        },
        {
          "paperId": "fb367084876f836bcb33b710f0dd366af69513a1",
          "title": "Dialogue Act-based Breakdown Detection in Negotiation Dialogues"
        },
        {
          "paperId": "80d5684c1e7f5ae34b524ed25dd51f2aa2b613c9",
          "title": "Learning Grounded Pragmatic Communication"
        },
        {
          "paperId": "d6981d415ab968fb42f3ba145b2c389d015b24d5",
          "title": "WeaSuL: Weakly Supervised Dialogue Policy Learning: Reward Estimation for Multi-turn Dialogue"
        },
        {
          "paperId": "476dfdfae0cf07fdf4c7a3609763cddc1b48a6e7",
          "title": "ORIENTED DIALOGUE SYSTEM"
        },
        {
          "paperId": "d0babb55df48d544f4765a29fba7fd6c854d03ba",
          "title": "Phrase-Level Action Reinforcement Learning for Neural Dialog Response Generation"
        },
        {
          "paperId": "aea24fe9653bcb9d654e53bdfc66b810d79d0a2c",
          "title": "Task-Completion Dialogue Policy Learning via Monte Carlo Tree Search with Dueling Network"
        },
        {
          "paperId": "479f50023b4c4d95df7512ebf1d2723508821be5",
          "title": "A Multi-Persona Chatbot for Hotline Counselor Training"
        },
        {
          "paperId": "314358d6e9969cb89da0ad0b6aa4f406294d3ff4",
          "title": "Generalized Conditioned Dialogue Generation Based on Pre-trained Language Model"
        },
        {
          "paperId": "3fa25dc99305113f1d28051094ebefd843f5281b",
          "title": "Generating Strategic Dialogue for Negotiation with Theory of Mind"
        },
        {
          "paperId": "25ccaf7d0220423b58561970cb5e0bb5ebcf5b75",
          "title": "Learning to Plan and Realize Separately for Open-Ended Dialogue Systems"
        },
        {
          "paperId": "abb59785aee6e5732c64ba7364fcbbb277e97bb6",
          "title": "Preliminary Annotation Results: Bargaining Roles for Bilateral Dialogues"
        },
        {
          "paperId": "a4789304d4f5337d7c1e18dcc0f13049f2b5dbc5",
          "title": "Conversational Graph Grounded Policy Learning for Open-Domain Conversation Generation"
        },
        {
          "paperId": "de18584421e2192623dcc3e35cf4294d7d42e1aa",
          "title": "GoChat: Goal-oriented Chatbots with Hierarchical Reinforcement Learning"
        },
        {
          "paperId": "bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d",
          "title": "Experience Grounds Language"
        },
        {
          "paperId": "de094826a8d25994f103e207689d6eb1b595642e",
          "title": "Dynamic Composition for Conversational Domain Exploration"
        },
        {
          "paperId": "716efab73e1916fdc2a23727b581d200271ed499",
          "title": "BERT in Negotiations: Early Prediction of Buyer-Seller Negotiation Outcomes"
        },
        {
          "paperId": "c7ee2561a77673a2ba4eb83fa6f484f8ac72db7d",
          "title": "Breakdown Detection in Negotiation Dialogues (Student Abstract)"
        },
        {
          "paperId": "4d0146679527de1208d801d253ab30f9de92bd39",
          "title": "Modeling Dialogues with Hashcode Representations: A Nonparametric Approach"
        },
        {
          "paperId": "c4e8f111b4cb6cdfe0f718d0d81ca138c0e9464e",
          "title": "Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition"
        },
        {
          "paperId": "333df5eed6cc5a0a511a37726a1652c731a33b56",
          "title": "Recent Advances and Challenges in Task-oriented Dialog System"
        },
        {
          "paperId": "3c0ad47e15c1ff9761aee446bb132d816973aedf",
          "title": "Effects of Persuasive Dialogues: Testing Bot Identities and Inquiry Strategies"
        },
        {
          "paperId": "c9baad6c2d6c112e758e9d9a2f3287622c21a19b",
          "title": "End-to-End Trainable Non-Collaborative Dialog System"
        },
        {
          "paperId": "7b622d7a991da4d2091d7bdb420177ca596f13a8",
          "title": "Augmenting Non-Collaborative Dialog Systems with Explicit Semantic and Strategic Dialog History"
        },
        {
          "paperId": "e9055015b403098f60769a447134e30d5767d583",
          "title": "MOSS: End-to-End Dialog System Framework with Modular Supervision"
        },
        {
          "paperId": "cf952005c8de87a9cf11801a3201829eb66249cd",
          "title": "IEC: Towards Interest-Eliciting Neural Conversational Agents"
        },
        {
          "paperId": "be2ce82730600d9b2eb2df9f2762f9d4beb6222d",
          "title": "Executing Instructions in Situated Collaborative Interactions"
        },
        {
          "paperId": "e7a7de829b11dce0a9da38ebe90371d786c00e87",
          "title": "Recommendation as a Communication Game: Self-Supervised Bot-Play for Goal-oriented Dialogue"
        },
        {
          "paperId": "97398a7e0a46ffe89fb41fa3ff93a3da32febe57",
          "title": "How to Build User Simulators to Train RL-based Dialog Systems"
        },
        {
          "paperId": "f60a65c9c287b44bd864abe2f87d49e44294ff5f",
          "title": "A Dynamic Strategy Coach for Effective Negotiation"
        },
        {
          "paperId": "877ddea1117b0bfad645d42e3baeab165a659919",
          "title": "Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog"
        },
        {
          "paperId": "3d52d429b4d83d096dd354e8470bf3655e8b67bc",
          "title": "Persuasion for Good: Towards a Personalized Persuasive Dialogue System for Social Good"
        },
        {
          "paperId": "06236fa6fd407f4b629be41049c880a8569c20ab",
          "title": "Target-Guided Open-Domain Conversation"
        },
        {
          "paperId": "e191b7eca9cf259abab3905dab1a54d1be091a7a",
          "title": "Designing a Symbolic Intermediate Representation for Neural Surface Realization"
        },
        {
          "paperId": "88fbcfc01bab901aae084d734d82558d34105ea0",
          "title": "Show, Price and Negotiate: A Hierarchical Attention Recurrent Visual Negotiator"
        },
        {
          "paperId": "590d2e1c66c9f11575df4ea9dc63d605b1c05e05",
          "title": "Rethinking Action Spaces for Reinforcement Learning in End-to-end Dialog Agents with Latent Variable Models"
        },
        {
          "paperId": "a0a0dbdfb7d88c8e38b9d8088ddf194195c2e6c7",
          "title": "AUGMENTING NON-COLLABORATIVE DIALOG SYS-"
        },
        {
          "paperId": "67aacf323a4db1a0a82ffa19336b2842e4608d8a",
          "title": "Seller Messages Buyer Product Description Listing Price Coach : Product Listing : Detect Strategies 2 1 2 3 5 Realize Strategies Select Optimal Strategies 4 3 Predict Strategies"
        },
        {
          "paperId": "e65594a323f979d3366b108fa30bddde5a9f8d90",
          "title": "Modeling Psychotherapy Dialogues with Kernelized Hashcode Representations: A Nonparametric Information-Theoretic Approach."
        },
        {
          "paperId": "4ad44c9af940ff630f395624b932d778b5e05a74",
          "title": "Dialogue Modeling Via Hash Functions: Applications to Psychotherapy"
        }
      ],
      "references": [
        {
          "paperId": "92ec8259e58f5107057e4e99fdcd03df6a08f9e7",
          "title": "Emergent Communication through Negotiation"
        },
        {
          "paperId": "6a8dbea5e40831bd6e987c03b76487f45ac49599",
          "title": "Deal or No Deal? End-to-End Learning of Negotiation Dialogues"
        },
        {
          "paperId": "77826df024f97583eb05700a28e11056a4aab848",
          "title": "Latent Intention Dialogue Models"
        },
        {
          "paperId": "fcbedb12dcc3618df7a76012944c64fd62a18286",
          "title": "Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings"
        },
        {
          "paperId": "bca1b5e690907843dff3b16d738268cd7db53430",
          "title": "Evaluating Persuasion Strategies and Deep Reinforcement Learning methods for Negotiation Dialogue agents"
        },
        {
          "paperId": "58dfeb0bc41429393bf27ff882b7b679031f106c",
          "title": "Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders"
        },
        {
          "paperId": "aea427bcea46c83021e62f4fb10178557510ca5d",
          "title": "Latent Variable Dialogue Models and their Diversity"
        },
        {
          "paperId": "9c49763e37c20007ebbddbe405b02546705a2d83",
          "title": "Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning"
        },
        {
          "paperId": "d261286b18d37aaf5ebf70a0325e43fb5357442d",
          "title": "Training End-to-End Dialogue Systems with the Ubuntu Dialogue Corpus"
        },
        {
          "paperId": "176f1d608b918eec8dc4b75e7b6e0acaba84a447",
          "title": "Adversarial Learning for Neural Dialogue Generation"
        },
        {
          "paperId": "fb32191ec07ba4d7badc76ca428c816995b5785a",
          "title": "End-to-End Reinforcement Learning of Dialogue Agents for Information Access"
        },
        {
          "paperId": "f81be44000814e7bcb12ae04b4e2d9c01b6515b3",
          "title": "Learning End-to-End Goal-Oriented Dialog"
        },
        {
          "paperId": "0a22389bd99b7efe3627ec6fc77ddaf3ff5e2faa",
          "title": "A Network-based End-to-End Trainable Task-oriented Dialogue System"
        },
        {
          "paperId": "1298dae5751fb06184f6b067d1503bde8037bdb7",
          "title": "Deep Reinforcement Learning for Dialogue Generation"
        },
        {
          "paperId": "4d025da955a51350df56a54bbb59c1c1b7630e2b",
          "title": "Discourse Structure and Dialogue Acts in Multiparty Dialogue: the STAC Corpus"
        },
        {
          "paperId": "4ba339bd571585fadb1fb1d14ef902b6784f574f",
          "title": "The Dialog State Tracking Challenge Series: A Review"
        },
        {
          "paperId": "e6926644f7afc4226c003048b27d47af72a57c9c",
          "title": "Strategic Dialogue Management via Deep Reinforcement Learning"
        },
        {
          "paperId": "feb4245aa7eaec9eaf225cc0d0676e41b9308a37",
          "title": "Reinforcement Learning in Multi-Party Trading Dialog"
        },
        {
          "paperId": "5247a6e3a60ff0381355e66bfc313bf27512ae0c",
          "title": "A Neural Network Approach to Context-Sensitive Generation of Conversational Responses"
        },
        {
          "paperId": "f30b6b28805513bb0a6eec8db69116a9b5538a81",
          "title": "Toward Natural Turn-Taking in a Virtual Human Negotiation Agent"
        },
        {
          "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
          "title": "GloVe: Global Vectors for Word Representation"
        },
        {
          "paperId": "101d619b5911e9c2fda6f02365c593ae61617cb6",
          "title": "Reinforcement Learning of Cooperative Persuasive Dialogue Policies using Framing"
        },
        {
          "paperId": null,
          "title": "Learning noncooperative dialogue behaviours"
        },
        {
          "paperId": "84b520a8d6de79f62bb095b565d077e95bfb6f5b",
          "title": "POMDP-Based Statistical Spoken Dialog Systems: A Review"
        },
        {
          "paperId": "411654b5805de7eb554587d917b8bd4d01710323",
          "title": "Modelling Strategic Conversation: model, annotation design and corpus"
        },
        {
          "paperId": "2352b3a6e4adc7b010ee5de424079480e3afbfbf",
          "title": "Goal-driven Answers in the Cards Dialogue Corpus"
        },
        {
          "paperId": "184ecd47d94d27711fe81c168b23096766a7b47a",
          "title": "A Cultural Decision-Making Model for Negotiation based on Inverse Reinforcement Learning"
        },
        {
          "paperId": "0fe2e9dea7df47bad029e2072e25ca1fddf80371",
          "title": "Goal-Driven Answers in the CardsDialogue Corpus"
        },
        {
          "paperId": "413c1142de9d91804d6d11c67ff3fed59c9fc279",
          "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization"
        },
        {
          "paperId": "7a3db5fe8dc49d893851e4bc0ffa9d87944c8cea",
          "title": "Multi-party, Multi-issue, Multi-strategy Negotiation for Multi-modal Virtual Agents"
        },
        {
          "paperId": "87b2f3ab89fc9231cfa6f8a17f4253de67c4c3d8",
          "title": "Learning Mixed Initiative Dialog Strategies By Using Reinforcement Learning On Both Conversants"
        },
        {
          "paperId": "4c915c1eecb217c123a36dc6d3ce52d12c742614",
          "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning"
        },
        {
          "paperId": "b2c340691a2ced929772c84eeb920f69c89aa5b7",
          "title": "Negotiation games : applying game theory to bargaining and arbitration"
        }
      ],
      "summary": "A modular approach based on coarse dialogue acts (e.g., propose(price=50)) that decouples strategy and generation that can flexibly set the strategy using supervised learning, reinforcement learning, or domain-specific knowledge without degeneracy is proposed.",
      "title": "Decoupling Strategy and Generation in Negotiation Dialogues",
      "venue": "EMNLP",
      "year": 2018,
      "authors": "He He,Derek Chen,Anusha Balakrishnan,Percy Liang"
    },
    {
      "id": "cabb0a468af8184e0e930841435b65679b580521",
      "url": "https://www.semanticscholar.org/paper/cabb0a468af8184e0e930841435b65679b580521",
      "citationCount": 34,
      "influentialCitationCount": 0,
      "referenceCount": 23,
      "citations": [
        {
          "paperId": "90e39ec249b42a23ee9847c45aa21b1fc3c6d22f",
          "title": "An Overview of Financial Technology Innovation"
        },
        {
          "paperId": "251c3afbaafcc9b5178534be9109f644bfc5e912",
          "title": "Enhancing Knowledge Bases with Quantity Facts"
        },
        {
          "paperId": "fa118351f9088723499d89cc754c1caea4be16d2",
          "title": "Twitter-aided decision making: a review of recent developments"
        },
        {
          "paperId": "e722bcf6a706259dbae2950e385141a1ab7c4a3d",
          "title": "WUST at NTCIR-16 FinNum-3 Task"
        },
        {
          "paperId": "4b470b68603b86582779fa33e5f034116c73685c",
          "title": "Overview of the NTCIR-16 FinNum-3 Task: Investor’s and Manager’s Fine-grained Claim Detection"
        },
        {
          "paperId": "01f68682c88c23376dcbf194c846232cba5f28a6",
          "title": "Distilling Numeral Information for Volatility Forecasting"
        },
        {
          "paperId": "320c1c6647a5b975c901347f71638c881888686b",
          "title": "NQuAD: 70,000+ Questions for Machine Comprehension of the Numerals in Text"
        },
        {
          "paperId": "40a1f266bb5ca853837355bfff272a55f0049c81",
          "title": "Mining Numbers in Text: A Survey"
        },
        {
          "paperId": "c9343c26a0e604f7afd94b7290bbdf8d96cd65b6",
          "title": "Predicting Numerals in Natural Language Text Using a Language Model Considering the Quantitative Aspects of Numerals"
        },
        {
          "paperId": "2e4aeff09548d160f91cb95b54cbc02ef53925b6",
          "title": "Evaluating the Rationales of Amateur Investors"
        },
        {
          "paperId": "28a5a53dafacebad8a7c47773079caeffb9a5baa",
          "title": "Representing Numbers in NLP: a Survey and a Vision"
        },
        {
          "paperId": "2ecde642d9aa18d0b9b385ca76ab2520a66b5ace",
          "title": "Financial Opinion Mining"
        },
        {
          "paperId": "0cb1f68586f1d0e1f4a970b902b33f4cd5a6c7cd",
          "title": "Is Domain Adaptation Worth Your Investment? Comparing BERT and FinBERT on Financial Tasks"
        },
        {
          "paperId": "dd97b0b041587fe73d4dbfc8a72e2cb615e85cb8",
          "title": "Numerals in Financial Narratives"
        },
        {
          "paperId": "3b458728091590afaa9a2b0d86444b5fa6c80275",
          "title": "Sources and Corpora"
        },
        {
          "paperId": "7a318a00d61b1f063e954061e3bec35faf84b372",
          "title": "Organizing Financial Opinions"
        },
        {
          "paperId": "0772212420238b48233256a59912f8e6a31cde3d",
          "title": "NumClaim: Investor's Fine-grained Claim Detection"
        },
        {
          "paperId": "a5ae9f992264908e51c7925280f42ee17a500858",
          "title": "NLP in FinTech Applications: Past, Present and Future"
        },
        {
          "paperId": "c4295b53a1f6c4d609d6dfabd7e08d4b416b02e4",
          "title": "Issues and Perspectives from 10,000 Annotated Financial Social Media Data"
        },
        {
          "paperId": "45ed7728a9b863d4c10efb53a8a4423ab7e67378",
          "title": "Predictive Analytics on Emotional Data Mined from Digital Social Networks with a Focus on Financial Markets"
        },
        {
          "paperId": "e35a35e92e8b45c4826173dfb92440685189e025",
          "title": "MIG at the NTCIR-15 FinNum-2 Task: Use the Transfer Learning and Feature Engineering for Numeral Attachment Task"
        },
        {
          "paperId": "fae743a3c57699793f59b81ac28b6a54015bcc82",
          "title": "TMUNLP at the NTCIR-15 FinNum-2"
        },
        {
          "paperId": "d84fa6b830ebb6188042d66aa850fe5ddcfbb2d4",
          "title": "Next Cashtag Prediction on Social Trading Platforms with Auxiliary Tasks"
        },
        {
          "paperId": "0b078d4bbed8bcfd57fdab6da40ecd8a9601fc62",
          "title": "Crowd View: Converting Investors' Opinions into Indicators"
        },
        {
          "paperId": "2a0fc534ebf1b22b5851f5b1fb781dd707f7c530",
          "title": "Numeral Attachment with Auxiliary Tasks"
        },
        {
          "paperId": "11bb73c6cdec9c5ce579194c26ad74c722e32f95",
          "title": "Numeracy-600K: Learning Numeracy for Detecting Exaggerated Information in Market Comments"
        },
        {
          "paperId": "226144c37145e815d8bebfb8544fca9d69490348",
          "title": "Final Report of the NTCIR-14 FinNum Task: Challenges and Current Status of Fine-Grained Numeral Understanding in Financial Social Media Data"
        },
        {
          "paperId": "ed7f9ecb033ea3f139bc97096caf406a8f0d91a9",
          "title": "CrowdPT: Summarizing Crowd Opinions as Professional Analyst"
        },
        {
          "paperId": "a6b5266f340c36773f85a02673f35f381a1fc964",
          "title": "Overview of the NTCIR-14 FinNum Task Fine-Grained Numeral Understanding in Financial Social Media Data"
        },
        {
          "paperId": "bb5f0975be40fc85907a3baadc66f16f73611679",
          "title": "WUST at the NTCIR-14 FinNum Task"
        },
        {
          "paperId": "97a223895354de39bd6ac1ba2cddc14946e9bb8e",
          "title": "Overview of the NTCIR-15 FinNum-2 Task: Numeral Attachment in Financial Tweets"
        },
        {
          "paperId": "21d95e881ab980b599a2086285204933d76149df",
          "title": "FinNum Task : Enriched Sequence Labeling for Numeral Classification"
        },
        {
          "paperId": "1d88b554dadf7a70dde8e5bbfe72393784a29ffb",
          "title": "aiai at the NTCIR-14 FinNum Task : Financial Numeral Tweets Fine-Grained Classification Using Deep Word and Character Embedding-Based Attention Model"
        },
        {
          "paperId": "14314873db4e784d8dff288483ceccffa9ec0c2e",
          "title": "ASNLU at the NTCIR-14 finnum task : Incorporating Knowledge into DNN for Financial Numeral Classification"
        }
      ],
      "references": [
        {
          "paperId": "dd0a60b3dbd35db5cac6680e4a5bf8efbfabb52e",
          "title": "SemEval-2017 Task 5: Fine-Grained Sentiment Analysis on Financial Microblogs and News"
        },
        {
          "paperId": "6642586d72329dbb78af964670876b8308f91c80",
          "title": "Predicting Stock Market Behavior using Data Mining Technique and News Sentiment Analysis"
        },
        {
          "paperId": "055c6b7ca71dee209689264fec2d93cb97e69a79",
          "title": "Learning to Generate Market Comments from Stock Prices"
        },
        {
          "paperId": "2366429e00310005da25612a3941fdaed1854fd1",
          "title": "Temporal information extraction from clinical text"
        },
        {
          "paperId": "24158c9fc293c8a998ac552b1188404a877da292",
          "title": "Neural Architectures for Named Entity Recognition"
        },
        {
          "paperId": "513167c08db5139162710aad9b2c217b344df2c4",
          "title": "Numerical Relation Extraction with Minimal Supervision"
        },
        {
          "paperId": "e9c7307e1278c63e32af87ef0ae69a2cfbb6083d",
          "title": "An Evaluation of Security Analysts' Forecasts"
        },
        {
          "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
          "title": "Adam: A Method for Stochastic Optimization"
        },
        {
          "paperId": "70c621519bc02d7d2aaa79ca2320fe1af38b6f87",
          "title": "News impact on stock price return via sentiment analysis"
        },
        {
          "paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba",
          "title": "Convolutional Neural Networks for Sentence Classification"
        },
        {
          "paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
          "title": "Distributed Representations of Words and Phrases and their Compositionality"
        },
        {
          "paperId": "1a8bd823c82bd68ab4fb83d13a730badbfe0726d",
          "title": "The future of financial services"
        },
        {
          "paperId": "e498784edf2c02fe0b228479f88120f08b381cb6",
          "title": "Twitter mood predicts the stock market"
        },
        {
          "paperId": "6c93b11ce414776553ed5cb9eb5d3cffa2a5fc6b",
          "title": "Extraction and Approximation of Numerical Attributes from the Web"
        },
        {
          "paperId": "8d8536e5f330d9aca2e9ad608563b7a2cacd5f91",
          "title": "Temporal Information Extraction"
        },
        {
          "paperId": "fa22ad98844764c014fd8e79cb1ed9e4caf8a9ee",
          "title": "ISO-TimeML: An International Standard for Semantic Annotation"
        },
        {
          "paperId": "04b23f577c20d1a0e2a67aadda555f58e6d23d6e",
          "title": "Support Vector Machines"
        },
        {
          "paperId": "24424f4050700dfa940851385d2e1ab7ba5d0cdc",
          "title": "Extended Named Entity Ontology with Attribute Information"
        },
        {
          "paperId": "11b1cd032ebbc9d25ca661c5595ce32de614c90b",
          "title": "Analyzing the Analysts: Career Concerns and Biased Earnings Forecasts"
        },
        {
          "paperId": null,
          "title": "Support vector machines. IEEE Intelligent Systems and their applications"
        },
        {
          "paperId": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
          "title": "Long Short-Term Memory"
        },
        {
          "paperId": "e23c34414e66118ecd9b08cf0cd4d016f59b0b85",
          "title": "Bidirectional recurrent neural networks"
        },
        {
          "paperId": "7e7343a5608fff1c68c5259db0c77b9193f1546d",
          "title": "The measurement of observer agreement for categorical data."
        }
      ],
      "summary": "This work is the first attempt to understand numerals in financial social media data, and it provides the first comparison of fine-grained opinion of individual investors and analysts based on their forecast price.",
      "title": "Numeral Understanding in Financial Tweets for Fine-Grained Crowd-Based Forecasting",
      "venue": "2018 IEEE/WIC/ACM International Conference on Web Intelligence (WI)",
      "year": 2018,
      "authors": "Chung-Chi Chen,Hen-Hsen Huang,Yow-Ting Shiue,Hsin-Hsi Chen"
    },
    {
      "id": "81b4920ad488affaee27389ff9540b7fea90a4ce",
      "url": "https://www.semanticscholar.org/paper/81b4920ad488affaee27389ff9540b7fea90a4ce",
      "citationCount": 89,
      "influentialCitationCount": 17,
      "referenceCount": 37,
      "citations": [
        {
          "paperId": "4f8ae9cdbae875f477aec5ae648f2ad8efc754c6",
          "title": "\"John is 50 years old, can his son be 65?\"Evaluating NLP Models' Understanding of Feasibility"
        },
        {
          "paperId": "87c815840242d055a16bdec14f2fd4d8dce42ccb",
          "title": "Towards Faithful Model Explanation in NLP: A Survey"
        },
        {
          "paperId": "0cf8d106a7265388e96eaee5833fa5010bae0cb1",
          "title": "A Kind Introduction to Lexical and Grammatical Aspect, with a Survey of Computational Approaches"
        },
        {
          "paperId": "167c6bb04315245d2e7b4b1ea60564a6330f4c3d",
          "title": "ILLUME: Rationalizing Vision-Language Models by Interacting with their Jabber"
        },
        {
          "paperId": "15bacb240e2598457af4ded3039b6988aa9706f0",
          "title": "Few-shot Adaptation Works with UnpredicTable Data"
        },
        {
          "paperId": "4e6f63dc99c09560991cf89207f9fa12c739e711",
          "title": "Can Language Models perform Abductive Commonsense Reasoning?"
        },
        {
          "paperId": "98f19ca97512361b12475b42b67a617de14d33a1",
          "title": "Mitigating Reporting Bias in Semi-supervised Temporal Commonsense Inference with Probabilistic Soft Logic"
        },
        {
          "paperId": "150211bdd0b03658e64089e9581b82a0597e1e8e",
          "title": "KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Few-Shot NLP"
        },
        {
          "paperId": "b0291bee1d532e8dc082753329d2579549100479",
          "title": "Eliciting Transferability in Multi-task Learning with Task-level Mixture-of-Experts"
        },
        {
          "paperId": "7890ece03cfb88e0620f8e791105569bd7128c76",
          "title": "Housekeep: Tidying Virtual Households using Commonsense Reasoning"
        },
        {
          "paperId": "6e5944b759ef1c2ea2f694221cf54ef718ec95ee",
          "title": "Reasoning about Procedures with Natural Language Processing: A Tutorial"
        },
        {
          "paperId": "078f4efd448822b0e25d3ee0aec842ced606a595",
          "title": "Go Back in Time: Generating Flashbacks in Stories with Event Temporal Prompts"
        },
        {
          "paperId": "81986b8a3d3fe6c5be06fc4527953fb514ad12e8",
          "title": "Improving In-Context Few-Shot Learning via Self-Supervised Training"
        },
        {
          "paperId": "05f0d34eb777d74e85b525e86e84d68a21d04a64",
          "title": "Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions"
        },
        {
          "paperId": "659f3a959e59eb0e5382143417e7d9d2667d2fff",
          "title": "Extracting seizure frequency from epilepsy clinic notes: a machine reading approach to natural language processing"
        },
        {
          "paperId": "97f456643712e9618edd7465676c62af3c8ae690",
          "title": "A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models"
        },
        {
          "paperId": "7e5ca499cd9b932921bda84db98f75087d0b0683",
          "title": "Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey"
        },
        {
          "paperId": "c3a454e50ec0610f1380d55b1988a5eb5d45207b",
          "title": "Zero-shot Commonsense Question Answering with Cloze Translation and Consistency Optimization"
        },
        {
          "paperId": "47df3fd32d00220c85c2c51a571254fd99b2ecc7",
          "title": "MetaICL: Learning to Learn In Context"
        },
        {
          "paperId": "3ac5aa6ac59253611ef3cb72a95cbe21ef5dda1b",
          "title": "Reframing Instructional Prompts to GPTk’s Language"
        },
        {
          "paperId": "f977d79980ed2dfc7b2194fe680895e49b3b60a9",
          "title": "From LSAT: The Progress and Challenges of Complex Reasoning"
        },
        {
          "paperId": "0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4",
          "title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension"
        },
        {
          "paperId": "ac8d33e4c0a45e227a47353f3f26fbb231482dc1",
          "title": "Time-Aware Language Models as Temporal Knowledge Bases"
        },
        {
          "paperId": "cbdb45fc16b0885905b91d84281c310e6cb49e9c",
          "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions"
        },
        {
          "paperId": "85084fbba9965bdf3446ede255041793f2aa4545",
          "title": "Decomposing and Recomposing Event Structure"
        },
        {
          "paperId": "a5584d2d9b0de9e1692241d46d0c70942919cd60",
          "title": "Answer-level Calibration for Free-form Multiple Choice Question Answering"
        },
        {
          "paperId": "624c6580cf5abd31db63b38596ffa3622c731b00",
          "title": "Generating Temporally-ordered Event Sequences via Event Optimal Transport"
        },
        {
          "paperId": "d29faea4f03cfe337a56dabce84b2d5ce7e473f5",
          "title": "Attention-Focused Adversarial Training for Robust Temporal Reasoning"
        },
        {
          "paperId": "a341c4ea08e5005ad2b1a5bdfc6e3d10fbbb3a1b",
          "title": "Good Night at 4 pm?! Time Expressions in Different Cultures"
        },
        {
          "paperId": "efaee35df6cb22caa6988a26159b7959f42b14ee",
          "title": "Are Visual-Linguistic Models Commonsense Knowledge Bases?"
        },
        {
          "paperId": "82c6f5ffd4d2d43ae7684601f607eae26a759a5a",
          "title": "Analytical Reasoning of Text"
        },
        {
          "paperId": "190d5823c9405a34ccdfcf6591a54d5a0bcf3f3c",
          "title": "Improving Event Duration Question Answering by Leveraging Existing Temporal Information Extraction Data"
        },
        {
          "paperId": "b672d2ec81c9395c312d57a27931864d07664592",
          "title": "A Meta-framework for Spatiotemporal Quantity Extraction from Text"
        },
        {
          "paperId": "65017016b35928cd0e2ab0037200d0f92619b120",
          "title": "Low-resource Learning with Knowledge Graphs: A Comprehensive Survey"
        },
        {
          "paperId": "3ac5281ffa29ed078cc066d79a0edfdbd7056728",
          "title": "Exploring Universal Intrinsic Task Subspace via Prompt Tuning"
        },
        {
          "paperId": "174a0e6da0dfb7f96d4a0a4076eed154c439e41a",
          "title": "Probing Language Models for Understanding of Temporal Expressions"
        },
        {
          "paperId": "2375e447e86f52a35655c3069bcb047e6655eeb8",
          "title": "Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning"
        },
        {
          "paperId": "5f6a790722f18f0aa7419f8e0c6404c46acd99ba",
          "title": "Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding"
        },
        {
          "paperId": "b1a71d8fb272016d618917a3fc392551fa941c81",
          "title": "Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers"
        },
        {
          "paperId": "32c1767b956dec644f8bfc1f009fc5a1af47f0cc",
          "title": "Discourse-Level Event Temporal Ordering with Uncertainty-Guided Graph Completion"
        },
        {
          "paperId": "62953ca1252c9febe07c7007a10911726f37792d",
          "title": "TIMEDIAL: Temporal Commonsense Reasoning in Dialog"
        },
        {
          "paperId": "6597d61bdb531051678c773526758a6dc113b9ce",
          "title": "COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences"
        },
        {
          "paperId": "4ca39cf99747b8962fe37e7e025e284872df3425",
          "title": "Comparing Test Sets with Item Response Theory"
        },
        {
          "paperId": "b39f76381897ab744250673a058e5dd06c009813",
          "title": "Event Time Extraction and Propagation via Graph Attention Networks"
        },
        {
          "paperId": "ccbc69b6e187b9a1614f76c5ede30b139f3dc9fb",
          "title": "Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks"
        },
        {
          "paperId": "7b486a6eac4b46cf39e41c97b25ea22c5d27a883",
          "title": "Natural Instructions: Benchmarking Generalization to New Tasks from Natural Language Instructions"
        },
        {
          "paperId": "7fa273f450251523e6b7fcc2eb3fdbdfd4a30493",
          "title": "CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP"
        },
        {
          "paperId": "c7f977f556d2060238fdc1286d057d46958afaf9",
          "title": "ESTER: A Machine Reading Comprehension Dataset for Event Semantic Relation Reasoning"
        },
        {
          "paperId": "2ef4be35f8424ea768aa2e1b44392b3eddbc780b",
          "title": "Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema"
        },
        {
          "paperId": "c5943bc4b22a63f595cb1c2823a449e03aad4787",
          "title": "AR-LSAT: Investigating Analytical Reasoning of Text"
        },
        {
          "paperId": "7a9ef45ec4a6fe04c373715f455924b6c1c51ddd",
          "title": "DAGN: Discourse-Aware Graph Network for Logical Reasoning"
        },
        {
          "paperId": "54bb329be4b557d38e0628b651e7074524d35be2",
          "title": "LOME: Large Ontology Multilingual Extraction"
        },
        {
          "paperId": "d6c919ee0d51432496513ef4b6b2dbd128819779",
          "title": "Microtask Detection"
        },
        {
          "paperId": "33b06c74eea3f400b6f5ef14ef163aef1db42d16",
          "title": "Conditional Generation of Temporally-ordered Event Sequences"
        },
        {
          "paperId": "6eee69031d2e11aa03a5a8fcb219cff4562863be",
          "title": "ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning"
        },
        {
          "paperId": "9d436d25981ea61db728bd490f0d54376d08953e",
          "title": "Temporal Reasoning on Implicit Events from Distant Supervision"
        },
        {
          "paperId": "5dfc43bb697acf5eacf8b8a05d78dba8beb0dd42",
          "title": "Paragraph-Level Commonsense Transformers with Recurrent Memory"
        },
        {
          "paperId": "ce7091d011be560aca0fba8511e5a41a1436f5d7",
          "title": "Aligning AI With Shared Human Values"
        },
        {
          "paperId": "30602e3382df3abedb5f225b55b7efce8580f74d",
          "title": "ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data"
        },
        {
          "paperId": "8424082e3bf4792462eb112d7ebcecf5b0dc3613",
          "title": "Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"
        },
        {
          "paperId": "00dea030e47950e55d6497a5c8acf5053baebcbd",
          "title": "ALICE++: Adversarial Training for Robust and Effective Temporal Reasoning"
        },
        {
          "paperId": "9485076ed295c6997b8a3148e1f37156eff97813",
          "title": "Research Statement: Climbing the Generality Ladder in NLP"
        },
        {
          "paperId": "21f018727ce0fa0f932188fa5abcc32426aba7b6",
          "title": "On End-to-end Automatic Fact-checking Systems"
        },
        {
          "paperId": "8b383bd2eb054787d84636dab7b6ecf4a318c6c7",
          "title": "Towards a Language Model for Temporal Commonsense Reasoning"
        },
        {
          "paperId": "177bc611389fac0e6239355a2d9eaa66f7aac53e",
          "title": "Improving Unsupervised Commonsense Reasoning Using Knowledge-Enabled Natural Language Inference"
        },
        {
          "paperId": "6bb369f874f49cd51415f216f1a3f635f2ca1eed",
          "title": "ESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations"
        },
        {
          "paperId": "e6d84cf9ae6efa10919bff765613e883a761db62",
          "title": "Open Temporal Relation Extraction for Question Answering"
        },
        {
          "paperId": "5d1e996bb64a0d082c85010b9c78036db4da02e0",
          "title": "GCRC: A New Challenging MRC Dataset from Gaokao Chinese for Explainable Evaluation"
        },
        {
          "paperId": "655db2fd59e4112d0a5596e1695c5765457ca53c",
          "title": "Extraction of Common-Sense Relations from Procedural Task Instructions using BERT"
        },
        {
          "paperId": "9793b07ba09d9f2ac9cabd8117daa93bf3db4346",
          "title": "DEER: A Data Efficient Language Model for Event Temporal Reasoning"
        },
        {
          "paperId": "d865c87f6ce4e0be29ae1e3780fae66b8034d04b",
          "title": "TR at SemEval-2020 Task 4: Exploring the Limits of Language-model-based Common Sense Validation"
        },
        {
          "paperId": "f2143dd2a2c9c43d9d28462c1cd2a51f5f3db2c6",
          "title": "Improving Event Duration Prediction via Time-aware Pre-training"
        },
        {
          "paperId": "3d2035edd4dd48e1e638279409e11bf689c461e1",
          "title": "Temporal Reasoning in Natural Language Inference"
        },
        {
          "paperId": "a0e7d24f91da6a1be5f8856963e3b160d47ca7b7",
          "title": "Towards Zero Shot Conditional Summarization with Adaptive Multi-task Fine-Tuning"
        },
        {
          "paperId": "6bd8c038fdcae6cc155f27bcca9acdf04c06c786",
          "title": "Deriving Commonsense Inference Tasks from Interactive Fictions"
        },
        {
          "paperId": "f642ddb69f712e2b517cb6c6d9a062506991d1ed",
          "title": "Commonsense Learning: An Indispensable Path towards Human-centric Multimedia"
        },
        {
          "paperId": "45f952c21130d655090058e864c2772358a1de72",
          "title": "Commonsense Reasoning for Natural Language Processing"
        },
        {
          "paperId": "5f4a70c0abeafa4ecff90739bd8efbe05ea3bac5",
          "title": "Applying the T5 language model and duration units normalization to address temporal common sense understanding on the MCTACO dataset"
        },
        {
          "paperId": "d075301119b79e5b6b9306f9981d256191882981",
          "title": "Adversarial Training for Commonsense Inference"
        },
        {
          "paperId": "07c1c2429b63fefdae41eb546c31b40de2a880f7",
          "title": "INFOTABS: Inference on Tables as Semi-structured Data"
        },
        {
          "paperId": "b9485d1e2c66c3ae452ec4903c2a157caef4d2ed",
          "title": "Temporal Common Sense Acquisition with Minimal Supervision"
        },
        {
          "paperId": "5cc4fa603085753610f18c1429b383c62cbed043",
          "title": "ForecastQA: Machine Comprehension of Temporal Text for Answering Forecasting Questions"
        },
        {
          "paperId": "36d6c8895bbc755964b8b2136c6fd6087a7af089",
          "title": "TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions"
        },
        {
          "paperId": "9808d59113029d96f48a0376b1578dbab5427bb4",
          "title": "Unsupervised Commonsense Question Answering with Self-Talk"
        },
        {
          "paperId": "35e6783307f82d1faa39be0653431305abec7271",
          "title": "Evaluating Models’ Local Decision Boundaries via Contrast Sets"
        },
        {
          "paperId": "9fec5868542b4d9070306f1418d1d21666226e90",
          "title": "Evaluating NLP Models via Contrast Sets"
        },
        {
          "paperId": "35115a7e16fb658a5f8e09eeb60a5f3204b24598",
          "title": "Tell Me About Your Day: Designing a Conversational Agent for Time and Stress Management"
        },
        {
          "paperId": "287ed28f1e0f56ff56834580a1bf6720daeb6b2b",
          "title": "Reasoning-Driven Question-Answering for Natural Language Understanding"
        },
        {
          "paperId": "4043a936960de8e149dc208178fe1bcb157c7fa4",
          "title": "Recent Advances in Natural Language Inference: A Survey of Benchmarks, Resources, and Approaches"
        }
      ],
      "references": [
        {
          "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
          "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
          "paperId": "e20cf061851898e8477fea6f195cfe44f8ad7a86",
          "title": "CogCompTime: A Tool for Understanding Time in Natural Language"
        },
        {
          "paperId": "0e9d94639ac964754f812019af91b7c519103b46",
          "title": "Learning Scalar Adjective Intensity from Paraphrases"
        },
        {
          "paperId": "711b1f7cc4e92d6f40c7813c6f0e1c2e179d48ad",
          "title": "Commonsense for Generative Multi-Hop Question Answering Tasks"
        },
        {
          "paperId": "190eb1dff4c08fc12a085242b67c0442cfe5fc84",
          "title": "Reasoning about Actions and State Changes by Injecting Commonsense Knowledge"
        },
        {
          "paperId": "af5c4b80fbf847f69a202ba5a780a3dd18c1a027",
          "title": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"
        },
        {
          "paperId": "bbad0b301561c9b44a43f2880b29f143dc7297ba",
          "title": "Temporal Information Extraction by Predicting Relative Time-lines"
        },
        {
          "paperId": "8eb9b2ea146e3a381a29b4a9314c36f61b71e367",
          "title": "Extracting Commonsense Properties from Embeddings with Limited Human Guidance"
        },
        {
          "paperId": "34901b737b11aa51b688fa18c2eab47639d7b8c6",
          "title": "Joint Reasoning for Temporal and Causal Relations"
        },
        {
          "paperId": "82580fe4c429ac76f94c514c1ffc066844b13192",
          "title": "Determining Event Durations: Models and Error Analysis"
        },
        {
          "paperId": "15d742935a5bb28e464048ed33ff18128c9b09e7",
          "title": "MITRE at SemEval-2018 Task 11: Commonsense Reasoning without Commonsense Knowledge"
        },
        {
          "paperId": "6c5eb60ac554f2210eb18124ca0f052806eec2b0",
          "title": "SemEval-2018 Task 11: Machine Comprehension Using Commonsense Knowledge"
        },
        {
          "paperId": "99ad0533f84c110da2d0713d5798e6e14080b159",
          "title": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences"
        },
        {
          "paperId": "2754380bf487a455112d9c5594914688f9205ac0",
          "title": "Constructing Narrative Event Evolutionary Graph for Script Event Prediction"
        },
        {
          "paperId": "b7fbce48dfd734adbab95c669c290d9e4aaf3272",
          "title": "Event2Mind: Commonsense Inference on Events, Intents, and Reactions"
        },
        {
          "paperId": "a6ced6341e8bf2d819cbc7de73b869752019afdd",
          "title": "Improving Temporal Relation Extraction with a Globally Acquired Statistical Resource"
        },
        {
          "paperId": "bff8ae9e28323d217b9ad5a7321e58f79607f557",
          "title": "A Multi-Axis Annotation Scheme for Event Temporal Relations"
        },
        {
          "paperId": "88bb0a28bb58d847183ec505dda89b63771bb495",
          "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
        },
        {
          "paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f",
          "title": "Deep Contextualized Word Representations"
        },
        {
          "paperId": "a9e0fa94e59261f5905951e93d515af64c3fc7cb",
          "title": "A Structured Learning Approach to Temporal Relation Extraction"
        },
        {
          "paperId": "c9f343b492c170c726f607c255ec6c7177dc5800",
          "title": "Verb Physics: Relative Physical Knowledge of Actions and Objects"
        },
        {
          "paperId": "13f2792d63e933f8a9acda03d95a2d801d7c70f3",
          "title": "Ordinal Common-sense Inference"
        },
        {
          "paperId": "83e7654d545fbbaaf2328df365a781fb67b841b4",
          "title": "Enhanced LSTM for Natural Language Inference"
        },
        {
          "paperId": "dde70b2fe5ead57cea2556c52a975865e3bb4d30",
          "title": "What Happens Next? Event Prediction Using a Compositional Neural Network Model"
        },
        {
          "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
          "title": "GloVe: Global Vectors for Word Representation"
        },
        {
          "paperId": "7b82383b09bedb765ab9c5c2153978035aacd830",
          "title": "Context-dependent Semantic Parsing for Time Expressions"
        },
        {
          "paperId": "32a257bb8cbee2e6f833f7c7f6e9bcacde4919d7",
          "title": "Extracting fine-grained durations for verbs from Twitter"
        },
        {
          "paperId": "fb0b11046474b8f1c810f947f313c7c7229a988f",
          "title": "SemEval-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning"
        },
        {
          "paperId": "407314c1b491511564c089d26e7e92c0c93af754",
          "title": "Learning Temporal Information for States and Events"
        },
        {
          "paperId": "085730e9355193859e4172c729821aaedc71f4c2",
          "title": "Using Query Patterns to Learn the Duration of Events"
        },
        {
          "paperId": null,
          "title": "Choice of plausible alternatives : An evaluation of commonsense causal reason"
        },
        {
          "paperId": "7716a474569d94a9dc0074a2cdfc3b32641b721e",
          "title": "HeidelTime: High Quality Rule-Based Extraction and Normalization of Temporal Expressions"
        },
        {
          "paperId": "0c739b915d633cc3c162e4ef1e57b796c2dc2217",
          "title": "VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations"
        },
        {
          "paperId": "bb0968ab69b8655d442f17cdfe317ce3bfebd789",
          "title": "Can we derive general world knowledge from texts"
        },
        {
          "paperId": "fc090a68e45e0e6337136777d21c87b76a90ae72",
          "title": "From TreeBank to PropBank"
        },
        {
          "paperId": "cf3955f27c54f425c29c372af3aa19778abe2051",
          "title": "Representations of commonsense knowledge"
        }
      ],
      "summary": "It is found that the best current methods used on MCTACO are still far behind human performance, by about 20%, and several directions for improvement are discussed.",
      "title": "“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding",
      "venue": "EMNLP",
      "year": 2019,
      "authors": "Ben Zhou,Daniel Khashabi,Qiang Ning,D. Roth"
    },
    {
      "id": "d88f31a0091eee02c5a2aa2013914818cdef114e",
      "url": "https://www.semanticscholar.org/paper/d88f31a0091eee02c5a2aa2013914818cdef114e",
      "citationCount": 39,
      "influentialCitationCount": 1,
      "referenceCount": 28,
      "citations": [
        {
          "paperId": "01b2f7601ab3df0d2982a204e2fb309f6622646f",
          "title": "Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"
        },
        {
          "paperId": "42bc0824d8ca35105d181aaa0183654535325f55",
          "title": "Structural Biases for Improving Transformers on Translation into Morphologically Rich Languages"
        },
        {
          "paperId": "d9f65f6e3d13001309bf6a93f2c968f927c06149",
          "title": "Artificial neural network modelling of the neural population code underlying mathematical operations"
        },
        {
          "paperId": "40edfa97cd02268fccff75eb9c693b11c1a968b2",
          "title": "Formal Specifications from Natural Language"
        },
        {
          "paperId": "f2611a09cf0942170785ee3025cb511de3bdec2e",
          "title": "Neurocompositional computing: From the Central Paradox of Cognition to a new generation of AI systems"
        },
        {
          "paperId": "2e03ee15c00561a891eaf197515da720d1566411",
          "title": "Causal Transformer for Estimating Counterfactual Outcomes"
        },
        {
          "paperId": "1ee909a2a78d287cac549d5deea96f3cbd1c7092",
          "title": "Enhancing Neural Mathematical Reasoning by Abductive Combination with Symbolic Library"
        },
        {
          "paperId": "6027247d7f4256c10bdc71b8584d5927e616fa37",
          "title": "Logic Tensor Networks"
        },
        {
          "paperId": "d129841cb2e30e25000dcd9edb83c880fc4babc1",
          "title": "Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention"
        },
        {
          "paperId": "dc88d2bbcebd810d7c80ba281739908005b12235",
          "title": "Neurocompositional computing in human and machine intelligence: A tutorial"
        },
        {
          "paperId": "2db6c10f135d5701ae7aec45986124ce264c1344",
          "title": "Learning Symbolic Rules for Reasoning in Quasi-Natural Language"
        },
        {
          "paperId": "04db9b694280134f09af5fa787a306907edba29d",
          "title": "How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN"
        },
        {
          "paperId": "612facf3ed023eff6695e5192d6a0be33bd83693",
          "title": "Generating Symbolic Reasoning Problems with Transformer GANs"
        },
        {
          "paperId": "ed535e93d5b5a8b689e861e9c6083a806d1535c2",
          "title": "The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers"
        },
        {
          "paperId": "d0dae92c4d37520ae20c072ec64fdb718874bfd0",
          "title": "A Reinforcement Learning Environment for Mathematical Reasoning via Program Synthesis"
        },
        {
          "paperId": "86589b6286ef3c55b8b4fccfb41a3b30b7afdf61",
          "title": "Going Beyond Linear Transformers with Recurrent Fast Weight Programmers"
        },
        {
          "paperId": "45f7c0caf0f1f88a568279de52d7a8c29aa8e28f",
          "title": "Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization"
        },
        {
          "paperId": "f3879fedf036175aefdb750c5527d184f038b932",
          "title": "Compositional Processing Emerges in Neural Networks Solving Math Problems"
        },
        {
          "paperId": "2cc3ab9fa41ba2804e301f7eae9598636e62422a",
          "title": "Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"
        },
        {
          "paperId": "58c74cec28f3416b9a1d308bb2d6519d21d53ab0",
          "title": "LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning"
        },
        {
          "paperId": "5e11e806d24dd80ecf0f91e7aacedbba8d9fd6fc",
          "title": "Learning Associative Inference Using Fast Weight Memory"
        },
        {
          "paperId": "804a6d7c23335bbca6eec3b7d3c8366dcbe395a5",
          "title": "Hopfield Networks is All You Need"
        },
        {
          "paperId": "05f5f8b2065a520846d89771ebaea2bb1534e9c6",
          "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
        },
        {
          "paperId": "9ebb28851b253817f9a0ea5ddc22b0fd9a934a2f",
          "title": "Deep Learning--based Text Classification"
        },
        {
          "paperId": "b32e631ae7779d93b9979c61c5b920a76342063e",
          "title": "Teaching Temporal Logics to Neural Networks"
        },
        {
          "paperId": "624d6a768e6f6d7e368ede0d22ee691d1aaf3a40",
          "title": "T EACHING T EMPORAL L OGICS TO N EURAL N ET WORKS ∗"
        },
        {
          "paperId": "c2d50f17ea6769f6f5663ccac37a9627a0543184",
          "title": "Mathematical Reasoning in General Artificial Intelligence Workshop , ICLR 2021"
        },
        {
          "paperId": "adc61e21eafecfbf6ebecc570f9f913659a2bfb2",
          "title": "Deep Learning Based Text Classification: A Comprehensive Review"
        },
        {
          "paperId": "8babeaffc8413747412765803a50d2c3adbbbbdd",
          "title": "DEBERTA: DECODING-ENHANCED BERT"
        },
        {
          "paperId": "ef19c8b51ff4bc3026a3bf09b81a11f5b3fe04c7",
          "title": "Inflectional paradigms as interacting systems"
        },
        {
          "paperId": "d642868ce4325ebf3026c0aa0c497a079f112a8d",
          "title": "On the Binding Problem in Artificial Neural Networks"
        },
        {
          "paperId": "c2b4d96db34bd472e84c9234838cc4e808eb1ba9",
          "title": "Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language"
        },
        {
          "paperId": "3efbcfeeb0ea1051a71101d3318da4411081f0b8",
          "title": "Scaling Laws for Autoregressive Generative Modeling"
        },
        {
          "paperId": "7e9af8a6081dc00187bd4a6727751d1721bd7816",
          "title": "Evaluating Logical Generalization in Graph Neural Networks"
        },
        {
          "paperId": "5ade9c2fabc0fc526ea05445f8d0f92666266681",
          "title": "Transformers Generalize to the Semantics of Logics"
        },
        {
          "paperId": "97d69e7e8c04714bf58dcbe5ae7454db69b657a7",
          "title": "The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence"
        },
        {
          "paperId": "caa202043b553cea0b099d591308e66db8a300ba",
          "title": "Dreaming with ARC"
        },
        {
          "paperId": "1de88fd6bdaac4afe6dd3b7ca80923e1dbff40e1",
          "title": "Artificial General Intelligence: 13th International Conference, AGI 2020, St. Petersburg, Russia, September 16–19, 2020, Proceedings"
        },
        {
          "paperId": "f886a0dc2dda508f744bc9e1c3750b468ed2f65b",
          "title": "Transformer Model for Mathematical Reasoning-CS 230 Final Report"
        }
      ],
      "references": [
        {
          "paperId": "59a916cdc943f0282908e6f3fa0360f4c5fb78d0",
          "title": "Stabilizing Transformers for Reinforcement Learning"
        },
        {
          "paperId": "830995ef17cc291c13f42dfd9f462137de1d2179",
          "title": "Augmenting Self-attention with Persistent Memory"
        },
        {
          "paperId": "afd110eace912c2b273e64851c6b4df2658622eb",
          "title": "Visualizing and Measuring the Geometry of BERT"
        },
        {
          "paperId": "165d51a547cd920e6ac55660ad5c404dcb9562ed",
          "title": "Open Sesame: Getting inside BERT’s Linguistic Knowledge"
        },
        {
          "paperId": "67146c46304720c026a3c9bc324ffc2285df2c0b",
          "title": "A Perspective on Objects and Systematic Generalization in Model-Based RL"
        },
        {
          "paperId": "455a8838cde44f288d456d01c76ede95b56dc675",
          "title": "A Structural Probe for Finding Syntax in Word Representations"
        },
        {
          "paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c",
          "title": "BERT Rediscovers the Classical NLP Pipeline"
        },
        {
          "paperId": "afed6dc6900d3b37e528b9086661bba583d60bf6",
          "title": "Analysing Mathematical Reasoning Abilities of Neural Models"
        },
        {
          "paperId": "735708fb10c3420a1138b0d1c7a0923ff4de3c41",
          "title": "A Simple Recurrent Unit with Reduced Tensor Product Representations"
        },
        {
          "paperId": "6c7494a47cc5421a7b636c244e13586dc2dab007",
          "title": "Systematic Generalization: What Is Required and Can It Be Learned?"
        },
        {
          "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
          "paperId": "f6fa98b0d1f6bff608a056acd7b82e2a9dd0a68e",
          "title": "Learning to Reason with Third-Order Tensor Products"
        },
        {
          "paperId": "b5ecf634902b329aea683176c56aac35029da621",
          "title": "Learning Distributed Representations of Symbolic Structure Using Binding and Unbinding Operations"
        },
        {
          "paperId": "fd4ae71916cf400bfd1490f275e91b154eb69160",
          "title": "Relational recurrent neural networks"
        },
        {
          "paperId": "856fe866bcce5e7a540655bea6ecc7406bdcfcba",
          "title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"
        },
        {
          "paperId": "d4a9876f00fd9caf0358fd73e5572e53a47cda12",
          "title": "Question-Answering with Grammatically-Interpretable Representations"
        },
        {
          "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
          "title": "Attention is All you Need"
        },
        {
          "paperId": "dc620583dc06f7262707b4e2a9b7df91dbd384e0",
          "title": "Deep Learning of Grammatically-Interpretable Representations Through Question-Answering"
        },
        {
          "paperId": "b92aa7024b87f50737b372e5df31ef091ab54e62",
          "title": "Training Very Deep Networks"
        },
        {
          "paperId": "b71ac1e9fb49420d13e084ac67254a0bbd40f83f",
          "title": "Understanding the difficulty of training deep feedforward neural networks"
        },
        {
          "paperId": "9ca9f28676ad788d04ba24a51141a9a0a0df4d67",
          "title": "A new model for learning in graph domains"
        },
        {
          "paperId": "c58dd287a476b4722c5b6b1316629e2874682219",
          "title": "Learning task-dependent distributed representations by backpropagation through structure"
        },
        {
          "paperId": "e62f7643f616aaad65ffd47155a53bfa325e455d",
          "title": "The Correlation Theory of Brain Function"
        },
        {
          "paperId": "61639af1a89c69094bcc0ed40fad752832b037c3",
          "title": "Reducing the Ratio Between Learning Complexity and Number of Time Varying Variables in Fully Recurrent Nets"
        },
        {
          "paperId": "bc22e87a26d020215afe91c751e5bdaddd8e4922",
          "title": "Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks"
        },
        {
          "paperId": "24484e1105bd28acbf0184c94ac9833511328087",
          "title": "Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems"
        },
        {
          "paperId": "3106e66537a0c8f53278e553bcb38f0b0992ec0e",
          "title": "Distributed Representations"
        },
        {
          "paperId": "b7efb6b6f7e9ffa017e970a098665f76d4dfeca2",
          "title": "Polynomial Theory of Complex Systems"
        }
      ],
      "summary": "The TP-Transformer's attention maps give better insights into how it is capable of solving the Mathematics Dataset's challenging problems, and incorporates Tensor-Product Representations within the Transformer in order to better support the explicit representation of relation structure.",
      "title": "Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving",
      "venue": "ArXiv",
      "year": 2019,
      "authors": "Imanol Schlag,P. Smolensky,Roland Fernandez,N. Jojic,J. Schmidhuber,Jianfeng Gao"
    },
    {
      "id": "5e0cffc51e8b64a8f11326f955fa4b4f1803e3be",
      "url": "https://www.semanticscholar.org/paper/5e0cffc51e8b64a8f11326f955fa4b4f1803e3be",
      "citationCount": 185,
      "influentialCitationCount": 21,
      "referenceCount": 61,
      "citations": [
        {
          "paperId": "72e3a1b4745d6f25b8cd186bd56fbad6e3a8464b",
          "title": "Judgment aggregation, discursive dilemma and reflective equilibrium: Neural language models as self-improving doxastic agents"
        },
        {
          "paperId": "8f60454584a8d1f1d6f278198af5e0bd2d103067",
          "title": "A Survey of Parameters Associated with the Quality of Benchmarks in NLP"
        },
        {
          "paperId": "f2dadc182d33f3ad74f95505e5b6611c3abafe02",
          "title": "MetaFill: Text Infilling for Meta-Path Generation on Heterogeneous Information Networks"
        },
        {
          "paperId": "5ad026ea27a3f114c79376b47b3372bd8de90821",
          "title": "CHARD: Clinical Health-Aware Reasoning Across Dimensions for Text Generation Models"
        },
        {
          "paperId": "0828722a8317a556c8753cfe1a8cf3a3eec0004f",
          "title": "Measuring and Narrowing the Compositionality Gap in Language Models"
        },
        {
          "paperId": "559bfba3bee31f6061a5d5c7061f22794de47e39",
          "title": "State-of-the-art generalisation research in NLP: a taxonomy and review"
        },
        {
          "paperId": "fb21ab7aacfae1679e70a74d1b9b6ba19a096f36",
          "title": "Recitation-Augmented Language Models"
        },
        {
          "paperId": "f2878e3a27950c674425ba896bfd1c2be7dd67e5",
          "title": "Negation, Coordination, and Quantifiers in Contextualized Language Models"
        },
        {
          "paperId": "416e92370e15e25ec23547ef32e6c1702c1ca1fc",
          "title": "CommunityLM: Probing Partisan Worldviews from Language Models"
        },
        {
          "paperId": "c8ff9fef927b6ccf242c20b3939a7442a0633ab3",
          "title": "VIPHY: Probing \"Visible\" Physical Commonsense Knowledge"
        },
        {
          "paperId": "a6e39438f766a4df502d6922c7f253398af60c14",
          "title": "Pre-Trained Language Models and Their Applications"
        },
        {
          "paperId": "aa3bae7def250ba247ea2c187ad1c1a99b3bf737",
          "title": "Lost in Context? On the Sense-wise Variance of Contextualized Word Embeddings"
        },
        {
          "paperId": "94e5ca13e8c884509ccad233c77979923b677a3e",
          "title": "UnCommonSense: Informative Negative Knowledge about Everyday Concepts"
        },
        {
          "paperId": "5b5f9554b71321dc6df2518e1347354e4ac67c0d",
          "title": "Pro-tuning: Unified Prompt Tuning for Vision Tasks"
        },
        {
          "paperId": "f3cf71c51b882fe3111d71c4bf104297d38197f8",
          "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models"
        },
        {
          "paperId": "f1f96887d1756980bfbfabcea1230c8ec0f062ba",
          "title": "An Empirical Survey on Long Document Summarization: Datasets, Models and Metrics"
        },
        {
          "paperId": "9ecc1b8494328f7b2d1b58595b2c28eb768c2c7c",
          "title": "longhorns at DADC 2022: How many linguists does it take to fool a Question Answering model? A systematic approach to adversarial attacks."
        },
        {
          "paperId": "3f5d3c57ccb3b9059eb8a94fa7af4d9f299c56c6",
          "title": "Entity-Graph Enhanced Cross-Modal Pretraining for Instance-level Product Retrieval"
        },
        {
          "paperId": "36bbb161a3fbe8e309bc618ab05b428b22c56147",
          "title": "The Curious Case of Control"
        },
        {
          "paperId": "470bd6813d303d4ea776def788d215caaa8e772f",
          "title": "History Compression via Language Models in Reinforcement Learning"
        },
        {
          "paperId": "a399e64a2698fb6e6d43b1a9576469afd4a29db3",
          "title": "GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models"
        },
        {
          "paperId": "4e5f7cd537a1bbcd090f9887b1b59f39a3715dba",
          "title": "Instruction Induction: From Few Examples to Natural Language Task Descriptions"
        },
        {
          "paperId": "a005a6160efa2fd055581b8222b41f71f966ea50",
          "title": "Life after BERT: What do Other Muppets Understand about Language?"
        },
        {
          "paperId": "7f84d56fb8feb4e50cd6c3da3e3fd4ff6c4772cf",
          "title": "ElitePLM: An Empirical Study on General Language Ability Evaluation of Pretrained Language Models"
        },
        {
          "paperId": "c90a14b54ed56c975fda929418342cf373925e4e",
          "title": "Entity-aware Transformers for Entity Search"
        },
        {
          "paperId": "cd471b5ef162906ef3d9a84398b3f98e9ee4bf56",
          "title": "A Review on Language Models as Knowledge Bases"
        },
        {
          "paperId": "706c6b3781374b0b11f98f204a4ddd05b26ed009",
          "title": "Knowledge Infused Decoding"
        },
        {
          "paperId": "c4e991c0c5c21608a5a21d31fd478ce7b7fb527d",
          "title": "Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View"
        },
        {
          "paperId": "537d98e241975dc5c32d9372ae85134dffe45532",
          "title": "DP-KB: Data Programming with Knowledge Bases Improves Transformer Fine Tuning for Answer Sentence Selection"
        },
        {
          "paperId": "50f7d69ddd7f9b34f5121607fbdcc57236d65b8c",
          "title": "Can Pre-trained Language Models Interpret Similes as Smart as Human?"
        },
        {
          "paperId": "243227a279e6a4193ef653fd763d2b34c9edb45d",
          "title": "Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach"
        },
        {
          "paperId": "18dc24da603896db2264e9174116407a95c593d5",
          "title": "“Is Whole Word Masking Always Better for Chinese BERT?”: Probing on Chinese Grammatical Error Correction"
        },
        {
          "paperId": "7bebb48d34c219b119ca2d4ffc97d7fd4940c35c",
          "title": "On the data requirements of probing"
        },
        {
          "paperId": "92a8f7f09f3705cb5a6009a42220a6f01ea084e8",
          "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"
        },
        {
          "paperId": "926689bf5f21e82a24830a68a0dc879ac0783784",
          "title": "Kformer: Knowledge Injection in Transformer Feed-Forward Layers"
        },
        {
          "paperId": "c3a454e50ec0610f1380d55b1988a5eb5d45207b",
          "title": "Zero-shot Commonsense Question Answering with Cloze Translation and Consistency Optimization"
        },
        {
          "paperId": "63f17017257063ee034c4082d93005dc4b25d42d",
          "title": "Pushing the Limits of Rule Reasoning in Transformers through Natural Language Satisfiability"
        },
        {
          "paperId": "0b483b550b21ec42d693fc04a372dbb10dd07019",
          "title": "Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge"
        },
        {
          "paperId": "c5fbf9a62a91e3182f65e3746d3263387effa4a7",
          "title": "The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail"
        },
        {
          "paperId": "c131a9eb834055a3b846c3d3816cd23a3434e528",
          "title": "DeepA2: A Modular Framework for Deep Argument Analysis with Pretrained Neural Text2Text Language Models"
        },
        {
          "paperId": "a418c0daa98a3639e1b1bd682c68644250259944",
          "title": "Transformers in the loop: Polarity in neural models of language"
        },
        {
          "paperId": "299983121dec88d4cc8e4ea2aa06514787d8d878",
          "title": "Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models"
        },
        {
          "paperId": "2cf3cd3a7a08fc91eecab45e73299940c9c439dc",
          "title": "Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills"
        },
        {
          "paperId": "966a38882be844dbf7e8b15478e1bdf3c75ef8a6",
          "title": "A Closer Look at How Fine-tuning Changes BERT"
        },
        {
          "paperId": "ada0e2e476523714a5109c8bb19588140e2314e7",
          "title": "Core Challenges in Embodied Vision-Language Planning"
        },
        {
          "paperId": "1a0d8dbd0252193abe9d64f72fc56cc1f05ed3eb",
          "title": "CURIE: An Iterative Querying Approach for Reasoning About Situations"
        },
        {
          "paperId": "f5d1dbdaa6c8a44f388f3f7fe538403baefc1252",
          "title": "Large pre-trained language models contain human-like biases of what is right and wrong to do"
        },
        {
          "paperId": "37bf0bf34603145246c3311df19e2afdf6e0270a",
          "title": "JAKET: Joint Pre-training of Knowledge Graph and Language Understanding"
        },
        {
          "paperId": "fed460648303afa32247e493847e4dc73dc1a5b3",
          "title": "Can We Guide a Multi-Hop Reasoning Language Model to Incrementally Learn at Each Single-Hop?"
        },
        {
          "paperId": "cd54a839ba12e7417e321b4407788dc426ebaefc",
          "title": "Exploiting Numerical-Contextual Knowledge to Improve Numerical Reasoning in Question Answering"
        },
        {
          "paperId": "414536eafa990570530433b2f112ad343c66b4b0",
          "title": "Deep Learning Enabled Consumer Research for Product Development"
        },
        {
          "paperId": "41237c1350eb2371363af543a6c6a30db9d09197",
          "title": "AnaLog: Testing Analytical and Deductive Logic Learnability in Language Models"
        },
        {
          "paperId": "af7503c8afb957edbf6f196f3821a8cd79bfadf6",
          "title": "Data-Efficient Concept Extraction from Pre-trained Language Models for Commonsense Explanation Generation"
        },
        {
          "paperId": "0ad58485bb1b0ff3cdcf0d2a087ce8c40de529e8",
          "title": "Where Does Linguistic Information Emerge in Neural Language Models? Measuring Gains and Contributions across Layers"
        },
        {
          "paperId": "44772fe1c3fa422a3da7e25092db2544893d6bfb",
          "title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming"
        },
        {
          "paperId": "82c6f5ffd4d2d43ae7684601f607eae26a759a5a",
          "title": "Analytical Reasoning of Text"
        },
        {
          "paperId": "b71c245e093568d8c95aa889f968ce72b18e3d8b",
          "title": "The First Workshop on Commonsense Representation and Reasoning May 27 , 2022"
        },
        {
          "paperId": "06396c7cd5d223a1776abf8811359ec7bc05d420",
          "title": "Knowledge-Augmented Methods for Natural Language Processing"
        },
        {
          "paperId": "f4a2a13b5ff0f4b4a83f3b4effe84dbf8bb84884",
          "title": "LoNLI: An Extensible Framework for Testing Diverse Logical Reasoning Capabilities for NLI"
        },
        {
          "paperId": "2db6c10f135d5701ae7aec45986124ce264c1344",
          "title": "Learning Symbolic Rules for Reasoning in Quasi-Natural Language"
        },
        {
          "paperId": "2af476f7c2a7c040fc9ab7750bf41a84f66aa947",
          "title": "Knowledge Based Multilingual Language Model"
        },
        {
          "paperId": "a466d10b80dbdee3b130bef73ec62f3a89eb389b",
          "title": "Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples"
        },
        {
          "paperId": "c23d9d44e8bc68408cea9f305d1f24d915bc0d0d",
          "title": "Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey"
        },
        {
          "paperId": "d5784fd3ac7e06ec030abb8f7787faa9279c1a50",
          "title": "Interpreting Deep Learning Models in Natural Language Processing: A Review"
        },
        {
          "paperId": "290867638c5ca520de5c48aa4336f196d426c226",
          "title": "Knowledge Enhanced Pretrained Language Models: A Compreshensive Survey"
        },
        {
          "paperId": "1b94ebedacda0c21a4b8a40a5a40afcea4cc719a",
          "title": "When Combating Hype, Proceed with Caution"
        },
        {
          "paperId": "bde10a2d3dfd138ea6a83f14c17ea3c3f9ac6db0",
          "title": "A Survey of Knowledge Enhanced Pre-trained Models"
        },
        {
          "paperId": "e55391a9406245584b3e5b3225dad2e171b9a06b",
          "title": "RuleBERT: Teaching Soft Rules to Pre-Trained Language Models"
        },
        {
          "paperId": "e113570afc29562bfde48f89fbb5efa624610573",
          "title": "Distilling Relation Embeddings from Pretrained Language Models"
        },
        {
          "paperId": "1a40d4c7ee1044618c08e9f3cfee19eb2c12071d",
          "title": "SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based Sentiment Analysis"
        },
        {
          "paperId": "6cc9484612ab146c9fed9f7dce283c815af3cbc8",
          "title": "Not All Models Localize Linguistic Knowledge in the Same Place: A Layer-wise Probing on BERToids’ Representations"
        },
        {
          "paperId": "ceef266c59698999c9283a0cda852d8bc1ce27ea",
          "title": "How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy"
        },
        {
          "paperId": "884c7aac3358a6f91887dd0d091759963764bedd",
          "title": "A Bayesian Framework for Information-Theoretic Probing"
        },
        {
          "paperId": "58c7a31bf47948d936de937b1cf7b49463608557",
          "title": "NumGPT: Improving Numeracy Ability of Generative Pre-trained Models"
        },
        {
          "paperId": "26c60f8ffb0d66d8732d22af6f5b539e49f2a1e6",
          "title": "Discover AI Knowledge to Preserve Cultural Heritage"
        },
        {
          "paperId": "e6f94081276a7a5e6aef34a080cb3d3a4b1b9c20",
          "title": "Rethinking Why Intermediate-Task Fine-Tuning Works"
        },
        {
          "paperId": "9fabd0280e910cf4608dc27dc0268d5b8ed2d3ba",
          "title": "SAPPHIRE: Approaches for Enhanced Concept-to-Text Generation"
        },
        {
          "paperId": "ad0e6dfee9be05e2c9021779f2f995a3caeb3642",
          "title": "How Can the [MASK] Know? The Sources and Limitations of Knowledge in BERT"
        },
        {
          "paperId": "896c76bf99de9c1ba6fc3dfb695dfa66e73e1eb3",
          "title": "Probing Pre-Trained Language Models for Disease Knowledge"
        },
        {
          "paperId": "7747ecbc26b1688e6cad1a6ce83914efa2a3c04c",
          "title": "Prompting Contrastive Explanations for Commonsense Reasoning Tasks"
        },
        {
          "paperId": "d65a064eb837f838faf6ff67781b62450b92b159",
          "title": "CommonsenseQA 2.0: Exposing the Limits of AI through Gamification"
        },
        {
          "paperId": "5aab57cc0530560d82c74c055f664280619d7e81",
          "title": "PROST: Physical Reasoning about Objects through Space and Time"
        },
        {
          "paperId": "f62acd332fd7a6f35b117ed4ffaf93b19483dcf7",
          "title": "Can Generative Pre-trained Language Models Serve As Knowledge Bases for Closed-book QA?"
        },
        {
          "paperId": "143310c074eb09d5e60adea4c42250dbe03bf9f2",
          "title": "Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution"
        },
        {
          "paperId": "38cfcd4681cf85ab507ec0586c753182a4c8eecb",
          "title": "John praised Mary because _he_? Implicit Causality Bias and Its Interaction with Explicit Cues in LMs"
        },
        {
          "paperId": "c9343c26a0e604f7afd94b7290bbdf8d96cd65b6",
          "title": "Predicting Numerals in Natural Language Text Using a Language Model Considering the Quantitative Aspects of Numerals"
        },
        {
          "paperId": "bc5f9a01a0b9e3015c094ff92f5ff1863612a2b7",
          "title": "What BERTs and GPTs know about your brand? Probing contextual language models for affect associations"
        },
        {
          "paperId": "84c8d939c765dd30574e6c7e6d0a3eb82db1c8fb",
          "title": "ERNIE-NLI: Analyzing the Impact of Domain-Specific External Knowledge on Enhanced Representations for NLI"
        },
        {
          "paperId": "489ffd70cb2afd550ab809bc90f5a766eb07aa80",
          "title": "Inside ASCENT: Exploring a Deep Commonsense Knowledge Base and its Usage in Question Answering"
        },
        {
          "paperId": "12b336abe8b9889bfd2b82ff790e53603a899cbe",
          "title": "Inspecting the concept knowledge graph encoded by modern language models"
        },
        {
          "paperId": "408cc1103ab953a26c7071ffd9ce808469f77a01",
          "title": "Understanding by Understanding Not: Modeling Negation in Language Models"
        },
        {
          "paperId": "4f92221c7cedb1bd6212276e1c122dcac9860750",
          "title": "Regression Bugs Are In Your Model! Measuring, Reducing and Analyzing Regressions In NLP Model Updates"
        },
        {
          "paperId": "befbef9f4e4c6269fa712294430ff916cf2fd51c",
          "title": "Let’s Play Mono-Poly: BERT Can Reveal Words’ Polysemy Level and Partitionability into Senses"
        },
        {
          "paperId": "ccbc69b6e187b9a1614f76c5ede30b139f3dc9fb",
          "title": "Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks"
        },
        {
          "paperId": "db74b20baa82dbcaf641f82fb776e71e581f679b",
          "title": "Novel Aficionados and Doppelgängers: a referential task for semantic representations of individual entities"
        },
        {
          "paperId": "1c0f8a6b8e6f4cc6fc81c4019fc07a2e5ec17107",
          "title": "Probing for Bridging Inference in Transformer Language Models"
        },
        {
          "paperId": "b2816194217673d0b3be5db0b04666cb2f4cdbf5",
          "title": "Modeling Age of Acquisition Norms Using Transformer Networks"
        },
        {
          "paperId": "0672f88d5dc762002b515ca4a0a9f101017fea35",
          "title": "Probing Across Time: What Does RoBERTa Know and When?"
        },
        {
          "paperId": "2ef4be35f8424ea768aa2e1b44392b3eddbc780b",
          "title": "Back to Square One: Artifact Detection, Training and Commonsense Disentanglement in the Winograd Schema"
        },
        {
          "paperId": "c5943bc4b22a63f595cb1c2823a449e03aad4787",
          "title": "AR-LSAT: Investigating Analytical Reasoning of Text"
        },
        {
          "paperId": "209f9bde2dee7cf1677801586562ffe56d435d38",
          "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts"
        },
        {
          "paperId": "2e2ea29356e006fdbedb7592caf5090260f81f1e",
          "title": "What’s in Your Head? Emergent Behaviour in Multi-Task Transformer Models"
        },
        {
          "paperId": "00c209ea764f709a5ce7d7fdc16c2352551bfa83",
          "title": "DirectProbe: Studying Representations without Classifiers"
        },
        {
          "paperId": "d331de3b6bebb0f9af1fddf1b730ec057a7026d4",
          "title": "Relational World Knowledge Representation in Contextual Language Models: A Review"
        },
        {
          "paperId": "8b652c4d7a8d5836925ce0fe28a91dc661778524",
          "title": "What's the best place for an AI conference, Vancouver or ______: Why completing comparative questions is difficult"
        },
        {
          "paperId": "b265827019f420b44c79fd87be1cc6000329c762",
          "title": "Exploring the Role of BERT Token Representations to Explain Sentence Probing Results"
        },
        {
          "paperId": "008b9fc834f5839a25febe150f3076d550ee442f",
          "title": "Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2"
        },
        {
          "paperId": "54df9a3f12fa4810a94a7a5929d0cc7a672b13c4",
          "title": "Information to Wisdom: Commonsense Knowledge Extraction and Compilation"
        },
        {
          "paperId": "2cc3ab9fa41ba2804e301f7eae9598636e62422a",
          "title": "Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"
        },
        {
          "paperId": "69a72ff5b30642d11c96635e99aadad3140d33a7",
          "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"
        },
        {
          "paperId": "73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19",
          "title": "Measuring and Improving Consistency in Pretrained Language Models"
        },
        {
          "paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
          "title": "Making Pre-trained Language Models Better Few-shot Learners"
        },
        {
          "paperId": "634e8fbeba53d45828846dd541ce0a0078c57b68",
          "title": "Syntax-Enhanced Pre-trained Model"
        },
        {
          "paperId": "1d7f3297924a9dd90cfc0df522ebe9138c28b46f",
          "title": "Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals"
        },
        {
          "paperId": "33422275fbb9958f55419620697faf531482699b",
          "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering"
        },
        {
          "paperId": "bc87279d4b32a425377ff18ab63f7ecf95ff228c",
          "title": "Rethinking embedding coupling in pre-trained language models"
        },
        {
          "paperId": "14dddd1d8cb2e8c5f4e9998fef84e715cb321ac9",
          "title": "Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI"
        },
        {
          "paperId": "01400290c7db96c4d665d1c29519c42ba47401e0",
          "title": "A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks"
        },
        {
          "paperId": "fc97c3f375c7228a1df7caa5c0ce5d2a6a171bd7",
          "title": "What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams"
        },
        {
          "paperId": "4d766ae60fbbaea4f6c4ca8ad7f14569ec3189d1",
          "title": "Critical Thinking for Language Models"
        },
        {
          "paperId": "a6a83754a0d1e9a8e41c1e9bbdbca32d3b9d1fd3",
          "title": "It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners"
        },
        {
          "paperId": "bde0c85ed3d61de2a8874ddad70497b3d68bc8ad",
          "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"
        },
        {
          "paperId": "011869f932f89d047ce2bd36d73a95cc04888193",
          "title": "RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms"
        },
        {
          "paperId": "567469dc08fbb702df9f525637e9a3fc43bb1fbb",
          "title": "Probing Contextual Language Models for Common Ground with Visual Representations"
        },
        {
          "paperId": "4f03e69963b9649950ba29ae864a0de8c14f1f86",
          "title": "K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters"
        },
        {
          "paperId": "8ae9a17c87a4518b513e860683a0ef7824be994d",
          "title": "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"
        },
        {
          "paperId": "c2d50f17ea6769f6f5663ccac37a9627a0543184",
          "title": "Mathematical Reasoning in General Artificial Intelligence Workshop , ICLR 2021"
        },
        {
          "paperId": "42fffebf899440941faf558dfef001c85c5c442d",
          "title": "BERT, XLNet or RoBERTa: The Best Transfer Learning Model to Detect Clickbaits"
        },
        {
          "paperId": "593284010379e02f6ab57e7208b5511185ce8c0e",
          "title": "Probing Pre-trained Language Models for Semantic Attributes and their Values"
        },
        {
          "paperId": "61cd4ffdaf2c0daa3d432ff9fecdd064d6e72886",
          "title": "Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI"
        },
        {
          "paperId": "07f6f423950210e7eecc3affc2499a9b59d346df",
          "title": "Test Harder than You Train: Probing with Extrapolation Splits"
        },
        {
          "paperId": "1841cf23c65ff2f27f21ba0d2268c3445f20332f",
          "title": "Few-Shot Text Generation with Natural Language Instructions"
        },
        {
          "paperId": "b7cd1d84baaa3a65be4216e39086e034b8432845",
          "title": "Exploring the Use of Neural Transformers for Psycholinguistics"
        },
        {
          "paperId": "1db86e01300e2f30fd08b46e63ea11656cb6dcf5",
          "title": "TGEA: An Error-Annotated Dataset and Benchmark Tasks for TextGeneration from Pretrained Language Models"
        },
        {
          "paperId": "7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8",
          "title": "AND does not mean OR: Using Formal Languages to Study Language Models’ Representations"
        },
        {
          "paperId": "291a00d8433fecd2dd10f7f13b62dae8ce500043",
          "title": "Analyzing the Forgetting Problem in Pretrain-Finetuning of Open-domain Dialogue Response Models"
        },
        {
          "paperId": "1bbbe12b060f54a97a4e81fd23cbb7932ff9de53",
          "title": "Language Models have a Moral Dimension"
        },
        {
          "paperId": "eb2fc03b8865b8e1b4cb933d917ea269ebe14584",
          "title": "Learning from Mistakes: Using Mis-predictions as Harm Alerts in Language Pre-Training"
        },
        {
          "paperId": "9ba6ad0de7dbe1a3b10c44106049adb96f87d483",
          "title": "KgPLM: Knowledge-guided Language Model Pre-training via Generative and Discriminative Learning"
        },
        {
          "paperId": "9d1f9406ed676171d9975e27606c95633ca898b1",
          "title": "On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in BERT"
        },
        {
          "paperId": "d865c87f6ce4e0be29ae1e3780fae66b8034d04b",
          "title": "TR at SemEval-2020 Task 4: Exploring the Limits of Language-model-based Common Sense Validation"
        },
        {
          "paperId": "6906c562e182ea22c086b2bb44ed9cec8602a220",
          "title": "Classifier Probes May Just Learn from Linear Context Features"
        },
        {
          "paperId": "42595cb533b03ad44738102fb0c3cef3e4b5c27c",
          "title": "Controlling the Imprint of Passivization and Negation in Contextualized Representations"
        },
        {
          "paperId": "c0c6f3ba310099b9a1ebe899f5f58bb04574df97",
          "title": "Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification"
        },
        {
          "paperId": "15bcf3b7aa6511a55d7066419453a6d5906b2db8",
          "title": "It’s not Greek to mBERT: Inducing Word-Level Translations from Multilingual BERT"
        },
        {
          "paperId": "5b71bcf769e7efab90ceba56b9e4f898899538fe",
          "title": "BERT Knows Punta Cana Is Not Just Beautiful, It’s Gorgeous: Ranking Scalar Adjectives with Contextualised Representations"
        },
        {
          "paperId": "b4013141cceb937c46ea5f84f8c06f6bf1215106",
          "title": "PRover: Proof Generation for Interpretable Reasoning over Rules"
        },
        {
          "paperId": "1c6f94fb3d888167355afb580f04d55cd517ebc6",
          "title": "On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers"
        },
        {
          "paperId": "4571feb26d903053661efd2f2f144e010902f458",
          "title": "Discern: Discourse-Aware Entailment Reasoning Network for Conversational Machine Reading"
        },
        {
          "paperId": "ee5fff85d3ec62698eddba162f054b7e73670b2a",
          "title": "Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics"
        },
        {
          "paperId": "260cce438595c708433719a75c72889fefa5f731",
          "title": "Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA"
        },
        {
          "paperId": "15aa86556be1579eadf8fbb0bc486f9427e681f0",
          "title": "Evaluating representations by the complexity of learning low-loss predictors"
        },
        {
          "paperId": "64da659c0687762359226b4cf455520c78acd165",
          "title": "Neural Language Generation: Formulation, Methods, and Evaluation"
        },
        {
          "paperId": "cc4db47a416aba22d1073e59a6867b6997928b7c",
          "title": "What does BERT know about books, movies and music? Probing BERT for Conversational Recommendation"
        },
        {
          "paperId": "a3aa1562e72aecaa06bcec4613cc3d020f4b8e49",
          "title": "Pre-trained Language Models as Symbolic Reasoners over Knowledge?"
        },
        {
          "paperId": "6f2b90ee5a0feea87264148c25a874f84bae20a0",
          "title": "Are Pretrained Language Models Symbolic Reasoners over Knowledge?"
        },
        {
          "paperId": "79e5447a394ad178aa0fdaef1d2f1ecf9bf07e88",
          "title": "Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge"
        },
        {
          "paperId": "c8b00d4706fc8979a9c5f410addccbcfe1c0d894",
          "title": "When Bert Forgets How To POS: Amnesic Probing of Linguistic Properties and MLM Predictions"
        },
        {
          "paperId": "26cfb57a9722599b361858d454ec816420723e36",
          "title": "Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models"
        },
        {
          "paperId": "7a5d33bccf1f7fa6d00c070f6a6ed22467267368",
          "title": "Commonsense Evidence Generation and Injection in Reading Comprehension"
        },
        {
          "paperId": "3c2f6c2cf7c9b121a0e46a660846bb0a5dad0ee8",
          "title": "DQI: Measuring Data Quality in NLP"
        },
        {
          "paperId": "688e324770a8c3db0af25968bd15e2cc33b9a83d",
          "title": "Can BERT Reason? Logically Equivalent Probes for Evaluating the Inference Capabilities of Language Models"
        },
        {
          "paperId": "016760dc4a05489ddf5dbb48aecbb49e214e1b71",
          "title": "Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"
        },
        {
          "paperId": "467ac47b2e01ce6ae74e8d70561ca0f8f66c7b8c",
          "title": "Probing Text Models for Common Ground with Visual Representations"
        },
        {
          "paperId": "0ebb1d1fbf488fba8c18a5a6057a6ccd9e87510f",
          "title": "Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models"
        },
        {
          "paperId": "c30b457fdfb0623b87379de79ffaa570a7f3bb48",
          "title": "Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation"
        },
        {
          "paperId": "2297c559ac3509b3ab456229f7032b2a0fbf23c1",
          "title": "Pretraining on Non-linguistic Structure as a Tool for Analyzing Learning Bias in Language Models"
        },
        {
          "paperId": "196c558ce126f5f7d66df4ea52e2442848fc65be",
          "title": "Asking without Telling: Exploring Latent Ontologies in Contextual Representations"
        },
        {
          "paperId": "1e391e60081dcb148cc2732e1f8aa26655fef31e",
          "title": "Explaining Question Answering Models through Text Generation"
        },
        {
          "paperId": "774319233a107a29622003a115aa6c79f4a7b37f",
          "title": "Probing Neural Language Models for Human Tacit Assumptions"
        },
        {
          "paperId": "885e52a5b1a4fb36fae0e3fe31e9d4da21e03b41",
          "title": "On the Existence of Tacit Assumptions in Contextualized Language Models"
        },
        {
          "paperId": "f4b585c9a79dfce0807b445a09036ea0f9cbcdce",
          "title": "Information-Theoretic Probing with Minimum Description Length"
        },
        {
          "paperId": "bd20069f5cac3e63083ecf6479abc1799db33ce0",
          "title": "A Primer in BERTology: What We Know About How BERT Works"
        },
        {
          "paperId": "0fe2636446cd686830da3d971b31a004d6094b3c",
          "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages"
        },
        {
          "paperId": "c44120f765fc43994c5cfb4e12e4f62999efeae6",
          "title": "How Context Affects Language Models' Factual Predictions"
        },
        {
          "paperId": "15ad2b27c5248e7d1db5456794ca1ca8a8198f5d",
          "title": "Transformers as Soft Reasoners over Language"
        },
        {
          "paperId": "80376bdec5f534be78ba82821f540590ebce5559",
          "title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?"
        },
        {
          "paperId": "5a9001cdccdb8b1de227a45eccc503d32d1a2464",
          "title": "What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge"
        },
        {
          "paperId": "81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85",
          "title": "How Can We Know What Language Models Know?"
        },
        {
          "paperId": "db88987a25e036a9fd8b69f8575e0a75d63b260b",
          "title": "On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers"
        },
        {
          "paperId": "adc79a6040c49f048cf4ed81fbc441fef661d5ed",
          "title": "End-to-end Dialog Systems with Numerical Slot Filling"
        },
        {
          "paperId": "37bf0bf34603145246c3311df19e2afdf6e0270a",
          "title": "Preprint 1 Masked entity prediction : Earth → Q 2 : Earth Earth"
        },
        {
          "paperId": "189b518d70ad34c8de6f613bf3bd5051077608bc",
          "title": "Probing Language Models for Common Ground with Visual Representations"
        },
        {
          "paperId": "5414be4432fcfe1e83932127ee2463d107e5b61c",
          "title": "A Sequence-to-sequence Approach for Numerical Slot-filling Dialog Systems"
        },
        {
          "paperId": "170ec3ac79b81f21ac35247b7f8e73991a14ebac",
          "title": "Can RoBERTa Reason? A Systematic Approach to Probe Logical Reasoning in Language Models"
        }
      ],
      "references": [
        {
          "paperId": "81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85",
          "title": "How Can We Know What Language Models Know?"
        },
        {
          "paperId": "68c1bf884f0fc0e86641466a1f1fa67e79f16a17",
          "title": "Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly"
        },
        {
          "paperId": "207da6d2c07289bf72a2b5974bb3f011ebb5dd0d",
          "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding"
        },
        {
          "paperId": "681fbcd98acf20df3355eff3585994bd1f9008b7",
          "title": "Probing Natural Language Inference Models through Semantic Fragments"
        },
        {
          "paperId": "a0e49f65b6847437f262c59d0d399255101d0b75",
          "title": "What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models"
        },
        {
          "paperId": "83f54abd65634f34e68b30c6fff2a57de7a34f37",
          "title": "Negated LAMA: Birds cannot fly"
        },
        {
          "paperId": "c2c165dd615d4fc31d4fef4b4acbcab1a1655983",
          "title": "On Making Reading Comprehension More Comprehensive"
        },
        {
          "paperId": "4c9a8caf940627126aaa9bd3ac813d07065c86a0",
          "title": "Diversify Your Datasets: Analyzing Generalization via Controlled Variance in Adversarial Datasets"
        },
        {
          "paperId": "8e914370a043fd626e172d55816952bc477bd582",
          "title": "Question Answering is a Format; When is it Useful?"
        },
        {
          "paperId": "0427110f0e79f41e69a8eb00a3ec8868bac26a4f",
          "title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings"
        },
        {
          "paperId": "199ff73d2f728e997f860b62a2322823d3e3d9e8",
          "title": "Designing and Interpreting Probes with Control Tasks"
        },
        {
          "paperId": "0e4cd6bae6ac1017e7b1b9bd644375aee65b8372",
          "title": "Show Your Work: Improved Reporting of Experimental Results"
        },
        {
          "paperId": "3cd331c997e90f737810aad6fcce4d993315189f",
          "title": "Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs"
        },
        {
          "paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3",
          "title": "Language Models as Knowledge Bases?"
        },
        {
          "paperId": "67c6e842ceafda5b31626c773b380f9e97423cd2",
          "title": "Is the Red Square Big? MALeViC: Modeling Adjectives Leveraging Visual Contexts"
        },
        {
          "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
          "paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
          "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
        },
        {
          "paperId": "afd110eace912c2b273e64851c6b4df2658622eb",
          "title": "Visualizing and Measuring the Geometry of BERT"
        },
        {
          "paperId": "165d51a547cd920e6ac55660ad5c404dcb9562ed",
          "title": "Open Sesame: Getting inside BERT’s Linguistic Knowledge"
        },
        {
          "paperId": "d34e6c84119cef8eb9149a27d6b4903131407ea6",
          "title": "How Large Are Lions? Inducing Distributions over Quantitative Attributes"
        },
        {
          "paperId": "455a8838cde44f288d456d01c76ede95b56dc675",
          "title": "A Structural Probe for Finding Syntax in Word Representations"
        },
        {
          "paperId": "f5309ce8a1ba8e47bb9d01e83094d88ac8d3f3e1",
          "title": "Where’s My Head? Definition, Data Set, and Models for Numeric Fused-Head Identification and Resolution"
        },
        {
          "paperId": "e46a16c0bdfeded39977a9f356cbea148f10bed9",
          "title": "Knowledge of animal appearance among sighted and blind adults"
        },
        {
          "paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c",
          "title": "BERT Rediscovers the Classical NLP Pipeline"
        },
        {
          "paperId": "e2587eddd57bc4ba286d91b27c185083f16f40ee",
          "title": "What do you learn from context? Probing for sentence structure in contextualized word representations"
        },
        {
          "paperId": "a5ec883e080d858b5df60f1b5d711d514459b1e4",
          "title": "Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition"
        },
        {
          "paperId": "19281b9ecdb5c07a93423a506627ab9d9b0cf039",
          "title": "Learning and Evaluating General Linguistic Intelligence"
        },
        {
          "paperId": "efeab0dcdb4c1cce5e537e57745d84774be99b9a",
          "title": "Assessing BERT's Syntactic Abilities"
        },
        {
          "paperId": "668f42a4d4094f0a66d402a16087e14269b31a1f",
          "title": "Analysis Methods in Neural Language Processing: A Survey"
        },
        {
          "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
          "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
          "paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
          "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"
        },
        {
          "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
          "paperId": null,
          "title": "Hannaneh Hajishirzi, Alon Talmor, and Sewon Min"
        },
        {
          "paperId": "22655979df781d222eaf812b0d325fa9adf11594",
          "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"
        },
        {
          "paperId": "ac11062f1f368d97f4c826c317bf50dcc13fdb59",
          "title": "Dissecting Contextual Word Embeddings: Architecture and Representation"
        },
        {
          "paperId": "af5c4b80fbf847f69a202ba5a780a3dd18c1a027",
          "title": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"
        },
        {
          "paperId": "1536e8958697c5364f68b2e2448905dbbeb3a0ca",
          "title": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"
        },
        {
          "paperId": "c8725f13be7434b69738491c66b45c9225258253",
          "title": "The Web as a Knowledge-Base for Answering Complex Questions"
        },
        {
          "paperId": "c423c2e5f3a0a931f113da53c184542a0fc5a1db",
          "title": "The Description Length of Deep Learning models"
        },
        {
          "paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f",
          "title": "Deep Contextualized Word Representations"
        },
        {
          "paperId": "7d5cf22c70484fe217936c66741fb73b2a278bde",
          "title": "Constructing Datasets for Multi-hop Reading Comprehension Across Documents"
        },
        {
          "paperId": "8eb9b2ea146e3a381a29b4a9314c36f61b71e367",
          "title": "Extracting Commonsense Properties from Embeddings with Limited Human Guidance"
        },
        {
          "paperId": "15bce8864a15c20cf483e18099f3601b94673565",
          "title": "Premise Selection for Theorem Proving by Deep Graph Embedding"
        },
        {
          "paperId": "c9f343b492c170c726f607c255ec6c7177dc5800",
          "title": "Verb Physics: Relative Physical Knowledge of Actions and Objects"
        },
        {
          "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
          "title": "Attention is All you Need"
        },
        {
          "paperId": "26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810",
          "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"
        },
        {
          "paperId": "83e7654d545fbbaaf2328df365a781fb67b841b4",
          "title": "Enhanced LSTM for Natural Language Inference"
        },
        {
          "paperId": "e44da7d8c71edcc6e575fa7faadd5e75785a7901",
          "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks"
        },
        {
          "paperId": "3aa52436575cf6768a0a1a476601825f6a62e58f",
          "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies"
        },
        {
          "paperId": "53d43ccc593bf44e9aa52e3971df1b9dd396e30d",
          "title": "Probing for semantic evidence of composition by means of simple classification tasks"
        },
        {
          "paperId": "1c3884e43bba39c50e6172c403c057659ef3ce83",
          "title": "Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects"
        },
        {
          "paperId": "1af68821518f03568f913ab03fc02080247a27ff",
          "title": "Neural Machine Translation of Rare Words with Subword Units"
        },
        {
          "paperId": "8a4257052ead8da5773afeff3aa6b6291ca20f60",
          "title": "Building a shared world: mapping distributional to model-theoretic semantic spaces"
        },
        { "paperId": null, "title": "Semisupervised sequence learning" },
        {
          "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
          "title": "GloVe: Global Vectors for Word Representation"
        },
        {
          "paperId": "5b35981142209ad7d003ec4341b64f9a04005e10",
          "title": "Wikidata"
        },
        {
          "paperId": "cceb698cbbb828537f2f195fb70b6fdc586d3327",
          "title": "Reporting bias and knowledge acquisition"
        },
        {
          "paperId": "b3c41f269f77726e3f480915af017c4527d15f4f",
          "title": "Donald Davidson's truth-theoretic semantics"
        },
        {
          "paperId": "d87ceda3042f781c341ac17109d1e94a717f5f60",
          "title": "WordNet : an electronic lexical database"
        },
        {
          "paperId": "e40e4a3f4cffa628e98097107f16be69dcf81262",
          "title": "Generalized quantifiers and natural language"
        },
        {
          "paperId": "53a139116437f716b17e60b0b8775708ea597ff5",
          "title": "Formal semantics of Natural Language: Adverbs of quantification"
        }
      ],
      "summary": "This work proposes eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition, and findings can help future work on designing new datasets, models, and objective functions for pre-training.",
      "title": "oLMpics-On What Language Model Pre-training Captures",
      "venue": "Transactions of the Association for Computational Linguistics",
      "year": 2019,
      "authors": "Alon Talmor,Yanai Elazar,Yoav Goldberg,Jonathan Berant"
    },
    {
      "id": "e081dfa64e5ed12fe15fc885ec82ef1f35ab9830",
      "url": "https://www.semanticscholar.org/paper/e081dfa64e5ed12fe15fc885ec82ef1f35ab9830",
      "citationCount": 7,
      "influentialCitationCount": 0,
      "referenceCount": 24,
      "citations": [
        {
          "paperId": "d6c6f61725cd10636821797f8b4aec0efc51ad98",
          "title": "From Analog to Digital Computing: Is Homo sapiens’ Brain on Its Way to Become a Turing Machine?"
        },
        {
          "paperId": "c36277b67814e0a522e786a38e1768612b0e63f2",
          "title": "Exploring the Learning Mechanisms of Neural Division Modules"
        },
        {
          "paperId": "0ce6a798f8222ed8f221326ca566311c648cb4dc",
          "title": "Learning Division with Neural Arithmetic Logic Modules"
        },
        {
          "paperId": "def7c67c3688bc459fbef3ce50a466a0cbf411a9",
          "title": "A Primer for Neural Arithmetic Logic Modules"
        },
        {
          "paperId": "0bfd4ed399054eae26c3cdaabc0aed80ca95e125",
          "title": "Neural Power Units"
        },
        {
          "paperId": "4599f96dd1a2584e00d342953fc7e1361ffd6e1f",
          "title": "Neural Status Registers"
        },
        {
          "paperId": "3653d82b9814dd5e64328282004a077b52c2a99e",
          "title": "Financial Fraud Detection with Improved Neural Arithmetic Logic Units"
        }
      ],
      "references": [
        {
          "paperId": "112ac68ddb0f021517dd465e89918fa52755cc35",
          "title": "Measuring Arithmetic Extrapolation Performance"
        },
        {
          "paperId": "8b85b7eedea2d6790e450c10f296255ac4152d32",
          "title": "Flow-based Network Traffic Generation using Generative Adversarial Networks"
        },
        {
          "paperId": "6f69e19348870552a7ab92d038c8b8d753fe6b60",
          "title": "Neural Arithmetic Expression Calculator"
        },
        {
          "paperId": "5fc548f3f3112de7eddad3744717dc2f9d22ca38",
          "title": "Neural Arithmetic Logic Units"
        },
        {
          "paperId": "da895fa70d45ba06efae6cb128e885ab49c5e977",
          "title": "Microscopy cell counting and detection with fully convolutional regression networks"
        },
        { "paperId": null, "title": "Microscopy cell counting" },
        {
          "paperId": "acd87843a451d18b4dc6474ddce1ae946429eaf1",
          "title": "Wasserstein Generative Adversarial Networks"
        },
        {
          "paperId": "618784f1c811dd3df9a0fb95de9868575b38f7a4",
          "title": "Improving the Neural GPU Architecture for Algorithm Learning"
        },
        {
          "paperId": "3a1d2057543cdf2a771f7421c8deab280a04779d",
          "title": "Paysim: a financial mobile money simulator for fraud detection"
        },
        {
          "paperId": "5e4eb58d5b47ac1c73f4cf189497170e75ae6237",
          "title": "Neural GPUs Learn Algorithms"
        },
        {
          "paperId": "b59d91e0699d4e1896a15bae13fd180bdaf77ea5",
          "title": "Neural Programmer-Interpreters"
        },
        {
          "paperId": "5b791cd374c7109693aaddee2c12d659ae4e3ec0",
          "title": "Grid Long Short-Term Memory"
        },
        {
          "paperId": "b0e52226bc29275698f35283060a97689b373490",
          "title": "Cross-scene crowd counting via deep convolutional neural networks"
        },
        {
          "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
          "title": "Adam: A Method for Stochastic Optimization"
        },
        {
          "paperId": "6775ba237f3c3d077ba3639d96cb307ce2bce260",
          "title": "Microscopy cell counting with fully convolutional regression networks"
        },
        { "paperId": null, "title": "Cross-scene crowd" },
        {
          "paperId": "0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a",
          "title": "Learning to Execute"
        },
        {
          "paperId": "eabba0b46f4325d28d6e02f4ebbf70f40a1263a5",
          "title": "An empirical comparison of botnet detection methods"
        },
        { "paperId": null, "title": "Learning to execute. arXiv [Preprint" },
        {
          "paperId": "b71ac1e9fb49420d13e084ac67254a0bbd40f83f",
          "title": "Understanding the difficulty of training deep feedforward neural networks"
        },
        {
          "paperId": "d9665992ee36699b8ae4a2e2294552cd4be9003a",
          "title": "Statistical fraud detection: A review"
        },
        {
          "paperId": null,
          "title": "NALU (v) U(-5, 5), (-10, -5) (14) iNALU (sw) E0.2, 0.5 (15) iNALU (sw) E0.8, 0.5 (16) iNALU (sw) N(-2, 4)"
        },
        { "paperId": null, "title": "NALU (m) E0.2, 0" }
      ],
      "summary": "An in-depth analysis reveals practical shortcomings by design, such as the inability to multiply or divide negative input values or training stability issues for deeper networks, and proposes an improved model architecture that solves stability issues and outperforms the original NALU model in means of arithmetic precision and convergence.",
      "title": "iNALU: Improved Neural Arithmetic Logic Unit",
      "venue": "Frontiers in Artificial Intelligence",
      "year": 2020,
      "authors": "Daniel Schlör,Markus Ring,A. Hotho"
    },
    {
      "id": "716efab73e1916fdc2a23727b581d200271ed499",
      "url": "https://www.semanticscholar.org/paper/716efab73e1916fdc2a23727b581d200271ed499",
      "citationCount": 2,
      "influentialCitationCount": 0,
      "referenceCount": 16,
      "citations": [
        {
          "paperId": "28a5a53dafacebad8a7c47773079caeffb9a5baa",
          "title": "Representing Numbers in NLP: a Survey and a Vision"
        },
        {
          "paperId": "04b748f940147bdd84a9047ad70ed1e9985d9770",
          "title": "An Improvisational Approach to Acquire Social Interactions"
        }
      ],
      "references": [
        {
          "paperId": "63748e59f4e106cbda6b65939b77589f40e48fcb",
          "title": "Text Summarization with Pretrained Encoders"
        },
        {
          "paperId": "5a9b1a72c7ef52b756ce022176f1496279e9482c",
          "title": "Intelligent Tutoring System for Negotiation Skills Training"
        },
        {
          "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
          "title": "Decoupled Weight Decay Regularization"
        },
        {
          "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
          "paperId": "b626754a0fd7de12c87e88165b2484ac5d98212a",
          "title": "Decoupling Strategy and Generation in Negotiation Dialogues"
        },
        {
          "paperId": "112d8c462be072de121c13a523535f94f0256383",
          "title": "How People Negotiate? From the Analysis of a Dialogue Corpus to a Dialogue System"
        },
        {
          "paperId": "bb669de2fce407df2f5cb2f8c51dedee3f467e04",
          "title": "The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation"
        },
        {
          "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
          "title": "Attention is All you Need"
        },
        {
          "paperId": "6a8dbea5e40831bd6e987c03b76487f45ac49599",
          "title": "Deal or No Deal? End-to-End Learning of Negotiation Dialogues"
        },
        {
          "paperId": "4d025da955a51350df56a54bbb59c1c1b7630e2b",
          "title": "Discourse Structure and Dialogue Acts in Multiparty Dialogue: the STAC Corpus"
        },
        {
          "paperId": "34b4c12146e6aab1309bfe91a86ff3fa76d1677a",
          "title": "Negotiation as a Challenge Problem for Virtual Humans"
        },
        {
          "paperId": "b8a6025630a0972f8a8bbf28376040951d109cfc",
          "title": "GENIUS: AN INTEGRATED ENVIRONMENT FOR SUPPORTING THE DESIGN OF GENERIC AUTOMATED NEGOTIATORS"
        },
        { "paperId": null, "title": "Genius: An integrated environment" },
        {
          "paperId": "bde21379dab0ccb52c96d1fb649fff4ed11f3e92",
          "title": "IAMhaggler: A Negotiation Agent for Complex Environments"
        },
        {
          "paperId": "cb826a3899752b796f14df1c50378c64954a6b0a",
          "title": "Statistical Significance Tests for Machine Translation Evaluation"
        },
        {
          "paperId": "395171da2b9c0617f2cc7adba9bbf13706effd9b",
          "title": "Negotiation behavior when cultures collide: the United States and Japan."
        }
      ],
      "summary": "This model is able to predict a negotiation's outcome within 10% for more than 70% of the cases, and suggests that rather than just being a way to realize a negotiation, natural language should be incorporated in the negotiation planning as well.",
      "title": "BERT in Negotiations: Early Prediction of Buyer-Seller Negotiation Outcomes",
      "venue": "ArXiv",
      "year": 2020,
      "authors": "Kushal Chawla,Gale M. Lucas,J. Gratch,Jonathan May"
    },
    {
      "id": "4599f96dd1a2584e00d342953fc7e1361ffd6e1f",
      "url": "https://www.semanticscholar.org/paper/4599f96dd1a2584e00d342953fc7e1361ffd6e1f",
      "citationCount": 5,
      "influentialCitationCount": 1,
      "referenceCount": 51,
      "citations": [
        {
          "paperId": "4578747d52aa1b3537612287352a803a7b17e999",
          "title": "Asynchronous Neural Networks for Learning in Graphs"
        },
        {
          "paperId": "c36277b67814e0a522e786a38e1768612b0e63f2",
          "title": "Exploring the Learning Mechanisms of Neural Division Modules"
        },
        {
          "paperId": "def7c67c3688bc459fbef3ce50a466a0cbf411a9",
          "title": "A Primer for Neural Arithmetic Logic Modules"
        },
        {
          "paperId": "30fcba6760b5fc5235ef8a0f2a61ccff63a7c3a0",
          "title": "MC-LSTM: Mass-Conserving LSTM"
        },
        {
          "paperId": "0bfd4ed399054eae26c3cdaabc0aed80ca95e125",
          "title": "Neural Power Units"
        }
      ],
      "references": [
        {
          "paperId": "6b989b8327db3a7212141c59c1569f0219775058",
          "title": "How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks"
        },
        {
          "paperId": "e081dfa64e5ed12fe15fc885ec82ef1f35ab9830",
          "title": "iNALU: Improved Neural Arithmetic Logic Unit"
        },
        {
          "paperId": "4aea918d9bd66440ce0c00abbfbb57b212d76158",
          "title": "Neural Arithmetic Units"
        },
        {
          "paperId": "b39eed03d345f5c244eac12fd1315d26eba77d62",
          "title": "Deep Learning for Symbolic Mathematics"
        },
        {
          "paperId": "a4a1a70b7cd477de254f88662787406dcd40d8bb",
          "title": "Neural Execution of Graph Algorithms"
        },
        {
          "paperId": "640a08c0bf0f69cd659bae3fcee7ad359d8eee7c",
          "title": "Neural Stored-program Memory"
        },
        {
          "paperId": null,
          "title": "Neural arithmetic units. In: 8th International Conference on Learning Representations (ICLR), Addis Ababa, Ethiopia"
        },
        {
          "paperId": "8331c6613c175e98fb2226523e8aabfb36fb2408",
          "title": "Towards Scale-Invariant Graph-related Problem Solving by Iterative Homogeneous GNNs"
        },
        {
          "paperId": "3c8a456509e6c0805354bd40a35e3f2dbf8069b1",
          "title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
        },
        {
          "paperId": "112ac68ddb0f021517dd465e89918fa52755cc35",
          "title": "Measuring Arithmetic Extrapolation Performance"
        },
        {
          "paperId": "3a6447361b20c249f5306ae17dee43f645430e31",
          "title": "Neural Logic Machines"
        },
        {
          "paperId": "afed6dc6900d3b37e528b9086661bba583d60bf6",
          "title": "Analysing Mathematical Reasoning Abilities of Neural Models"
        },
        {
          "paperId": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60",
          "title": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"
        },
        {
          "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
          "paperId": "f9717d29840f4d8f1cc19d1b1e80c5d12ec40608",
          "title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"
        },
        {
          "paperId": "e53fd0c9a04df127af80b2699f5f8f23a8a3e2af",
          "title": "On Evaluating the Generalization of LSTM Models in Formal Languages"
        },
        {
          "paperId": "6f69e19348870552a7ab92d038c8b8d753fe6b60",
          "title": "Neural Arithmetic Expression Calculator"
        },
        {
          "paperId": "5fc548f3f3112de7eddad3744717dc2f9d22ca38",
          "title": "Neural Arithmetic Logic Units"
        },
        {
          "paperId": "acc43abe319bca7652a91f7d4ca6187049fb82e4",
          "title": "Measuring abstract reasoning in neural networks"
        },
        {
          "paperId": "660f08f5aa355c19a851d848e0c1321c5c32c1ab",
          "title": "Learning Equations for Extrapolation and Control"
        },
        {
          "paperId": "4df7bbe3ca7806f39a490c99f17867a0ac299bc3",
          "title": "Learning Explanatory Rules from Noisy Data"
        },
        {
          "paperId": "856fe866bcce5e7a540655bea6ecc7406bdcfcba",
          "title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"
        },
        {
          "paperId": "0c253bb9aee9aa1ae7909700eda845bd3124197f",
          "title": "Neural Program Meta-Induction"
        },
        {
          "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
          "title": "Attention is All you Need"
        },
        {
          "paperId": "6b024162f81e8ff7aa34c3a43d601a912d012c78",
          "title": "Making Neural Programming Architectures Generalize via Recursion"
        },
        {
          "paperId": "e24cdf73b3e7e590c2fe5ecac9ae8aa983801367",
          "title": "Neural Message Passing for Quantum Chemistry"
        },
        {
          "paperId": "618784f1c811dd3df9a0fb95de9868575b38f7a4",
          "title": "Improving the Neural GPU Architecture for Algorithm Learning"
        },
        {
          "paperId": "efc2ee2c7a9ec5e798720c29c54512407efe06d0",
          "title": "Neural Functional Programming"
        },
        {
          "paperId": "15064b7e47b43a68b6661bc7c3eaaad207493191",
          "title": "Neural Program Lattices"
        },
        {
          "paperId": "cdc90e091e64b796f5069bda7bbfdc5a04bd6365",
          "title": "Extrapolation and learning equations"
        },
        {
          "paperId": null,
          "title": "Attention is all you need. In: Advances in neural information processing systems"
        },
        {
          "paperId": "3058d4a4fa6ebaac30e0279c6b8c30e5f969d315",
          "title": "Extensions and Limitations of the Neural GPU"
        },
        {
          "paperId": "784ee73d5363c711118f784428d1ab89f019daa5",
          "title": "Hybrid computing using a neural network with dynamic external memory"
        },
        {
          "paperId": "5e4eb58d5b47ac1c73f4cf189497170e75ae6237",
          "title": "Neural GPUs Learn Algorithms"
        },
        {
          "paperId": "49e24b1fd415e20839c09b5dd68a01f61d1681d9",
          "title": "Learning Simple Algorithms from Examples"
        },
        {
          "paperId": "b59d91e0699d4e1896a15bae13fd180bdaf77ea5",
          "title": "Neural Programmer-Interpreters"
        },
        { "paperId": null, "title": "Neural programmerinterpreters" },
        {
          "paperId": "e837b79de602c69395498c1fbbe39bbb4e6f75ad",
          "title": "Learning to Transduce with Unbounded Memory"
        },
        {
          "paperId": "5259755f9c100e220ffaa7e08439c5d34be7757a",
          "title": "Reinforcement Learning Neural Turing Machines - Revised"
        },
        {
          "paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e",
          "title": "End-To-End Memory Networks"
        },
        {
          "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
          "title": "Adam: A Method for Stochastic Optimization"
        },
        {
          "paperId": "71ae756c75ac89e2d731c9c79649562b5768ff39",
          "title": "Memory Networks"
        },
        {
          "paperId": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
          "title": "Going deeper with convolutions"
        },
        {
          "paperId": null,
          "title": "End-to-end memory networks. In: Advances in neural information processing systems"
        },
        {
          "paperId": null,
          "title": "Going deeper with convolutions. In: 31st Computer Vision and Pattern Recognition"
        },
        {
          "paperId": "54e325aee6b2d476bbbb88615ac15e251c6e8214",
          "title": "Generative Adversarial Nets"
        },
        {
          "paperId": "c3823aacea60bc1f2cabb9283144690a3d015db5",
          "title": "Neural Turing Machines"
        },
        {
          "paperId": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
          "title": "ImageNet classification with deep convolutional neural networks"
        },
        {
          "paperId": null,
          "title": "Learning XOR: exploring the space of a classic problem"
        },
        {
          "paperId": null,
          "title": "Learning XOR: exploring the space of a classic problem. Department of Computing Science and Mathematics"
        },
        { "paperId": null, "title": "A method for stochastic optimization" }
      ],
      "summary": "The Neural Status Register is introduced, inspired by physical Status Registers, and at the heart of the NSR are arithmetic comparisons between inputs that allow end-to-end differentiation and learns such comparisons reliably.",
      "title": "Neural Status Registers",
      "venue": "ArXiv",
      "year": 2020,
      "authors": "Lukas Faber,Roger Wattenhofer"
    },
    {
      "id": "45e5d7637a585a87d967a4a357d17c5d89aecea2",
      "url": "https://www.semanticscholar.org/paper/45e5d7637a585a87d967a4a357d17c5d89aecea2",
      "citationCount": 30,
      "influentialCitationCount": 1,
      "referenceCount": 28,
      "citations": [
        {
          "paperId": "9bd14955135aa1bf3927c6011232702d25394352",
          "title": "Interaction of lexical strata in hybrid compound words through gradient phonotactics"
        },
        {
          "paperId": "7412a5a69213021e0d063250a503eb85dd5ec0d4",
          "title": "Neural Networks and the Chomsky Hierarchy"
        },
        {
          "paperId": "bab33a1a464bc237bfeccfaba9bd155d2b90d186",
          "title": "Implicit n-grams Induced by Recurrence"
        },
        {
          "paperId": "0c7b82cef44cbdca4cbefe17a8ba7ca47fbffea9",
          "title": "Exploring a diverse world of effector domains and amyloid signaling motifs in fungal NLR proteins"
        },
        {
          "paperId": "3fa611d27986ba9983df8c4013114b995c1793cc",
          "title": "Extracting Finite Automata from RNNs Using State Merging"
        },
        {
          "paperId": "648069f71d7ab9d4c9a4c44a782e1b5d75179fd9",
          "title": "Stronger Separation of Analog Neuron Hierarchy by Deterministic Context-Free Languages"
        },
        {
          "paperId": "2b0e5a699095184144023cce674e9c88994598a6",
          "title": "Connecting Weighted Automata, Tensor Networks and Recurrent Neural Networks through Spectral Learning"
        },
        {
          "paperId": "33382bb7352b4aabd33a8d2285f5b3ba73861208",
          "title": "Training dynamics of neural language models"
        },
        {
          "paperId": "1781517a91be8248dd0febad65211a1b6614e199",
          "title": "On the Power of Saturated Transformers: A View from Circuit Complexity"
        },
        {
          "paperId": "0735fb79bf34698c1df4461a05ed51c232c412e4",
          "title": "Thinking Like Transformers"
        },
        {
          "paperId": "56f46451a8790a919c7048f869ee9aaac7ae2fd7",
          "title": "Extracting Weighted Automata for Approximate Minimization in Language Modelling"
        },
        {
          "paperId": "26cf06dc782a7c484c5c2463b1c9c7482536c4e4",
          "title": "Learning and Generalization in RNNs"
        },
        {
          "paperId": "957424a1f3319a2aab1a91539a00e712477b4b4a",
          "title": "Self-Attention Networks Can Process Bounded Hierarchical Languages"
        },
        {
          "paperId": "fdc1815b83a59c0667ac75c8696febf5f1fd59e8",
          "title": "Lexical strata and phonotactic perplexity minimization"
        },
        {
          "paperId": "6a3aa7acaacd72fecb33e32d1c524883a45a18b7",
          "title": "Formal Language Theory Meets Modern NLP"
        },
        {
          "paperId": "39928e0f5fd2d65aa92ea8fd03108b5d1f25f6f6",
          "title": "Stronger Separation of Analog Neuron Hierarchy by Deterministic Context-Free Languages"
        },
        {
          "paperId": "88547c13100b78a4a38b52292b722365a127a93b",
          "title": "Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent"
        },
        {
          "paperId": "93a640dfdac5950bde902db9db31e3502e7fffe2",
          "title": "Distillation of Weighted Automata from Recurrent Neural Networks using a Spectral Approach"
        },
        {
          "paperId": "032399b7fc693a9fc12bb26d6be8c02d77dd397a",
          "title": "What they do when in doubt: a study of inductive biases in seq2seq learners"
        },
        {
          "paperId": "7e1c4d2f0b276bd30656f7577444c48210ab54ad",
          "title": "Investigating Backpropagation Alternatives when Learning to Dynamically Count with Recurrent Neural Networks"
        },
        {
          "paperId": "5bd9af165c7fe18aa6235df5032155befa522454",
          "title": "Deep Sentiment Analysis: A Case Study on Stemmed Turkish Twitter Data"
        },
        {
          "paperId": "9f13b84e05a0c6c717b06038ddbfd9af55b51806",
          "title": "Parameter Norm Growth During Training of Transformers"
        },
        {
          "paperId": "d06e84ac9e912b415719f0e7f3163d59e0a329cd",
          "title": "RNNs Can Generate Bounded Hierarchical Languages with Optimal Memory"
        },
        {
          "paperId": "52d87b59a1fa6561b90d5ea56f21180a9b1a505c",
          "title": "How Can Self-Attention Networks Recognize Dyck-n Languages?"
        },
        {
          "paperId": "10c86505de83647c7b4157595ab10f64e97c94ef",
          "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages"
        },
        {
          "paperId": "0def0a8dfccd1e70bf0f0f8b1a2217252fa3f952",
          "title": "On the Ability of Self-Attention Networks to Recognize Counter Languages"
        },
        {
          "paperId": "75e17a2dc0a2b2c1cce5ab89d8c703fb8bddbef1",
          "title": "On the Computational Power of Transformers and Its Implications in Sequence Modeling"
        },
        {
          "paperId": "ae807218c494fd4fb74690d3a48d18cdfbb44b1d",
          "title": "Provably Stable Interpretable Encodings of Context Free Grammars in RNNs with a Differentiable Stack"
        },
        {
          "paperId": "299546bc706ee776aea9d58cb21c729ee4499bd2",
          "title": "A provably stable neural network Turing Machine"
        },
        {
          "paperId": "0d317ae9d1cc285cfa90f44e0afbdb34b8b24d41",
          "title": "Analog neuron hierarchy"
        }
      ],
      "references": [
        {
          "paperId": "d15e715d99a52f1c8a1ae6ec0ca853b13e04bbc7",
          "title": "On the Linguistic Capacity of Real-Time Counter Automata"
        },
        {
          "paperId": "b3564be8b79f25585acb035f3deaf4ae93c26d8f",
          "title": "Theoretical Limitations of Self-Attention in Neural Sequence Models"
        },
        {
          "paperId": "a28ee1bea680c745636d7e09218fffda5d544ffe",
          "title": "Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages"
        },
        {
          "paperId": "f0471ad6daecabbd2521c6cede46007a02260771",
          "title": "RNN Architecture Learning with Sparse Regularization"
        },
        {
          "paperId": "da6ba9f19581bd7e28bc280c53385a1327eb09dd",
          "title": "LSTM Networks Can Perform Dynamic Counting"
        },
        {
          "paperId": "a1b35b15a548819cc133e3e0e4cf9b01af80e35d",
          "title": "Sequential Neural Networks as Automata"
        },
        {
          "paperId": "a56ebc39b8c527774be705cccdcb5f66c7302e0c",
          "title": "Rational Recurrences"
        },
        {
          "paperId": "6bf2f1dc081b319ed55f2e185278c7ebfacd9e45",
          "title": "Bridging CNNs, RNNs, and Weighted Finite-State Machines"
        },
        {
          "paperId": "06354570d5f6be803d4a79bf59ecbb097bca8755",
          "title": "On the Practical Computational Power of Finite Precision RNNs for Language Recognition"
        },
        {
          "paperId": "93b4cc549a1bc4bc112189da36c318193d05d806",
          "title": "AllenNLP: A Deep Semantic Natural Language Processing Platform"
        },
        {
          "paperId": "31b26b31f28988ebcfe7ff356e7fda7e17f1558c",
          "title": "Recurrent Neural Networks as Weighted Language Recognizers"
        },
        {
          "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
          "title": "Attention is All you Need"
        },
        {
          "paperId": "2d876ed1dd2c58058d7197b734a8e4d349b8f231",
          "title": "Quasi-Recurrent Neural Networks"
        },
        {
          "paperId": null,
          "title": "2017) into the state expressiveness hierarchy. We consider a single-head self attention encoder"
        },
        {
          "paperId": "1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba",
          "title": "Convolutional Neural Networks for Sentence Classification"
        },
        {
          "paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e",
          "title": "Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"
        },
        {
          "paperId": "c71325ea1e3391a688c70024c855b1b142854a2c",
          "title": "Spectral learning of weighted automata"
        },
        {
          "paperId": "cb19a063ea91e716a2c84d7bd3ce8935a645ef8d",
          "title": "Rational and Recognisable Power Series"
        },
        {
          "paperId": "31b1beb7a33feee7853fe195e5bc554d581154c6",
          "title": "Counter machines and counter languages"
        },
        {
          "paperId": "4aee84bd4185f776229f5af6b6e35da33e5eae6b",
          "title": "The Handbook of Brain Theory and Neural Networks"
        },
        {
          "paperId": "e72d20e5b6e21a2013d4b0d62001a194d86a210e",
          "title": "Context-Free Recognition with Weighted Automata"
        },
        {
          "paperId": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
          "title": "Long Short-Term Memory"
        },
        {
          "paperId": "a49498e51840165d55b6badd4b52e34d17860bc0",
          "title": "On the Computational Power of Neural Nets"
        },
        {
          "paperId": "93aaf126443643fb0835df896ab07b523f2c9613",
          "title": "Analog computation via neural networks"
        },
        {
          "paperId": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
          "title": "Finding Structure in Time"
        },
        { "paperId": null, "title": "Matrices de Hankel" },
        {
          "paperId": "c49838c33a6d190dea1981a04f54ee3443ac607b",
          "title": "Realizations by Stochastic Finite Automata"
        },
        {
          "paperId": "19c278e06339c5a3e7dd94403ea70b7e919579bb",
          "title": "Turing Machines with Restricted Memory Access"
        }
      ],
      "summary": "It is hypothesized that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy, and empirical results to support this conjecture are provided.",
      "title": "A Formal Hierarchy of RNN Architectures",
      "venue": "ACL",
      "year": 2020,
      "authors": "William Cooper Merrill,Gail Weiss,Yoav Goldberg,Roy Schwartz,Noah A. Smith,Eran Yahav"
    },
    {
      "id": "8a5590978930070f50c5f9fbf61f67e5d95794f0",
      "url": "https://www.semanticscholar.org/paper/8a5590978930070f50c5f9fbf61f67e5d95794f0",
      "citationCount": 12,
      "influentialCitationCount": 1,
      "referenceCount": 74,
      "citations": [
        {
          "paperId": "3c554b11d6c21799bb1687d5db916a3f963da640",
          "title": "DETR++: Taming Your Multi-Scale Detection Transformer"
        },
        {
          "paperId": "155d0b97563f0a85623c8efbd67b9feef722ac60",
          "title": "A Review of Emerging Research Directions in Abstract Visual Reasoning"
        },
        {
          "paperId": "59cb81d7763d8de23d9b4144c16287cb6808da1d",
          "title": "Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven's Progressive Matrices"
        },
        {
          "paperId": "03da1b6759f70c10e49b33a0ee914cb893d6f949",
          "title": "Learning Algebraic Representation for Systematic Generalization in Abstract Reasoning"
        },
        {
          "paperId": "40b065eb3aa5c5a54962aee78ebe30943beaabb1",
          "title": "Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images"
        },
        {
          "paperId": "721379fe5d859f13442102a5681cf58952b5af14",
          "title": "Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution"
        },
        {
          "paperId": "316c5d697f7caf92b419213e929a6063afaf253c",
          "title": "ACRE: Abstract Causal REasoning Beyond Covariation"
        },
        {
          "paperId": "4d1b18ee2d6093fc977df09d62b2a0e40b62a698",
          "title": "A HINT from Arithmetic: On Systematic Generalization of Perception, Syntax, and Semantics"
        },
        {
          "paperId": "eaa88d697f92739f3569564329e9d037aabbe2d7",
          "title": "A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics"
        },
        {
          "paperId": "edaafec59b651d503ac7c8a86f8e2335273e1f7a",
          "title": "Learning by Fixing: Solving Math Word Problems with Weak Supervision"
        },
        {
          "paperId": "e764dee4e50db01d77976e8f313fc092fc0eba85",
          "title": "GRICE: A Grammar-based Dataset for Recovering Implicature and Conversational rEasoning"
        },
        {
          "paperId": "862bca372c0c7475f9e5eb4951b9309f39cd4ace",
          "title": "Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense"
        }
      ],
      "references": [
        {
          "paperId": "70336ddb0082a43cf6180a321cb0c0683ac6d59e",
          "title": "Theory-based Causal Transfer: Integrating Instance-level Induction and Abstract-level Structure Learning"
        },
        {
          "paperId": "0187141e52f6d5cd0cf812ced648960e8f610abe",
          "title": "Learning Perceptual Inference by Contrasting"
        },
        {
          "paperId": "afed6dc6900d3b37e528b9086661bba583d60bf6",
          "title": "Analysing Mathematical Reasoning Abilities of Neural Models"
        },
        {
          "paperId": "d0bfd3cb732471a0843a39d2d047caf60a844466",
          "title": "RAVEN: A Dataset for Relational and Analogical Visual REasoNing"
        },
        {
          "paperId": "0d16b0fbe1e2bf6ef02aea2e058f2e13c3a83fa2",
          "title": "Learning to Make Analogies by Contrasting Abstract Relational Structure"
        },
        {
          "paperId": "06f2e16b9e19268d714b956af49649b2e8c79fc3",
          "title": "Decomposing Human Causal Learning: Bottom-up Associative Learning and Top-down Schema Reasoning"
        },
        {
          "paperId": "acc43abe319bca7652a91f7d4ca6187049fb82e4",
          "title": "Measuring abstract reasoning in neural networks"
        },
        {
          "paperId": "b36a5bb1707bb9c70025294b3a310138aae8327a",
          "title": "Automatic differentiation in PyTorch"
        },
        {
          "paperId": "678fd7c48efe21434148b4b3482c2b8b3ee618fc",
          "title": "Deep Neural Solver for Math Word Problems"
        },
        {
          "paperId": "007112213ece771be72cbecfd59f048209facabd",
          "title": "A simple neural network module for relational reasoning"
        },
        {
          "paperId": "b123a0d46ad917b79c43c5ae981e03ed2458ed11",
          "title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"
        },
        {
          "paperId": "40d71f60c70527ba6007a0458c2004fa45753a3c",
          "title": "Associations of non-symbolic and symbolic numerical magnitude processing with mathematical competence: a meta-analysis."
        },
        {
          "paperId": "ba84b8f16efc032945cd8ed960c121a9bcd064d5",
          "title": "Annotating Derivations: A New Evaluation Strategy and Dataset for Algebra Word Problems"
        },
        {
          "paperId": "49830f605c316fec8e9e365e7e64adeea47e89d8",
          "title": "Modeling Visual Problem Solving as Analogical Reasoning"
        },
        { "paperId": null, "title": "and Forbus" },
        {
          "paperId": "09900b36ca35af12301507cf992675808a709838",
          "title": "How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"
        },
        {
          "paperId": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
          "title": "Deep Residual Learning for Image Recognition"
        },
        { "paperId": null, "title": "and Chang" },
        {
          "paperId": "815c84ab906e43f3e6322f2ca3fd5e1360c64285",
          "title": "Human-level concept learning through probabilistic program induction"
        },
        {
          "paperId": "f9c990b1b5724e50e5632b94fdb7484ece8a6ce7",
          "title": "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting"
        },
        {
          "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
          "title": "Adam: A Method for Stochastic Optimization"
        },
        {
          "paperId": "3a395daf6c97c084cf9c3827384c53caf6502921",
          "title": "Learning to Automatically Solve Algebra Word Problems"
        },
        { "paperId": null, "title": "and Ba" },
        {
          "paperId": "946dffe06446c38a397fafb8d54e48c764196712",
          "title": "A Source Book Of Gestalt Psychology"
        },
        {
          "paperId": "7606136f5c67d8806cdc8750770db1a8c5073f10",
          "title": "A Structure-Mapping Model of Raven's Progressive Matrices"
        },
        {
          "paperId": "a4ffb87c11a7d1a52777a99817e18640272606a4",
          "title": "Flexible and adaptive use of strategies and representations in mathematics education"
        },
        {
          "paperId": "8b0401a2c013b07f8daea62cff00df1dc1c0ec70",
          "title": "Solving Geometric Analogy Problems Through Two-Stage Analogical Mapping"
        },
        {
          "paperId": "6b34f82bc2a1f5a802bb47e8e8e176fa7cfa595d",
          "title": "Representation of number in the brain."
        },
        {
          "paperId": "f75c694f31ae66fe9b5f4ae0345b3cc8a7cff1a6",
          "title": "Oxford English Dictionary."
        },
        {
          "paperId": "573ed869823a4f6c0730a8d24e6307cdfcf81517",
          "title": "The ABC of cardinal and ordinal number representations"
        },
        { "paperId": null, "title": "and Nieder" },
        {
          "paperId": "d880bd87dea6c66de8c3326d2b2416947f15e6b8",
          "title": "How much does number matter to a monkey (Macaca mulatta)?"
        },
        { "paperId": null, "title": "and Mumford" },
        { "paperId": null, "title": "A stochastic grammar of images" },
        {
          "paperId": "cf54074bebbb4f9d148d2e6068b7f5e22a87e2b5",
          "title": "AND CASE"
        },
        {
          "paperId": "09699eae4b5f9ba6eb022c7381cc84a2e00c0ee7",
          "title": "Using Measures of Number Sense to Screen for Difficulties in Mathematics: Preliminary Findings"
        },
        {
          "paperId": "0939cd35617482641d6e5e4f7619e68601e8048a",
          "title": "Numbers, language, and the human mind"
        },
        {
          "paperId": "09962320700f0ff1c71d353d406793531b6fc807",
          "title": "Visual Analogy in Problem Solving"
        },
        {
          "paperId": "37e4180aaf2e4762cbdc9dd04e714643d86c219f",
          "title": "Culture and systems of thought: holistic versus analytic cognition."
        },
        {
          "paperId": "161889c6dfaadfda26a3834342354b52198ab433",
          "title": "Psychological models for the development of mathematical understanding: rational numbers and functions"
        },
        {
          "paperId": null,
          "title": "Visual analogy in problem solving . In IJCAI . [ Davis and Pérusse 1988 ]"
        },
        {
          "paperId": "b82e21162f76da0e082282a298e7519f9b0bdd81",
          "title": "Constraint relaxation and chunk decomposition in insight problem solving"
        },
        {
          "paperId": "5385a494e90c6fc01195bf44d680c6e01cf687b5",
          "title": "A System for Relational Reasoning in Human Prefrontal Cortex"
        },
        {
          "paperId": "7847fa2a5a86ab30068d9d6b1757a96c386ae43c",
          "title": "Visual Analogy: Consciousness as the Art of Connecting"
        },
        {
          "paperId": "1050141af39d7c64303e60b608db1cec23db528d",
          "title": "Bayesian Modeling of Human Concept Learning"
        },
        {
          "paperId": "249b235ef1fdc4de22032b60e6f12ed10adba0de",
          "title": "Ordering of the numerosities 1 to 9 by monkeys."
        },
        {
          "paperId": "94518df3c15d4c88ec16cc4cc9f301895bca1af1",
          "title": "Abstract representations of numbers in the animal and human brain"
        },
        {
          "paperId": "7e879518ec31255bc75c7f5a1586df9f39b231e6",
          "title": "Brain mechanisms of quantity are similar in 5-year-old children and adults."
        },
        { "paperId": null, "title": "Ordering of the numerosities" },
        {
          "paperId": null,
          "title": "Oxford pyschologists Press. [Raven 1936] Raven, J. C. 1936. Mental tests used in genetic studies: The performance of related individuals on tests mainly educative and mainly reproductive"
        },
        {
          "paperId": "a3b7e80260891dcd3844b1835df8dee3a1cd67c7",
          "title": "The Number Sense: How the Mind Creates Mathematics."
        },
        {
          "paperId": "9fc2448d9e38dd6a25fd236f8a78818f2ea758a5",
          "title": "Exploring the microstructure of children's central conceptual structures in the domain of number."
        },
        {
          "paperId": null,
          "title": "Ii. exploring the microstructure of children's central conceptual structures in the domain of number. Monographs of the Society for research in"
        },
        {
          "paperId": null,
          "title": "Ii. exploring the microstructure of children's central conceptual structures in the domain of number. Monographs of the Society for research in"
        },
        {
          "paperId": null,
          "title": "Visualisation and the development of number sense with kindergarten children"
        },
        {
          "paperId": null,
          "title": "Visualisation and the development of number sense with kindergarten children. Children's number learning: A research monograph of MERGA/AAMT Adelaide: Australian Association of Mathematics teachers"
        },
        {
          "paperId": "6d9dc49325c8dd4edbf854c2a2eb174a52d9b126",
          "title": "Addition and subtraction by human infants"
        },
        {
          "paperId": "8c560202ceda574c65fcef53ba6a22c2e6640384",
          "title": "The Organization of Learning"
        },
        {
          "paperId": "eb45b073b52040d05aa9b462415cb881595b85fc",
          "title": "Concept formation knowledge and experience in unsupervised learning"
        },
        {
          "paperId": "a64dddf69180e744f62a22d9be07736ad4b2d7d9",
          "title": "Assessment of a problem-centered second-grade mathematics project."
        },
        {
          "paperId": "40e2df90808e9a38b342bff588b2583e66059b44",
          "title": "THE NICOMACHEAN ETHICS"
        },
        {
          "paperId": "8aa8e60994006f803fd25f86f039e5a5b1e2542c",
          "title": "Animal cognition: the representation of space, time and number."
        },
        { "paperId": null, "title": "and Weiner" },
        { "paperId": null, "title": "Oxford english dictionary . Dictionary" },
        {
          "paperId": "617b1261af3be2a9e72adc343e82c9edc9aedad3",
          "title": "Numerical competence in animals: Definitional issues, current evidence, and a new research agenda"
        },
        { "paperId": null, "title": "and Pérusse" },
        {
          "paperId": "5ef6f2547e8a004a2549a1cfee26906879fe0025",
          "title": "Oxford English dictionary"
        },
        {
          "paperId": "93151da5382d95b2b70e415d814b585871117988",
          "title": "Perception of numbers by human infants."
        },
        {
          "paperId": "4978e96a066ab4e9189f0d117ef16378953d92e5",
          "title": "Theory of fluid and crystallized intelligence: A critical experiment."
        },
        {
          "paperId": "318cddbeb64fc022a482dfcb316f732af99e6d47",
          "title": "Manual for Raven's progressive matrices and vocabulary scales"
        },
        {
          "paperId": "43a0503e38b4c47a5b794c04439299bd78310189",
          "title": "Laws of organization in perceptual forms."
        },
        {
          "paperId": null,
          "title": "Mental tests used in genetic studies: The performance of related individuals on tests mainly educative and mainly reproductive"
        },
        {
          "paperId": null,
          "title": "The Nicomachean ethics . AJ Valpy . [ Temple and Posner 1998 ]"
        },
        {
          "paperId": null,
          "title": "Automatic differentiation in pytorch . In ICLR . [ Raven and Court 1998 ]"
        }
      ],
      "summary": "A dataset, Machine Number Sense (MNS), consisting of visual arithmetic problems automatically generated using a grammar model—And-Or Graph (AOG), called for attention in fusing the classic search-based algorithms with modern neural networks to discover the essential number concepts in future research.",
      "title": "Machine Number Sense: A Dataset of Visual Arithmetic Problems for Abstract and Relational Reasoning",
      "venue": "AAAI",
      "year": 2020,
      "authors": "Wenhe Zhang,Chi Zhang,Yixin Zhu,Song-Chun Zhu"
    },
    {
      "id": "36d6c8895bbc755964b8b2136c6fd6087a7af089",
      "url": "https://www.semanticscholar.org/paper/36d6c8895bbc755964b8b2136c6fd6087a7af089",
      "citationCount": 48,
      "influentialCitationCount": 4,
      "referenceCount": 49,
      "citations": [
        {
          "paperId": "98f19ca97512361b12475b42b67a617de14d33a1",
          "title": "Mitigating Reporting Bias in Semi-supervised Temporal Commonsense Inference with Probabilistic Soft Logic"
        },
        {
          "paperId": "750448e5d852ec0a2e4f7f809f16a1470b2b479b",
          "title": "StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models"
        },
        {
          "paperId": "078f4efd448822b0e25d3ee0aec842ced606a595",
          "title": "Go Back in Time: Generating Flashbacks in Stories with Event Temporal Prompts"
        },
        {
          "paperId": "cf047a55569834900045f4c82e08fb1b97d95f61",
          "title": "A Survey of Machine Narrative Reading Comprehension Assessments"
        },
        {
          "paperId": "7fe912158bb91365286edb4c828452275a26d9ff",
          "title": "Temporal Relation Extraction with Joint Semantic and Syntactic Attention"
        },
        {
          "paperId": "e4169d63c99b7348ddd7e35293ce6ddeb22a9dcc",
          "title": "A survey of methods for revealing and overcoming weaknesses of data-driven Natural Language Understanding"
        },
        {
          "paperId": "3352644e4ee2f5bbccf34a74d1cc8b919397d823",
          "title": "What Makes Reading Comprehension Questions Difficult?"
        },
        {
          "paperId": "5aab7a0f40e11de07a8ad67a6bd29602c7df489b",
          "title": "Synopses of Movie Narratives: a Video-Language Dataset for Story Understanding"
        },
        {
          "paperId": "79ec50cd1320697819fd44c4ac6570c48a312349",
          "title": "Faking Fake News for Real Fake News Detection: Propaganda-loaded Training Data Generation"
        },
        {
          "paperId": "3469127ffee73eca466bd86e4bfdfa8e8c71107f",
          "title": "Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs"
        },
        {
          "paperId": "0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4",
          "title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension"
        },
        {
          "paperId": "ac8d33e4c0a45e227a47353f3f26fbb231482dc1",
          "title": "Time-Aware Language Models as Temporal Knowledge Bases"
        },
        {
          "paperId": "337ac06d80da63166ac91461e68edab6e4f7efa7",
          "title": "Understand before Answer: Improve Temporal Reading Comprehension via Precise Question Understanding"
        },
        {
          "paperId": "aa027ca4a3e71001283471cb88becd2c3b2a3ad5",
          "title": "DCT-Centered Temporal Relation Extraction"
        },
        {
          "paperId": "586cc1f02f41a7d702bd441e28fc94b9833e81d4",
          "title": "Improving Event Temporal Relation Classification via Auxiliary Label-Aware Contrastive Learning"
        },
        {
          "paperId": "18cce8de8378a754dea254fd35ca67e312f5290e",
          "title": "Extracting Temporal Event Relation with Syntax-guided Graph Transformer"
        },
        {
          "paperId": "867915b9587c046ce8e4b71ab4dee2a1d8bf0b48",
          "title": "DocTime: A Document-level Temporal Dependency Graph Parser"
        },
        {
          "paperId": "b672d2ec81c9395c312d57a27931864d07664592",
          "title": "A Meta-framework for Spatiotemporal Quantity Extraction from Text"
        },
        {
          "paperId": "50ec3d960ac458573a1e4a1556420c5e96d58609",
          "title": "Distantly-Supervised Evidence Retrieval Enables Question Answering without Evidence Annotation"
        },
        {
          "paperId": "174a0e6da0dfb7f96d4a0a4076eed154c439e41a",
          "title": "Probing Language Models for Understanding of Temporal Expressions"
        },
        {
          "paperId": "9f9667c06c94cc854eb55f16cfad2b9db7da49cd",
          "title": "Salience-Aware Event Chain Modeling for Narrative Understanding"
        },
        {
          "paperId": "8c4e9f559367fbbf731e97f66cad5559373863b0",
          "title": "Extracting Event Temporal Relations via Hyperbolic Geometry"
        },
        {
          "paperId": "278351ba38c46b61337e5a810747a37e54185bf9",
          "title": "A Dataset for Answering Time-Sensitive Questions"
        },
        {
          "paperId": "62953ca1252c9febe07c7007a10911726f37792d",
          "title": "TIMEDIAL: Temporal Commonsense Reasoning in Dialog"
        },
        {
          "paperId": "47fe46e561e3b270407b7bffa176e35291f165a7",
          "title": "Question Answering Over Temporal Knowledge Graphs"
        },
        {
          "paperId": "6597d61bdb531051678c773526758a6dc113b9ce",
          "title": "COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences"
        },
        {
          "paperId": "7d9a3b94f78827952b078c664b0da1c02e1c2ee3",
          "title": "What Ingredients Make for an Effective Crowdsourcing Protocol for Difficult NLU Data Collection Tasks?"
        },
        {
          "paperId": "80e015b4edbe72cb10af0bd2cb065bba163d6e0d",
          "title": "“Nice Try, Kiddo”: Investigating Ad Hominems in Dialogue Responses"
        },
        {
          "paperId": "965840bb0a8803b78c169f4b387e9e14cc80fffc",
          "title": "Extracting Temporal Event Relation with Syntactic-Guided Temporal Graph Transformer"
        },
        {
          "paperId": "c7f977f556d2060238fdc1286d057d46958afaf9",
          "title": "ESTER: A Machine Reading Comprehension Dataset for Event Semantic Relation Reasoning"
        },
        {
          "paperId": "de74100ac5a16f9169b8a7fe98dd05647b0e8777",
          "title": "SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning"
        },
        {
          "paperId": "5b0c4820eca8c5749860b932ab437ff110edc652",
          "title": "Grid Search Hyperparameter Benchmarking of BERT, ALBERT, and LongFormer on DuoRC"
        },
        {
          "paperId": "d6c919ee0d51432496513ef4b6b2dbd128819779",
          "title": "Microtask Detection"
        },
        {
          "paperId": "33b06c74eea3f400b6f5ef14ef163aef1db42d16",
          "title": "Conditional Generation of Temporally-ordered Event Sequences"
        },
        {
          "paperId": "6eee69031d2e11aa03a5a8fcb219cff4562863be",
          "title": "ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning"
        },
        {
          "paperId": "9d436d25981ea61db728bd490f0d54376d08953e",
          "title": "Temporal Reasoning on Implicit Events from Distant Supervision"
        },
        {
          "paperId": "778560c0da279b82b0da91ba43a2a023b9c6711f",
          "title": "Neural Language Modeling for Contextualized Temporal Graph Generation"
        },
        {
          "paperId": "30602e3382df3abedb5f225b55b7efce8580f74d",
          "title": "ForecastQA: A Question Answering Challenge for Event Forecasting with Temporal Text Data"
        },
        {
          "paperId": "e6d84cf9ae6efa10919bff765613e883a761db62",
          "title": "Open Temporal Relation Extraction for Question Answering"
        },
        {
          "paperId": "6bb369f874f49cd51415f216f1a3f635f2ca1eed",
          "title": "ESTER: A Machine Reading Comprehension Dataset for Reasoning about Event Semantic Relations"
        },
        {
          "paperId": "43003de1bdf12e14e917c98807ad0ab244caa923",
          "title": "Distantly-Supervised Dense Retrieval Enables Open-Domain Question Answering without Evidence Annotation"
        },
        {
          "paperId": "8424082e3bf4792462eb112d7ebcecf5b0dc3613",
          "title": "Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"
        },
        {
          "paperId": "ce9e010c5cb2323dfed3af687b12875f01c759bc",
          "title": "TIMERS: Document-level Temporal Relation Extraction"
        },
        {
          "paperId": "f214512bee16dbed2ca60fd867bb6ba3c6c27669",
          "title": "Benchmarking Machine Reading Comprehension: A Psychological Perspective"
        },
        {
          "paperId": "9793b07ba09d9f2ac9cabd8117daa93bf3db4346",
          "title": "DEER: A Data Efficient Language Model for Event Temporal Reasoning"
        },
        {
          "paperId": "3d2035edd4dd48e1e638279409e11bf689c461e1",
          "title": "Temporal Reasoning in Natural Language Inference"
        },
        {
          "paperId": "09be2777ff6974e27c5f04c0fdf1edeaabe56e30",
          "title": "\"Nice Try, Kiddo\": Ad Hominems in Dialogue Systems"
        },
        {
          "paperId": "a0c52a5cca698636b5a516b24f824d23f506f6e8",
          "title": "Easy, Reproducible and Quality-Controlled Data Collection with CROWDAQ"
        }
      ],
      "references": [
        {
          "paperId": "b9485d1e2c66c3ae452ec4903c2a157caef4d2ed",
          "title": "Temporal Common Sense Acquisition with Minimal Supervision"
        },
        {
          "paperId": "35e6783307f82d1faa39be0653431305abec7271",
          "title": "Evaluating Models’ Local Decision Boundaries via Contrast Sets"
        },
        {
          "paperId": "9fec5868542b4d9070306f1418d1d21666226e90",
          "title": "Evaluating NLP Models via Contrast Sets"
        },
        {
          "paperId": "483a47868d797ba998ba2eb107fa24d3f817220a",
          "title": "QuASE: Question-Answer Driven Sentence Encoding"
        },
        {
          "paperId": "c2c165dd615d4fc31d4fef4b4acbcab1a1655983",
          "title": "On Making Reading Comprehension More Comprehensive"
        },
        {
          "paperId": "81b4920ad488affaee27389ff9540b7fea90a4ce",
          "title": "“Going on a vacation” takes longer than “Going for a walk”: A Study of Temporal Commonsense Understanding"
        },
        {
          "paperId": "e03b5bc5edeb44d4b47d225c0c26ac54088fe528",
          "title": "An Improved Neural Baseline for Temporal Relation Extraction"
        },
        {
          "paperId": "0abcbdf40f872e6baf1c082811d4ae93df787698",
          "title": "Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets"
        },
        {
          "paperId": "50dbda1cb9196f88914679fd34f4e44b2955434d",
          "title": "Reasoning Over Paragraph Effects in Situations"
        },
        {
          "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
          "paperId": "dda6fb309f62e2557a071522354d8c2c897a2805",
          "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"
        },
        {
          "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
          "paperId": "3838387ea8dd1bb8c2306be5a63c1c120075c5a2",
          "title": "Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning"
        },
        { "paperId": null, "title": "Question answering is a format" },
        {
          "paperId": "bbad0b301561c9b44a43f2880b29f143dc7297ba",
          "title": "Temporal Information Extraction by Predicting Relative Time-lines"
        },
        {
          "paperId": "34901b737b11aa51b688fa18c2eab47639d7b8c6",
          "title": "Joint Reasoning for Temporal and Causal Relations"
        },
        {
          "paperId": "bbb3a49edf69a1909c0cf453858b451ef23fcbaf",
          "title": "Context-Aware Neural Model for Temporal Information Extraction"
        },
        {
          "paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c",
          "title": "Know What You Don’t Know: Unanswerable Questions for SQuAD"
        },
        {
          "paperId": "82580fe4c429ac76f94c514c1ffc066844b13192",
          "title": "Determining Event Durations: Models and Error Analysis"
        },
        {
          "paperId": "907659d4744e2796d81fe2ce65d7235d79826f66",
          "title": "SemEval 2018 Task 6: Parsing Time Normalizations"
        },
        {
          "paperId": "bff8ae9e28323d217b9ad5a7321e58f79607f557",
          "title": "A Multi-Axis Annotation Scheme for Event Temporal Relations"
        },
        {
          "paperId": "a9e0fa94e59261f5905951e93d515af64c3fc7cb",
          "title": "A Structured Learning Approach to Temporal Relation Extraction"
        },
        {
          "paperId": "b05e8fac145817a276f922ac8d65644625448838",
          "title": "SemEval-2017 Task 12: Clinical TempEval"
        },
        {
          "paperId": "b67a64da079ce493c8d24e6212a6534490464bb9",
          "title": "Neural Architecture for Temporal Relation Extraction: A Bi-LSTM Approach for Detecting Narrative Containers"
        },
        {
          "paperId": "2a8e11f6b8d90de4a06a7bf29065184e2627acff",
          "title": "Classifying Temporal Relations by Bidirectional LSTM over Dependency Paths"
        },
        {
          "paperId": "fa025e5d117929361bcf798437957762eb5bb6d4",
          "title": "Zero-Shot Relation Extraction via Reading Comprehension"
        },
        {
          "paperId": "7f1b0539bec01e52c4ffa5543c5ed55880dead55",
          "title": "Neural Temporal Relation Extraction"
        },
        {
          "paperId": "1a210410493fbc052f0b7a54e7bc89cee20e8d28",
          "title": "Crowdsourcing Question-Answer Meaning Representations"
        },
        {
          "paperId": "1b4e435c0b62a09265e7c334508e8c16139302d0",
          "title": "Representations of Time Expressions for Temporal Relation Extraction with Convolutional Neural Networks"
        },
        {
          "paperId": "7dce3c780c85c41ff9dfe7511962ecddb2291ab8",
          "title": "Structured Learning for Temporal Relation Extraction from Clinical Records"
        },
        {
          "paperId": "05dd7254b632376973f3a1b4d39485da17814df5",
          "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
        },
        {
          "paperId": "a2fc4a8882a03f7cb4ccc3711592005c1e4af2b9",
          "title": "SemEval-2016 Task 12: Clinical TempEval"
        },
        {
          "paperId": "04041a04ede58a464c843f95608e813a7686f6eb",
          "title": "CaTeRS: Causal and Temporal Relation Scheme for Semantic Annotation of Event Structures"
        },
        {
          "paperId": "40322efad1be8b03541348fd56cfa1d37e279075",
          "title": "Richer Event Description: Integrating event coreference with temporal, causal and bridging annotation"
        },
        {
          "paperId": "7daf69424feafdce1c896ff19f9a08a5b31ad5d8",
          "title": "Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language"
        },
        {
          "paperId": "9c34bf7bdb5a4247772c5c6d4982aa4d41e67be9",
          "title": "Event Nugget Annotation: Processes and Issues"
        },
        {
          "paperId": "ece7f0706a75a0558f57178e674fb1e29abda031",
          "title": "SemEval-2015 Task 5: QA TempEval - Evaluating Temporal Information Understanding with Question Answering"
        },
        { "paperId": null, "title": "TimeLine: Cross-document event ordering" },
        {
          "paperId": "df4dcc3224e74f34e7c6bf7717ad76ec3b71a9b9",
          "title": "Dense Event Ordering with a Multi-Pass Architecture"
        },
        {
          "paperId": "341da28f0a153c260239cffc2a35caeb3f74725f",
          "title": "Income and Poverty in the United States: 2013"
        },
        {
          "paperId": "1f78e47c67c0ed3e5e3293337fa8c240dafa8b52",
          "title": "An Annotation Framework for Dense Event Ordering"
        },
        {
          "paperId": "226f51c22f2852cbb456588d083acf1b52071951",
          "title": "Temporal Annotation in the Clinical Domain"
        },
        {
          "paperId": "0d9d8be5ee0c1cda47beafea0ef0b14722cbd908",
          "title": "SemEval-2013 Task 1: TempEval-3: Evaluating Time Expressions, Events, and Temporal Relations"
        },
        {
          "paperId": "39c230241d51b1435472115aaa8c62b94ab9927d",
          "title": "Joint Inference for Event Timeline Construction"
        },
        {
          "paperId": "807aac46a06742026a365c978a8b5ff180f3d5cb",
          "title": "SemEval-2010 Task 13: TempEval-2"
        },
        {
          "paperId": "c23dd3ef19c54b458cf104f7f7fa0920c25f89c2",
          "title": "Timelines from Text: Identification of Syntactic Temporal Relations"
        },
        {
          "paperId": "ce428501603cc8f904038a08bc1897cdc589f098",
          "title": "SemEval-2007 Task 15: TempEval Temporal Relation Identification"
        },
        { "paperId": null, "title": "The TIMEBANK corpus" },
        {
          "paperId": "d36afe59ad1b706e020f55b54740bc9cddf25dcd",
          "title": "Towards a General Theory of Action and Time"
        }
      ],
      "summary": "TORQUE is introduced, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships, and results show that RoBERTa-large achieves an exact-match score of 51% on the test set of TORQUE, about 30% behind human performance.",
      "title": "TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions",
      "venue": "EMNLP",
      "year": 2020,
      "authors": "Qiang Ning,Hao Wu,Rujun Han,Nanyun Peng,Matt Gardner,D. Roth"
    },
    {
      "id": "016760dc4a05489ddf5dbb48aecbb49e214e1b71",
      "url": "https://www.semanticscholar.org/paper/016760dc4a05489ddf5dbb48aecbb49e214e1b71",
      "citationCount": 67,
      "influentialCitationCount": 7,
      "referenceCount": 31,
      "citations": [
        {
          "paperId": "e02dce6ee032a13b1653f69b034a35676e7d4dc2",
          "title": "Can Language Representation Models Think in Bets?"
        },
        {
          "paperId": "4f8ae9cdbae875f477aec5ae648f2ad8efc754c6",
          "title": "\"John is 50 years old, can his son be 65?\"Evaluating NLP Models' Understanding of Feasibility"
        },
        {
          "paperId": "3b622664d44a57280d3a189fa6475e56b96f1add",
          "title": "CIKQA: Learning Commonsense Inference with a Unified Knowledge-in-the-loop QA Paradigm"
        },
        {
          "paperId": "b47a3f7bcf540adb6fd97869c51449888d3160bb",
          "title": "Can Language Models Be Specific? How?"
        },
        {
          "paperId": "2591c66c6006c9c275a3dc7108a487934bc1c06f",
          "title": "Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering"
        },
        {
          "paperId": "15bacb240e2598457af4ded3039b6988aa9706f0",
          "title": "Few-shot Adaptation Works with UnpredicTable Data"
        },
        {
          "paperId": "150211bdd0b03658e64089e9581b82a0597e1e8e",
          "title": "KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Few-Shot NLP"
        },
        {
          "paperId": "0b7e9b6b588baa3f24fdde06feec26a067aa74bd",
          "title": "MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data"
        },
        {
          "paperId": "011095a0082e5e301f9bf30267b193c1c9e7e370",
          "title": "Perturbation Augmentation for Fairer NLP"
        },
        {
          "paperId": "a399e64a2698fb6e6d43b1a9576469afd4a29db3",
          "title": "GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models"
        },
        {
          "paperId": "d48b29889241551e1ee6622fa78c3fa4159255dd",
          "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning"
        },
        {
          "paperId": "39f0f28848990f74eeb9019f579c6ebcc8ef3ea1",
          "title": "TransTab: Learning Transferable Tabular Transformers Across Tables"
        },
        {
          "paperId": "4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80",
          "title": "Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence"
        },
        {
          "paperId": "251c3afbaafcc9b5178534be9109f644bfc5e912",
          "title": "Enhancing Knowledge Bases with Quantity Facts"
        },
        {
          "paperId": "cf2453589ee98561c599694fce3220aa79e5c56b",
          "title": "Probing Script Knowledge from Pre-Trained Models"
        },
        {
          "paperId": "591c54203cb06602d302230de1d6a60d753a4d8e",
          "title": "What If: Generating Code to Answer Simulation Questions"
        },
        {
          "paperId": "682ee9c06f3eff3e3708b4d0419dc85ecf9c6c87",
          "title": "What do Toothbrushes do in the Kitchen? How Transformers Think our World is Structured"
        },
        {
          "paperId": "706c6b3781374b0b11f98f204a4ddd05b26ed009",
          "title": "Knowledge Infused Decoding"
        },
        {
          "paperId": "237f5ca6fcccef2b77a2212b34fb06a1dbd09b72",
          "title": "Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting"
        },
        {
          "paperId": "50f7d69ddd7f9b34f5121607fbdcc57236d65b8c",
          "title": "Can Pre-trained Language Models Interpret Similes as Smart as Human?"
        },
        {
          "paperId": "8c62277dada489904a63de4dd87336c27c68fb5e",
          "title": "Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models"
        },
        {
          "paperId": "7bebb48d34c219b119ca2d4ffc97d7fd4940c35c",
          "title": "On the data requirements of probing"
        },
        {
          "paperId": "833a2f1817cb9aeb292620454889cae78e26dda4",
          "title": "Impact of Pretraining Term Frequencies on Few-Shot Reasoning"
        },
        {
          "paperId": "4497f3ed54ac520b50ffa05df04b37a59d4c1265",
          "title": "Identifying Weaknesses in Machine Translation Metrics Through Minimum Bayes Risk Decoding: A Case Study for COMET"
        },
        {
          "paperId": "7e5ca499cd9b932921bda84db98f75087d0b0683",
          "title": "Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey"
        },
        {
          "paperId": "1ea78b1684371de0e859edf759e5e659152d31e3",
          "title": "What do Large Language Models Learn about Scripts?"
        },
        {
          "paperId": "24f09c1eb5d7b662bd53892f846600c5e0b66a6c",
          "title": "Masked Measurement Prediction: Learning to Jointly Predict Quantities and Units from Textual Context"
        },
        {
          "paperId": "47df3fd32d00220c85c2c51a571254fd99b2ecc7",
          "title": "MetaICL: Learning to Learn In Context"
        },
        {
          "paperId": "12a763cb52f650710900790ca0bc43e5d5b88be6",
          "title": "Generated Knowledge Prompting for Commonsense Reasoning"
        },
        {
          "paperId": "0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4",
          "title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension"
        },
        {
          "paperId": "06396c7cd5d223a1776abf8811359ec7bc05d420",
          "title": "Knowledge-Augmented Methods for Natural Language Processing"
        },
        {
          "paperId": "31662a7beb1acbdda51ecb6d8ec419fb9887a817",
          "title": "Topicalization in Language Models: A Case Study on Japanese"
        },
        {
          "paperId": "2a83a92b08e0f3873d07162c73c67e533321112e",
          "title": "Aligning Generative Language Models with Human Values"
        },
        {
          "paperId": "da20a595aee606dec221bd1c5757274b17f39c66",
          "title": "Probing Measuring Skills in Pre-trained Language Models"
        },
        {
          "paperId": "e02a757617c2c42eb62889cc4d4aee3765928303",
          "title": "The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus"
        },
        {
          "paperId": "1360dc6d21ccbf2c23701c9ddd7c6ab8d04e4531",
          "title": "The World of an Octopus: How Reporting Bias Influences a Language Model’s Perception of Color"
        },
        {
          "paperId": "3ac5281ffa29ed078cc066d79a0edfdbd7056728",
          "title": "Exploring Universal Intrinsic Task Subspace via Prompt Tuning"
        },
        {
          "paperId": "02d4e18bc3677be373ff3f31c4a5702cb4e1f520",
          "title": "Pre-trained Language Models in Biomedical Domain: A Survey fromMultiscale Perspective"
        },
        {
          "paperId": "0cea4db3dea404ed55b3a179b496357dc5c8ff80",
          "title": "Pre-trained Language Models in Biomedical Domain: A Systematic Survey"
        },
        {
          "paperId": "18ae4f4ccce2602fc09d5cf1ddcdfd9eaf416958",
          "title": "Automatic Generation of Word Problems for Academic Education via Natural Language Processing (NLP)"
        },
        {
          "paperId": "37588705a2af7d5b24d901dd33ade1ff293aabdd",
          "title": "Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"
        },
        {
          "paperId": "0c4b2ae22f08425563a69527a3433516fc9737a1",
          "title": "CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge"
        },
        {
          "paperId": "f640ef82eef63f16e9e732409e72aec638a53a3d",
          "title": "Accurate, yet inconsistent? Consistency Analysis on Language Understanding Models"
        },
        {
          "paperId": "e248b22b7107ab057a0ea42a4dd075a5a4b2df26",
          "title": "How to Query Language Models?"
        },
        {
          "paperId": "40a1f266bb5ca853837355bfff272a55f0049c81",
          "title": "Mining Numbers in Text: A Survey"
        },
        {
          "paperId": "e337ed6543c2e6e7e51c312c7d998798fc79fdde",
          "title": "Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases"
        },
        {
          "paperId": "c6fd846b9b8f9eb0a492d6d6242fffce987c4580",
          "title": "Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning"
        },
        {
          "paperId": "62953ca1252c9febe07c7007a10911726f37792d",
          "title": "TIMEDIAL: Temporal Commonsense Reasoning in Dialog"
        },
        {
          "paperId": "5aab57cc0530560d82c74c055f664280619d7e81",
          "title": "PROST: Physical Reasoning about Objects through Space and Time"
        },
        {
          "paperId": "6597d61bdb531051678c773526758a6dc113b9ce",
          "title": "COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences"
        },
        {
          "paperId": "c9343c26a0e604f7afd94b7290bbdf8d96cd65b6",
          "title": "Predicting Numerals in Natural Language Text Using a Language Model Considering the Quantitative Aspects of Numerals"
        },
        {
          "paperId": "f80b837a211f71c6647c5755d4569559f0c2c0f7",
          "title": "Language Model as an Annotator: Exploring DialoGPT for Dialogue Summarization"
        },
        {
          "paperId": "ccbc69b6e187b9a1614f76c5ede30b139f3dc9fb",
          "title": "Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense Reasoning Tasks"
        },
        {
          "paperId": "7fa273f450251523e6b7fcc2eb3fdbdfd4a30493",
          "title": "CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP"
        },
        {
          "paperId": "d331de3b6bebb0f9af1fddf1b730ec057a7026d4",
          "title": "Relational World Knowledge Representation in Contextual Language Models: A Review"
        },
        {
          "paperId": "28a5a53dafacebad8a7c47773079caeffb9a5baa",
          "title": "Representing Numbers in NLP: a Survey and a Vision"
        },
        {
          "paperId": "2cc3ab9fa41ba2804e301f7eae9598636e62422a",
          "title": "Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"
        },
        {
          "paperId": "9f620ad41b4e506e777c0665681b839c89cd682a",
          "title": "Dimensions of Commonsense Knowledge"
        },
        {
          "paperId": "71fab1ce3c66998ba681ab378484be77690327a9",
          "title": "RiddleSense: Reasoning about Riddle Questions Featuring Linguistic Creativity and Commonsense Knowledge"
        },
        {
          "paperId": "011869f932f89d047ce2bd36d73a95cc04888193",
          "title": "RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms"
        },
        {
          "paperId": "c2d50f17ea6769f6f5663ccac37a9627a0543184",
          "title": "Mathematical Reasoning in General Artificial Intelligence Workshop , ICLR 2021"
        },
        {
          "paperId": "8424082e3bf4792462eb112d7ebcecf5b0dc3613",
          "title": "Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"
        },
        {
          "paperId": "884c0b6db564208d99cadf2548f0aa96dee5f859",
          "title": "Commonsense Reasoning with Implicit Knowledge in Natural Language"
        },
        {
          "paperId": "080df61ee1c15ff3c8e5d0d82d60bfd80e372e38",
          "title": "Probing Toxic Content in Large Pre-Trained Language Models"
        },
        {
          "paperId": "2b8c6c2ae778184f41db3467ed2cde5342aae676",
          "title": "RiddleSense: Answering Riddle Questions as Commonsense Reasoning"
        },
        {
          "paperId": "3118c0633cb2d0f91b5ef88840343e47c3ca5623",
          "title": "Do Language Embeddings capture Scales?"
        },
        {
          "paperId": "94271aa76b145af14be87f2c8ee8b9075f17a349",
          "title": "Do Language Embeddings capture Scales?"
        }
      ],
      "references": [
        {
          "paperId": "35ca61aee1d58ccb641eec3804ffd96fdb968cb7",
          "title": "GenericsKB: A Knowledge Base of Generic Statements"
        },
        {
          "paperId": "3dd61d97827e3f380bf9304101149a3f865051fc",
          "title": "Injecting Numerical Reasoning Skills into Language Models"
        },
        {
          "paperId": "5e0cffc51e8b64a8f11326f955fa4b4f1803e3be",
          "title": "oLMpics-On What Language Model Pre-training Captures"
        },
        {
          "paperId": "f67fcbb1aec92ae293998ddfd904f61a31bef334",
          "title": "Inducing Relational Knowledge from BERT"
        },
        {
          "paperId": "521b4e26df0f1cf5763dece14cbb218df152dc59",
          "title": "Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering"
        },
        {
          "paperId": null,
          "title": "Evaluating commonsense in pretrained language models"
        },
        {
          "paperId": "727c9d3846ebd80a9138d0e6c9e995d9afc1d312",
          "title": "CommonGen: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning"
        },
        {
          "paperId": "41094beddc498680d807e99e07efa41fec5d6724",
          "title": "How Pre-trained Word Representations Capture Commonsense Physical Comparisons"
        },
        {
          "paperId": "136396e988dbbdaca433096e931b22732c3d95dd",
          "title": "Text2Math: End-to-end Parsing Text into Math Expressions"
        },
        {
          "paperId": "bd2895717effe3cc52e77fbee3da8117cf1c01e1",
          "title": "Measuring Numerical Common Sense: Is A Word Embedding Approach Effective?"
        },
        {
          "paperId": "0427110f0e79f41e69a8eb00a3ec8868bac26a4f",
          "title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings"
        },
        {
          "paperId": "710d183174844da5b7f392667f3cc25d2b098dde",
          "title": "KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning"
        },
        {
          "paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3",
          "title": "Language Models as Knowledge Bases?"
        },
        {
          "paperId": "f98e135986414cccf29aec593d547c0656e4d82c",
          "title": "Commonsense Knowledge Mining from Pretrained Models"
        },
        {
          "paperId": "17dbd7b72029181327732e4d11b52a08ed4630d0",
          "title": "Natural Questions: A Benchmark for Question Answering Research"
        },
        {
          "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
          "paperId": "95a251513853c6032bdecebd4b74e15795662986",
          "title": "What Does BERT Look at? An Analysis of BERT’s Attention"
        },
        {
          "paperId": "d34e6c84119cef8eb9149a27d6b4903131407ea6",
          "title": "How Large Are Lions? Inducing Distributions over Quantitative Attributes"
        },
        {
          "paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c",
          "title": "BERT Rediscovers the Classical NLP Pipeline"
        },
        {
          "paperId": "28b74bb7c8b08cceb2430ec2d54dfa0f3225d796",
          "title": "VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research"
        },
        {
          "paperId": "dda6fb309f62e2557a071522354d8c2c897a2805",
          "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"
        },
        {
          "paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
          "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"
        },
        {
          "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
          "paperId": "8cb592fa5e30e6fa5abe7041767768964f1f8cf4",
          "title": "Do Language Models Have Common Sense"
        },
        {
          "paperId": "d7b6753a2d4a2b286c396854063bde3a91b75535",
          "title": "A Simple Method for Commonsense Reasoning"
        },
        {
          "paperId": "c9f343b492c170c726f607c255ec6c7177dc5800",
          "title": "Verb Physics: Relative Physical Knowledge of Actions and Objects"
        },
        {
          "paperId": "26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810",
          "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"
        },
        {
          "paperId": "2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9",
          "title": "MAWPS: A Math Word Problem Repository"
        },
        {
          "paperId": "71b7178df5d2b112d07e45038cb5637208659ff7",
          "title": "Microsoft COCO: Common Objects in Context"
        },
        {
          "paperId": "45a23651bcc5a6cc993d722e71b0d301a6dc9dee",
          "title": "Open Mind Common Sense: Knowledge Acquisition from the General Public"
        },
        {
          "paperId": "566eb7be43b8a2b2daff82b03711098a84859b2a",
          "title": "of the Association for Computational Linguistics:"
        }
      ],
      "summary": "Investigating whether and to what extent one can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process finds that this may not work for numerical Commonsense knowledge.",
      "title": "Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models",
      "venue": "EMNLP",
      "year": 2020,
      "authors": "Bill Yuchen Lin,Seyeon Lee,Rahul Khanna,Xiang Ren"
    },
    {
      "id": "72e6b14c081ad3e6a1c295b60e5c834837e6b304",
      "url": "https://www.semanticscholar.org/paper/72e6b14c081ad3e6a1c295b60e5c834837e6b304",
      "citationCount": 10,
      "influentialCitationCount": 1,
      "referenceCount": 45,
      "citations": [
        {
          "paperId": "fb30166c218bef3597b0d9789ad340defc3989ca",
          "title": "In-BoXBART: Get Instructions into Biomedical Multi-Task Learning"
        },
        {
          "paperId": "0e6e8274d0dcbc1c3c1ccdbd87f3e5d53fdf62b4",
          "title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension"
        },
        {
          "paperId": "cbdb45fc16b0885905b91d84281c310e6cb49e9c",
          "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions"
        },
        {
          "paperId": "37588705a2af7d5b24d901dd33ade1ff293aabdd",
          "title": "Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"
        },
        {
          "paperId": "fb1af83fcf237692768a30a4f9eb0ab61c500c14",
          "title": "Interviewer-Candidate Role Play: Towards Developing Real-World NLP Systems"
        },
        {
          "paperId": "dd75f713420cd8eb3e0b3ce870b2680cd93b39fd",
          "title": "Sequence to General Tree: Knowledge-Guided Geometry Word Problem Solving"
        },
        {
          "paperId": "28a5a53dafacebad8a7c47773079caeffb9a5baa",
          "title": "Representing Numbers in NLP: a Survey and a Vision"
        },
        {
          "paperId": "2cc3ab9fa41ba2804e301f7eae9598636e62422a",
          "title": "Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"
        },
        {
          "paperId": "c2d50f17ea6769f6f5663ccac37a9627a0543184",
          "title": "Mathematical Reasoning in General Artificial Intelligence Workshop , ICLR 2021"
        },
        {
          "paperId": "884c0b6db564208d99cadf2548f0aa96dee5f859",
          "title": "Commonsense Reasoning with Implicit Knowledge in Natural Language"
        }
      ],
      "references": [
        {
          "paperId": "ad5970584754cc7a1d91c95ab84a1e210258183a",
          "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System"
        },
        {
          "paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45",
          "title": "PIQA: Reasoning about Physical Commonsense in Natural Language"
        },
        {
          "paperId": "3cfb319689f06bf04c2e28399361f414ca32c4b3",
          "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
          "paperId": "6bc831932127ed850b539a1789a9b26f05fccf7d",
          "title": "A Simple and Effective Model for Answering Multi-span Questions"
        },
        {
          "paperId": "243e2140a14022531461c5f169ebbe14930efccc",
          "title": "Two Cheers for Rebooting AI: Building Artificial Intelligence We Can Trust"
        },
        {
          "paperId": "03dfcdeb5e3f0ce944304340f54eb82603159265",
          "title": "ORB: An Open Reading Benchmark for Comprehensive Evaluation of Machine Reading Comprehension"
        },
        {
          "paperId": "730043364aed106241ef18ab3e3b5e316802a254",
          "title": "NumNet: Machine Reading Comprehension with Numerical Reasoning"
        },
        {
          "paperId": "efa23afbed4c95bb416b72e1bd477f6c27471baf",
          "title": "Exploring ways to incorporate additional knowledge to improve Natural Language Commonsense Question Answering"
        },
        {
          "paperId": "0427110f0e79f41e69a8eb00a3ec8868bac26a4f",
          "title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings"
        },
        {
          "paperId": "52fa450740913a6cdcb4d9395b45e203f46cab79",
          "title": "Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"
        },
        {
          "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
          "paperId": "a4804394c362e7129312ae20fc094e60f56db3b0",
          "title": "Careful Selection of Knowledge to Solve Open Book Question Answering"
        },
        {
          "paperId": "28ad018c39d1578bea84e7cedf94459e3dbe1e70",
          "title": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"
        },
        {
          "paperId": "eef7cfe8267954adbb4675576072a1d80ca7a3a8",
          "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"
        },
        {
          "paperId": "d9f6ada77448664b71128bb19df15765336974a6",
          "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"
        },
        {
          "paperId": "afed6dc6900d3b37e528b9086661bba583d60bf6",
          "title": "Analysing Mathematical Reasoning Abilities of Neural Models"
        },
        {
          "paperId": "dda6fb309f62e2557a071522354d8c2c897a2805",
          "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"
        },
        {
          "paperId": "1eff94b44432a8e6af29288b8234494516579ad3",
          "title": "EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"
        },
        {
          "paperId": "51004bc6461a572e1189a0e3b32b441155d760ce",
          "title": "QuaRel: A Dataset and Models for Answering Questions about Qualitative Relationships"
        },
        {
          "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
          "paperId": "4a98b6353d8492ed6ebe71c3a7df5f866f457173",
          "title": "What’s Missing: A Knowledge Gap Guided Approach for Multi-hop Question Answering"
        },
        {
          "paperId": null,
          "title": "Tag-based multi-span extraction in reading comprehension"
        },
        {
          "paperId": "9784fbf77295860b2e412137b86356d70b25e3c0",
          "title": "The Natural Language Decathlon: Multitask Learning as Question Answering"
        },
        {
          "paperId": "8c6427cc1f4e1bbe5d6da34a4511842361f4fbb6",
          "title": "Hypothesis Only Baselines in Natural Language Inference"
        },
        {
          "paperId": "93b8da28d006415866bf48f9a6e06b5242129195",
          "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
        },
        {
          "paperId": "88bb0a28bb58d847183ec505dda89b63771bb495",
          "title": "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
        },
        {
          "paperId": "83cc0d20275fdc3a97cceacdc41fbb19953fa901",
          "title": "Mapping to Declarative Knowledge for Word Problem Solving"
        },
        {
          "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
          "title": "Improving Language Understanding by Generative Pre-Training"
        },
        {
          "paperId": "b123a0d46ad917b79c43c5ae981e03ed2458ed11",
          "title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"
        },
        {
          "paperId": "8b7336f5dd13a45d4aab38428b4a88ce507ea310",
          "title": "Unit Dependency Graph and Its Application to Arithmetic Word Problem Solving"
        },
        {
          "paperId": null,
          "title": "spacy 2: Natural language understanding with bloom embeddings"
        },
        {
          "paperId": "7b6f61e6067d3fe520e01b19e2941aa122fcd564",
          "title": "Learning from Explicit and Implicit Supervision Jointly For Algebra Word Problems"
        },
        {
          "paperId": "2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9",
          "title": "MAWPS: A Math Word Problem Repository"
        },
        {
          "paperId": "390808617b277987999b6e8e0aa5742aab54a6a0",
          "title": "My Computer Is an Honor Student - but How Intelligent Is It? Standardized Tests as a Measure of AI"
        },
        {
          "paperId": "abb33d75dc297993fcc3fb75e0f4498f413eb4f6",
          "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks"
        },
        {
          "paperId": "4c6fe6179c408e1fbb3871af13d1a8e64f766e54",
          "title": "Solving General Arithmetic Word Problems"
        },
        {
          "paperId": "17230f5b3956188055a48c5f4f61d131cce0662f",
          "title": "Parsing Algebraic Word Problems into Equations"
        },
        {
          "paperId": "818b92bfad6e11d849bae552be60111579d91e91",
          "title": "Elementary School Science and Math Tests as a Driver for AI: Take the Aristo Challenge!"
        },
        {
          "paperId": "a7862e14b4c20cefd6dc4f611f8aa866fabf130b",
          "title": "Learning to Solve Arithmetic Word Problems with Verb Categorization"
        },
        {
          "paperId": "3a395daf6c97c084cf9c3827384c53caf6502921",
          "title": "Learning to Automatically Solve Algebra Word Problems"
        },
        {
          "paperId": "128cb6b891aee1b5df099acb48e2efecfcff689f",
          "title": "The Winograd Schema Challenge"
        },
        {
          "paperId": "75f9f2743951edd71369372d9cd6416cc822f063",
          "title": "Number as a cognitive technology: Evidence from Pirahã language and cognition"
        },
        {
          "paperId": null,
          "title": "Honnibal and Montani, 2017] Matthew Honnibal and Ines Montani. spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing"
        },
        {
          "paperId": "566eb7be43b8a2b2daff82b03711098a84859b2a",
          "title": "of the Association for Computational Linguistics:"
        },
        {
          "paperId": "a3b7e80260891dcd3844b1835df8dee3a1cd67c7",
          "title": "The Number Sense: How the Mind Creates Mathematics."
        }
      ],
      "summary": "This work introduces NUMBERGAME, a multifaceted benchmark to evaluate model performance across numerical reasoning tasks of eight diverse formats, and takes forward the recent progress in generic system development, demonstrating the scope of under-explored tasks.",
      "title": "Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks",
      "venue": "ArXiv",
      "year": 2020,
      "authors": "Swaroop Mishra,Arindam Mitra,Neeraj Varshney,Bhavdeep Singh Sachdeva,Chitta Baral"
    },
    {
      "id": "c3d487ca75f26adadd94ba7af4b57aed8d661ff3",
      "url": "https://www.semanticscholar.org/paper/c3d487ca75f26adadd94ba7af4b57aed8d661ff3",
      "citationCount": 1,
      "influentialCitationCount": 0,
      "referenceCount": 9,
      "citations": [
        {
          "paperId": "edd712436dbecd05161fdbe1caf96fd9f1739564",
          "title": "Fvsoomm a Fuzzy Vectorial Space Model and Method of Personality, Cognitive Dissonance and Emotion in Decision Making"
        }
      ],
      "references": [
        {
          "paperId": "5c0f93b5e4f64d833fa5aab15f8642b78f2dfb9f",
          "title": "DETECTING HETEROGENEITY IN A MULTIDATABASE ENVIRONMENT THROUGH AN O-O MODEL"
        },
        {
          "paperId": null,
          "title": "Automatic knowledge acquisition for object oriented expert systems AVIGNON'93"
        },
        {
          "paperId": null,
          "title": "Automatic knowledge acquisition for object oriented expert systems AVIGNON'93"
        },
        {
          "paperId": null,
          "title": "Mécanismes de classification pour la recherche d'informations dans une base d'objets"
        },
        {
          "paperId": null,
          "title": "Creating Expert-Systems for Business Industry"
        },
        {
          "paperId": null,
          "title": "Object Oriented Model and Method Applied to Build an Expert System for Intoxication Diagnoses"
        },
        {
          "paperId": "92fc0736065d35fd089b86d60938930e5d18b7ab",
          "title": "Dynamic Derivation of Personalized Views"
        },
        {
          "paperId": "8d7197d958b8d89a566e2c7469c0aea6461ec841",
          "title": "Representational axes and temporal cooperative processes"
        },
        {
          "paperId": "61ed2ab133e48f34c59c14b4cc362cfa0f363392",
          "title": "A Step Towards Unification of Syntactic and Statistical Pattern Recognition"
        }
      ],
      "summary": "An Object Oriented Model for building Expert Systems is described and original algorithms which deal with total and partial structural similitude of objects to facilitate knowledge acquisition are proposed.",
      "title": "Automatic Knowledge Acquisition for Object-Oriented Expert Systems",
      "venue": "ArXiv",
      "year": 2020,
      "authors": "J. Colloc,D. Boulanger"
    },
    {
      "id": "0bfd4ed399054eae26c3cdaabc0aed80ca95e125",
      "url": "https://www.semanticscholar.org/paper/0bfd4ed399054eae26c3cdaabc0aed80ca95e125",
      "citationCount": 5,
      "influentialCitationCount": 3,
      "referenceCount": 33,
      "citations": [
        {
          "paperId": "4578747d52aa1b3537612287352a803a7b17e999",
          "title": "Asynchronous Neural Networks for Learning in Graphs"
        },
        {
          "paperId": "c36277b67814e0a522e786a38e1768612b0e63f2",
          "title": "Exploring the Learning Mechanisms of Neural Division Modules"
        },
        {
          "paperId": "0ce6a798f8222ed8f221326ca566311c648cb4dc",
          "title": "Learning Division with Neural Arithmetic Logic Modules"
        },
        {
          "paperId": "def7c67c3688bc459fbef3ce50a466a0cbf411a9",
          "title": "A Primer for Neural Arithmetic Logic Modules"
        },
        {
          "paperId": "30fcba6760b5fc5235ef8a0f2a61ccff63a7c3a0",
          "title": "MC-LSTM: Mass-Conserving LSTM"
        }
      ],
      "references": [
        {
          "paperId": "4e181e7f1067ee2b24db5f21e6c4cc0b4875edd2",
          "title": "Fractional SIR epidemiological models"
        },
        {
          "paperId": "4599f96dd1a2584e00d342953fc7e1361ffd6e1f",
          "title": "Neural Status Registers"
        },
        {
          "paperId": "e081dfa64e5ed12fe15fc885ec82ef1f35ab9830",
          "title": "iNALU: Improved Neural Arithmetic Logic Unit"
        },
        {
          "paperId": "4aea918d9bd66440ce0c00abbfbb57b212d76158",
          "title": "Neural Arithmetic Units"
        },
        {
          "paperId": "696b388ee6221c6dbcfd647a06883b2bfee773d9",
          "title": "Universal Differential Equations for Scientific Machine Learning"
        },
        {
          "paperId": "b39eed03d345f5c244eac12fd1315d26eba77d62",
          "title": "Deep Learning for Symbolic Mathematics"
        },
        {
          "paperId": "112ac68ddb0f021517dd465e89918fa52755cc35",
          "title": "Measuring Arithmetic Extrapolation Performance"
        },
        {
          "paperId": "afed6dc6900d3b37e528b9086661bba583d60bf6",
          "title": "Analysing Mathematical Reasoning Abilities of Neural Models"
        },
        {
          "paperId": "3c9961153493370500020c81527b3548c96f81e0",
          "title": "Data-driven discovery of coordinates and governing equations"
        },
        {
          "paperId": "bc00ff34ec7772080c7039b17f7069a2f7df0889",
          "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead"
        },
        {
          "paperId": null,
          "title": "Neural Ordinary Differential Equations. arXiv:1806.07366 [cs, stat], December 2019"
        },
        {
          "paperId": "e53fd0c9a04df127af80b2699f5f8f23a8a3e2af",
          "title": "On Evaluating the Generalization of LSTM Models in Formal Languages"
        },
        {
          "paperId": "86ccab92ea6bd1f1cfae8d6d1cca3ea71e89bdc7",
          "title": "Fashionable Modelling with Flux"
        },
        {
          "paperId": "6f69e19348870552a7ab92d038c8b8d753fe6b60",
          "title": "Neural Arithmetic Expression Calculator"
        },
        {
          "paperId": "5fc548f3f3112de7eddad3744717dc2f9d22ca38",
          "title": "Neural Arithmetic Logic Units"
        },
        {
          "paperId": "449310e3538b08b43227d660227dfd2875c3c3c1",
          "title": "Neural Ordinary Differential Equations"
        },
        {
          "paperId": "0c10a2cafb715f5c7b1810f89dd2eb3fadc5a38e",
          "title": "Finding numbers in the brain"
        },
        {
          "paperId": "856fe866bcce5e7a540655bea6ecc7406bdcfcba",
          "title": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks"
        },
        {
          "paperId": "34473be212082c64ee6c944758988fd64c9c9ef4",
          "title": "DifferentialEquations.jl – A Performant and Feature-Rich Ecosystem for\n Solving Differential Equations in Julia"
        },
        {
          "paperId": null,
          "title": "The Mythos of Model Interpretability. arXiv:1606.03490 [cs, stat], March 2017"
        },
        {
          "paperId": "5e4eb58d5b47ac1c73f4cf189497170e75ae6237",
          "title": "Neural GPUs Learn Algorithms"
        },
        {
          "paperId": "a2499dd426c46c645ee805d7594b6687547c72d4",
          "title": "Neural Random Access Machines"
        },
        {
          "paperId": "5b791cd374c7109693aaddee2c12d659ae4e3ec0",
          "title": "Grid Long Short-Term Memory"
        },
        {
          "paperId": null,
          "title": "Neural Random-Access Machines. arXiv:1511.06392 [cs], February 2016"
        },
        {
          "paperId": null,
          "title": "Neural GPUs Learn Algorithms. arXiv:1511.08228 [cs], March 2016"
        },
        {
          "paperId": "c3823aacea60bc1f2cabb9283144690a3d015db5",
          "title": "Neural Turing Machines"
        },
        {
          "paperId": null,
          "title": "Fractional SIR Epidemiological Models. preprint, Epidemiology, April 2020"
        },
        {
          "paperId": null,
          "title": "Neural Status Registers. arXiv:2004.07085 [cs, stat], April 2020"
        },
        {
          "paperId": "a3b7e80260891dcd3844b1835df8dee3a1cd67c7",
          "title": "The Number Sense: How the Mind Creates Mathematics."
        },
        {
          "paperId": "801257f13663c493851d55366b431fad0cc801a9",
          "title": "A contribution to the mathematical theory of epidemics"
        },
        {
          "paperId": null,
          "title": "Deep Learning for Symbolic Mathematics. arXiv:1912.01412 [cs], December 2019"
        },
        {
          "paperId": null,
          "title": "Measuring Arithmetic Extrapolation Performance. arXiv:1910.01888 [cs], November 2019"
        },
        {
          "paperId": null,
          "title": "Measuring arithmetic extrapolation performance. CoRR"
        }
      ],
      "summary": "The Neural Power Unit (NPU) is introduced that operates on the full domain of real numbers and is capable of learning arbitrary power functions in a single layer and fixes the shortcomings of existing arithmetic units and extends their expressivity.",
      "title": "Neural Power Units",
      "venue": "NeurIPS",
      "year": 2020,
      "authors": "Niklas Heim,T. Pevný,V. Šmídl"
    },
    {
      "id": "63857190aaf5aab1d94b54bb257b7b03b8cb5a50",
      "url": "https://www.semanticscholar.org/paper/63857190aaf5aab1d94b54bb257b7b03b8cb5a50",
      "citationCount": 22,
      "influentialCitationCount": 1,
      "referenceCount": 30,
      "citations": [
        {
          "paperId": "05e00e7c1e040ae79fdd50764d4a1fd5dd04edf8",
          "title": "Simplified State Space Layers for Sequence Modeling"
        },
        {
          "paperId": "f0b0d9a437d93b1d8b694b9ff57632afdf86c2fd",
          "title": "Efficient Long-Text Understanding with Short-Text Models"
        },
        {
          "paperId": "4c557a4dc8b9cf78d5b3a4d4e489e5223d7f0025",
          "title": "Explain My Surprise: Learning Efficient Long-Term Memory by Predicting Uncertain Outcomes"
        },
        {
          "paperId": "088abe4050e06287dac8c82616b37de4eb793950",
          "title": "Recurrent Memory Transformer"
        },
        {
          "paperId": "5deef7fc161cbdc884aff15b9810f8a432c1489a",
          "title": "k-means Mask Transformer"
        },
        {
          "paperId": "eaef083b9d661f42cc0d89d9d8156218f33a91d9",
          "title": "Long Range Language Modeling via Gated State Spaces"
        },
        {
          "paperId": "31a9744bd5421b3fbbad2ab38ce33bb2f352c77a",
          "title": "CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation"
        },
        {
          "paperId": "b9a701c90f3d3df27366f5b29a97f798eb940ac7",
          "title": "ChapterBreak: A Challenge Dataset for Long-Range Language Models"
        },
        {
          "paperId": "71e15a9a52dcafca57bff5f310b95e2c7d0cfc87",
          "title": "Diagonal State Spaces are as Effective as Structured State Spaces"
        },
        {
          "paperId": "2f5a60cce19c6e5ca5a527f713d3b1545d64f0ef",
          "title": "Socialformer: Social Network Inspired Long Document Modeling for Document Ranking"
        },
        {
          "paperId": "dbd684b259d6d7383ad4bb354ed53a0d12d38b63",
          "title": "Leveraging Multi-view Inter-passage Interactions for Neural Document Ranking"
        },
        {
          "paperId": "73d64ecbe3e846394444dab6c5e89ba33e5daa49",
          "title": "Memory transformer with hierarchical attention for long document processing"
        },
        {
          "paperId": "2ca44146b2ee3859d0a9f9e04e4bec0983e3f57b",
          "title": "Interpretable Self-supervised Multi-task Learning for COVID-19 Information Retrieval and Extraction"
        },
        {
          "paperId": "c1a4278f969acfc6682a924e31b95e1ade9703ee",
          "title": "Memory-efficient Transformers via Top-k Attention"
        },
        {
          "paperId": "af679d69fcc1d0fcf0f039aba937853bcb50a8de",
          "title": "Luna: Linear Unified Nested Attention"
        },
        {
          "paperId": "69285c9040c1356272752499b7e1e53ef25ac008",
          "title": "Beyond Paragraphs: NLP for Long Sequences"
        },
        {
          "paperId": "a4bd6a22dcdc740a9ff20af48fe2d828f0190b17",
          "title": "Value-aware Approximate Attention"
        },
        {
          "paperId": "787119e3c3f819244c82b7d97779473773e60696",
          "title": "MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers"
        },
        {
          "paperId": "9b1434c63cec05b12500bc70e09fa6861e03a80d",
          "title": "Topic Sentence Named Entity Recognition: A New Task with Its Dataset and Benchmarks"
        },
        {
          "paperId": "a8a048b862cf84a78e0a3f2e8ca8979b7c70ddb7",
          "title": "An explainability analysis of a sentiment prediction task using a transformer-based attention filter"
        },
        {
          "paperId": "71b6394ad5654f5cd0fba763768ba4e523f7bbca",
          "title": "Longformer: The Long-Document Transformer"
        },
        {
          "paperId": "d828ec273669d7c3769d1f66ec37edfbc5de15f3",
          "title": "TRANSFORMER-QL: A STEP TOWARDS MAKING TRANSFORMER NETWORK QUADRATICALLY LARGE"
        }
      ],
      "references": [
        {
          "paperId": "657329c633709dd1ac34a30d57341b186b1a47c2",
          "title": "Efficient Content-Based Sparse Attention with Routing Transformers"
        },
        {
          "paperId": "71b6394ad5654f5cd0fba763768ba4e523f7bbca",
          "title": "Longformer: The Long-Document Transformer"
        },
        {
          "paperId": "79cd9f77e5258f62c0e15d11534aea6393ef73fe",
          "title": "Dense Passage Retrieval for Open-Domain Question Answering"
        },
        {
          "paperId": "3dd61d97827e3f380bf9304101149a3f865051fc",
          "title": "Injecting Numerical Reasoning Skills into Language Models"
        },
        {
          "paperId": "80376bdec5f534be78ba82821f540590ebce5559",
          "title": "How Much Knowledge Can You Pack into the Parameters of a Language Model?"
        },
        {
          "paperId": "055fd6a9f7293269f1b22c1470e63bd02d8d9500",
          "title": "Reformer: The Efficient Transformer"
        },
        {
          "paperId": "b39eed03d345f5c244eac12fd1315d26eba77d62",
          "title": "Deep Learning for Symbolic Mathematics"
        },
        {
          "paperId": "f51497f463566581874c941353dd9d80069c5b77",
          "title": "Compressive Transformers for Long-Range Sequence Modelling"
        },
        {
          "paperId": "2cf3bd0cc1382f35384e259d99e4f9744eeaed28",
          "title": "Blockwise Self-Attention for Long Document Understanding"
        },
        {
          "paperId": "3cfb319689f06bf04c2e28399361f414ca32c4b3",
          "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
          "paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
          "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
        },
        {
          "paperId": "2e14e84ccec924ed770b58108ad1d9de6f0ca295",
          "title": "BP-Transformer: Modelling Long-Range Context via Binary Partitioning"
        },
        {
          "paperId": "1fa9ed2bea208511ae698a967875e943049f16b6",
          "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"
        },
        {
          "paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3",
          "title": "Language Models as Knowledge Bases?"
        },
        {
          "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
          "paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
          "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
        },
        {
          "paperId": "d6dccb5d71fbb6f5765f89633ba3a8e6809a720d",
          "title": "Stand-Alone Self-Attention in Vision Models"
        },
        {
          "paperId": "95f2aedef1453e5d88af9d5fddb79e0e56223cb0",
          "title": "Compositional Questions Do Not Necessitate Multi-hop Reasoning"
        },
        {
          "paperId": "455a8838cde44f288d456d01c76ede95b56dc675",
          "title": "A Structural Probe for Finding Syntax in Word Representations"
        },
        {
          "paperId": "21da617a0f79aabf94272107184606cefe90ab75",
          "title": "Generating Long Sequences with Sparse Transformers"
        },
        {
          "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
          "paperId": "22655979df781d222eaf812b0d325fa9adf11594",
          "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"
        },
        {
          "paperId": "8ba5f5106cd039a22f0d7fdf97d700e427dde282",
          "title": "Music Transformer"
        },
        {
          "paperId": "807370352540f171685998aec7a5701f7110f147",
          "title": "Understanding Back-Translation at Scale"
        },
        {
          "paperId": "1db9bd18681b96473f3c82b21edc9240b44dc329",
          "title": "Image Transformer"
        },
        {
          "paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f",
          "title": "Deep Contextualized Word Representations"
        },
        {
          "paperId": "d91043f0d48b9b2c8ff7ee321abb8fd7efafff7a",
          "title": "The NarrativeQA Reading Comprehension Challenge"
        },
        {
          "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
          "title": "Attention is All you Need"
        },
        {
          "paperId": "05dd7254b632376973f3a1b4d39485da17814df5",
          "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
        },
        {
          "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
          "title": "GloVe: Global Vectors for Word Representation"
        }
      ],
      "summary": "This work proposes to augment sparse Transformer blocks with a dense attention-based $\\textit{global memory}$ of length $M$ ($\\ll L$) which provides an aggregate global view of the entire input sequence to each position, and empirically shows that this method leads to substantial improvement on a range of tasks.",
      "title": "GMAT: Global Memory Augmentation for Transformers",
      "venue": "ArXiv",
      "year": 2020,
      "authors": "Ankit Gupta,Jonathan Berant"
    },
    {
      "id": "79e5447a394ad178aa0fdaef1d2f1ecf9bf07e88",
      "url": "https://www.semanticscholar.org/paper/79e5447a394ad178aa0fdaef1d2f1ecf9bf07e88",
      "citationCount": 62,
      "influentialCitationCount": 8,
      "referenceCount": 44,
      "citations": [
        {
          "paperId": "5484d228bfc50efbac6e86677bc2ec2ee4ede1a6",
          "title": "Scaling Instruction-Finetuned Language Models"
        },
        {
          "paperId": "c9e9ba31f4176bf85860cd7b46723c1ebb733989",
          "title": "Can Pretrained Language Models (Yet) Reason Deductively?"
        },
        {
          "paperId": "1ff913af955034a01e236729e1302f72f0dcaccb",
          "title": "COMPS: Conceptual Minimal Pair Sentences for testing Property Knowledge and Inheritance in Pre-trained Language Models"
        },
        {
          "paperId": "7cdf6ed502c5c294b84d357e31405a07b836b080",
          "title": "When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment"
        },
        {
          "paperId": "fa46f4ddd5c6e793a47c61db9c1ecde7ea1c82bc",
          "title": "Dynamic Generation of Interpretable Inference Rules in a Neuro-Symbolic Expert System"
        },
        {
          "paperId": "5581bf85386737bd3378eec68189759a05280bea",
          "title": "FOLIO: Natural Language Reasoning with First-Order Logic"
        },
        {
          "paperId": "cb11d43d90aa5da2c0a8a682efc52e204dc3e0e0",
          "title": "Reasoning over Logically Interacted Conditions for Question Answering"
        },
        {
          "paperId": "c4487e6690173618bfec98751ce3590204e981ee",
          "title": "Logical Reasoning with Span Predictions: Span-level Logical Atoms for Interpretable and Robust NLI Models"
        },
        {
          "paperId": "5e3a7235c730b512f0bb3004a6deb0c88ec9fc8e",
          "title": "On the Paradox of Learning to Reason from Data"
        },
        {
          "paperId": "241c0f3d21dbcd6081e21b8856200f6e2926bf98",
          "title": "Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models"
        },
        {
          "paperId": "4b516216d7d150a081fd74993bddf36b6b22c118",
          "title": "Chain of Thought Imitation with Procedure Cloning"
        },
        {
          "paperId": "e1943cbf4817605a1f988fe5fd785f6b707ca233",
          "title": "METGEN: A Module-Based Entailment Tree Generation Framework for Answer Explanation"
        },
        {
          "paperId": "0abcd69d87723b93bfc2074167418a4cf81a892e",
          "title": "Logically Consistent Adversarial Attacks for Soft Theorem Provers"
        },
        {
          "paperId": "8e9b51c87fab57313b6b30ed9b08b02a7abb5031",
          "title": "Inferring Implicit Relations with Language Models"
        },
        {
          "paperId": "2f21201ac9fcb88a72c56471402388ec2fc365a8",
          "title": "Inferring Implicit Relations in Complex Questions with Language Models"
        },
        {
          "paperId": "36589346063ff26506330451976280011273b935",
          "title": "Towards Teachable Reasoning Systems"
        },
        {
          "paperId": "cd471b5ef162906ef3d9a84398b3f98e9ee4bf56",
          "title": "A Review on Language Models as Knowledge Bases"
        },
        {
          "paperId": "243227a279e6a4193ef653fd763d2b34c9edb45d",
          "title": "Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach"
        },
        {
          "paperId": "242eaa55cce5daad200850dd10a788a0f960cdd8",
          "title": "Logical Reasoning for Task Oriented Dialogue Systems"
        },
        {
          "paperId": "5d0db797a45ce2453f821f7ded0b547d3fdab054",
          "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
        },
        {
          "paperId": "efd1e9b179e09cd62185f2a04e756e9ff44b8689",
          "title": "Natural Language Deduction through Search over Statement Compositions"
        },
        {
          "paperId": "41f44979cf1cd3f4cbd615dc130bc33721f5281b",
          "title": "Memory-assisted prompt editing to improve GPT-3 after deployment"
        },
        {
          "paperId": "bd44f34b47c8a4b6947695853fc2814ac69664a6",
          "title": "Datasheet for the Pile"
        },
        {
          "paperId": "0b483b550b21ec42d693fc04a372dbb10dd07019",
          "title": "Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge"
        },
        {
          "paperId": "188aa594c294f01fd18e0ff43eeb0d8430b39b90",
          "title": "Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback"
        },
        {
          "paperId": "3ecc1bb0171e7540410fbd454fed473b8b6fa437",
          "title": "On Semantic Cognition, Inductive Generalization, and Language Models"
        },
        {
          "paperId": "ed02d4a08a50d23709b7ce5df9878a8bc34ac12c",
          "title": "GNN is a Counter? Revisiting GNN for Question Answering"
        },
        {
          "paperId": "5b6c582d51266be9aa7e32bfdc20891e5231eca4",
          "title": "When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data"
        },
        {
          "paperId": "fed460648303afa32247e493847e4dc73dc1a5b3",
          "title": "Can We Guide a Multi-Hop Reasoning Language Model to Incrementally Learn at Each Single-Hop?"
        },
        {
          "paperId": "44772fe1c3fa422a3da7e25092db2544893d6bfb",
          "title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming"
        },
        {
          "paperId": "41a4482bb47de60fdc44492fa633701e425cb501",
          "title": "SpARC: Sparsity Activation Regularization for Consistency"
        },
        {
          "paperId": "63cd10c4ca6733a8894d55cf1343636fa816cf7c",
          "title": "Analyzing the Contribution of Commonsense Knowledge Sources for Why-Question Answering"
        },
        {
          "paperId": "07d5bba7d2bc511c88eb143a926d3c297298ad15",
          "title": "Interscript: A dataset for interactive learning of scripts through error feedback"
        },
        {
          "paperId": "f4a2a13b5ff0f4b4a83f3b4effe84dbf8bb84884",
          "title": "LoNLI: An Extensible Framework for Testing Diverse Logical Reasoning Capabilities for NLI"
        },
        {
          "paperId": "4a247cbfca9dcf91e2da24e6d2d84601a9041a8f",
          "title": "Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs"
        },
        {
          "paperId": "2af476f7c2a7c040fc9ab7750bf41a84f66aa947",
          "title": "Knowledge Based Multilingual Language Model"
        },
        {
          "paperId": "a466d10b80dbdee3b130bef73ec62f3a89eb389b",
          "title": "Enhancing Multilingual Language Model with Massive Multilingual Knowledge Triples"
        },
        {
          "paperId": "ecad7a322ede6de0013b46dc64429eed4c43e8af",
          "title": "BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief"
        },
        {
          "paperId": "6ed7f8d8673fbf380c45cefa30138ff2b77d1d1b",
          "title": "Abstraction, Reasoning and Deep Learning: A Study of the \"Look and Say\" Sequence"
        },
        {
          "paperId": "e55391a9406245584b3e5b3225dad2e171b9a06b",
          "title": "RuleBERT: Teaching Soft Rules to Pre-Trained Language Models"
        },
        {
          "paperId": "d65a064eb837f838faf6ff67781b62450b92b159",
          "title": "CommonsenseQA 2.0: Exposing the Limits of AI through Gamification"
        },
        {
          "paperId": "f5ca46585818771e64ee9449c930748fbee35cba",
          "title": "Improving Neural Model Performance through Natural Language Feedback on Their Explanations"
        },
        {
          "paperId": "911b7539e964782670e555930b291de16fa971c5",
          "title": "Flexible Generation of Natural Language Deductions"
        },
        {
          "paperId": "d331de3b6bebb0f9af1fddf1b730ec057a7026d4",
          "title": "Relational World Knowledge Representation in Contextual Language Models: A Review"
        },
        {
          "paperId": "f06ca547e5eaf0fd118f184b9ddd1cf7cd5c29e0",
          "title": "Updater-Extractor Architecture for Inductive World State Representations"
        },
        {
          "paperId": "4ea544c849aee3e4f99e3d835ab3ea9ed685a5b9",
          "title": "KI-BERT: Infusing Knowledge Context for Better Language and Domain Understanding"
        },
        {
          "paperId": "008b9fc834f5839a25febe150f3076d550ee442f",
          "title": "Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2"
        },
        {
          "paperId": "28a5a53dafacebad8a7c47773079caeffb9a5baa",
          "title": "Representing Numbers in NLP: a Survey and a Vision"
        },
        {
          "paperId": "e0c4a2116ec0f3dc48457627bbd8bfe1f0026479",
          "title": "Overcoming Poor Word Embeddings with Word Definitions"
        },
        {
          "paperId": "dcb67a471a3c6f782cf237ced1da28d3da23553c",
          "title": "Concepts, Properties and an Approach for Compositional Generalization"
        },
        {
          "paperId": "87c45a908537ffe1d2ab71a5d609bd7b4efa4fe1",
          "title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language"
        },
        {
          "paperId": "18e5fb8cec55a75b288a499c57d77ede541dc049",
          "title": "Knowledge-driven Data Construction for Zero-shot Evaluation in Commonsense Question Answering"
        },
        {
          "paperId": "d877020a6808f0d4baded6d6cd92d09bbc46bde2",
          "title": "Constructing Taxonomies from Pretrained Language Models"
        },
        {
          "paperId": "751f55beac45cec14b0aff6174ed0139afb54b08",
          "title": "Modelling Symbolic Knowledge Using Neural Representations"
        },
        {
          "paperId": "f0ba384a13adfd4280ed10ff9974b787193c2531",
          "title": "Improving scripts with a memory of natural feedback"
        },
        {
          "paperId": "8424082e3bf4792462eb112d7ebcecf5b0dc3613",
          "title": "Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"
        },
        {
          "paperId": "884c0b6db564208d99cadf2548f0aa96dee5f859",
          "title": "Commonsense Reasoning with Implicit Knowledge in Natural Language"
        },
        {
          "paperId": "1b806d23d6e1daee1a7fa8df12b009e5c64bee59",
          "title": "Are we there yet? Exploring clinical domain knowledge of BERT models"
        },
        {
          "paperId": "f074ec057ae40bf8ea330bfa0df166a93e459439",
          "title": "Flexible Operations for Natural Language Deduction"
        },
        {
          "paperId": "2db020e3398c06e3a22f12d8caffe76b0d9d1dda",
          "title": "Knowledge-driven Self-supervision for Zero-shot Commonsense Question Answering"
        },
        {
          "paperId": "4bda9767c11a97a3d1a83577cb8ec94f16ceccb5",
          "title": "A Closer Look at Linguistic Knowledge in Masked Language Models: The Case of Relative Clauses in American English"
        },
        {
          "paperId": "bea36d43c6c0cbcb197231aa70ca095e331a967b",
          "title": "Inducing Taxonomic Knowledge from Pretrained Transformers"
        }
      ],
      "references": [
        {
          "paperId": "ad5970584754cc7a1d91c95ab84a1e210258183a",
          "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System"
        },
        {
          "paperId": "15ad2b27c5248e7d1db5456794ca1ca8a8198f5d",
          "title": "Transformers as Soft Reasoners over Language"
        },
        {
          "paperId": "c44120f765fc43994c5cfb4e12e4f62999efeae6",
          "title": "How Context Affects Language Models' Factual Predictions"
        },
        {
          "paperId": "5e0cffc51e8b64a8f11326f955fa4b4f1803e3be",
          "title": "oLMpics-On What Language Model Pre-training Captures"
        },
        {
          "paperId": "5a9001cdccdb8b1de227a45eccc503d32d1a2464",
          "title": "What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge"
        },
        {
          "paperId": "b39eed03d345f5c244eac12fd1315d26eba77d62",
          "title": "Deep Learning for Symbolic Mathematics"
        },
        {
          "paperId": "521b4e26df0f1cf5763dece14cbb218df152dc59",
          "title": "Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering"
        },
        {
          "paperId": "207da6d2c07289bf72a2b5974bb3f011ebb5dd0d",
          "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding"
        },
        {
          "paperId": "681fbcd98acf20df3355eff3585994bd1f9008b7",
          "title": "Probing Natural Language Inference Models through Semantic Fragments"
        },
        {
          "paperId": "401dc39c2c8c910253d47980cfa3b4d2f7790d9b",
          "title": "WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale"
        },
        {
          "paperId": "c2c165dd615d4fc31d4fef4b4acbcab1a1655983",
          "title": "On Making Reading Comprehension More Comprehensive"
        },
        {
          "paperId": "2785e7e7f625630eeeedbc45124acf7931ba878d",
          "title": "Compositional Generalization for Primitive Substitutions"
        },
        {
          "paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3",
          "title": "Language Models as Knowledge Bases?"
        },
        {
          "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
          "paperId": "95f2aedef1453e5d88af9d5fddb79e0e56223cb0",
          "title": "Compositional Questions Do Not Necessitate Multi-hop Reasoning"
        },
        {
          "paperId": "0707f1e3791a6805bf4542605245cf4cdee3b9e0",
          "title": "Avoiding Reasoning Shortcuts: Adversarial Evaluation, Training, and Model Development for Multi-Hop QA"
        },
        {
          "paperId": "636904d91d9dd1a641a595d9578ba7640f35aa74",
          "title": "MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension"
        },
        {
          "paperId": "df2ce576eca0b3db177c83bc6cf0f9fe2c7714f0",
          "title": "Dynamically Fused Graph Network for Multi-hop Reasoning"
        },
        {
          "paperId": "afed6dc6900d3b37e528b9086661bba583d60bf6",
          "title": "Analysing Mathematical Reasoning Abilities of Neural Models"
        },
        {
          "paperId": "c21a4d70d83e0f6eb2a9e1c41d034842dd561e47",
          "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"
        },
        {
          "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
          "paperId": "22655979df781d222eaf812b0d325fa9adf11594",
          "title": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"
        },
        {
          "paperId": "af5c4b80fbf847f69a202ba5a780a3dd18c1a027",
          "title": "SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"
        },
        {
          "paperId": "53e50a313ddfd222d958469edb6742f19458ec74",
          "title": "Modeling Semantic Plausibility by Injecting World Knowledge"
        },
        {
          "paperId": "c8725f13be7434b69738491c66b45c9225258253",
          "title": "The Web as a Knowledge-Base for Answering Complex Questions"
        },
        {
          "paperId": "2997b26ffb8c291ce478bd8a6e47979d5a55c466",
          "title": "Annotation Artifacts in Natural Language Inference Data"
        },
        {
          "paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f",
          "title": "Deep Contextualized Word Representations"
        },
        {
          "paperId": "fbba8629ff9633ca57be1f2209d53d9bcfc2273c",
          "title": "Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference"
        },
        {
          "paperId": "7d5cf22c70484fe217936c66741fb73b2a278bde",
          "title": "Constructing Datasets for Multi-hop Reading Comprehension Across Documents"
        },
        {
          "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
          "title": "Improving Language Understanding by Generative Pre-Training"
        },
        {
          "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
          "title": "Attention is All you Need"
        },
        {
          "paperId": "26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810",
          "title": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"
        },
        {
          "paperId": "83e7654d545fbbaaf2328df365a781fb67b841b4",
          "title": "Enhanced LSTM for Natural Language Inference"
        },
        {
          "paperId": "2cd8e8f510c89c7c18268e8ad51c061e459ad321",
          "title": "A Decomposable Attention Model for Natural Language Inference"
        },
        {
          "paperId": "a9de2fc7ae3230652f27fd95e7175da8cf44f075",
          "title": "Harnessing Deep Neural Networks with Logic Rules"
        },
        {
          "paperId": "f04df4e20a18358ea2f689b4c129781628ef7fc1",
          "title": "A large annotated corpus for learning natural language inference"
        },
        {
          "paperId": "cf8f2ca0c2d618104bc8724a6effc509088f16c4",
          "title": "Never-Ending Learning"
        },
        {
          "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
          "title": "GloVe: Global Vectors for Word Representation"
        },
        {
          "paperId": "dab7e605237ad4f4fe56dcba2861b8f0a57112be",
          "title": "Wikidata: a free collaborative knowledgebase"
        },
        {
          "paperId": "e9cb15132afd04ae93ba693044fd73f6746f58c4",
          "title": "Recognizing Textual Entailment: Models and Applications"
        },
        {
          "paperId": "c9c17279a2a6e564219cf573a74c6675cebb62e6",
          "title": "True Knowledge: Open-Domain Question Answering Using Structured Knowledge and Inference"
        },
        {
          "paperId": "d87ceda3042f781c341ac17109d1e94a717f5f60",
          "title": "WordNet : an electronic lexical database"
        },
        {
          "paperId": "563e0a5ab5e384a4afc7ae71bada34bd709498cd",
          "title": "Interactive Transfer of Expertise: Acquisition of New Inference Rules"
        },
        {
          "paperId": "494aedf82da4755badc1fe74e4d21cf5fc029e9d",
          "title": "Programs with common sense"
        }
      ],
      "summary": "This work provides a first demonstration that LMs can be trained to reliably perform systematic reasoning combining both implicit, pre-trained knowledge and explicit natural language statements, and demonstrates that models learn to effectively perform inference which involves implicit taxonomic and world knowledge, chaining and counting.",
      "title": "Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge",
      "venue": "NeurIPS",
      "year": 2020,
      "authors": "Alon Talmor,Oyvind Tafjord,Peter Clark,Yoav Goldberg,Jonathan Berant"
    },
    {
      "id": "8256f48f759cf85044db251cc512f965834945b3",
      "url": "https://www.semanticscholar.org/paper/8256f48f759cf85044db251cc512f965834945b3",
      "citationCount": 113,
      "influentialCitationCount": 15,
      "referenceCount": 34,
      "citations": [
        {
          "paperId": "4c2534f9b03ac2f3810c07abc398a11bcf47258e",
          "title": "Transformers Learn Shortcuts to Automata"
        },
        {
          "paperId": "240300b1da360f22bf0b82c6817eacebba6deed4",
          "title": "What Makes Convolutional Models Great on Long Sequence Modeling?"
        },
        {
          "paperId": "5cdb940cb0e8158bc5bc5aabcee84ffeee0c30fe",
          "title": "Improve Transformer Pre-Training with Decoupled Directional Relative Position Encoding and Representation Differentiations"
        },
        {
          "paperId": "03b7d2e942eafabd14b229f8962fa6e943053f75",
          "title": "Melody Infilling with User-Provided Structural Context"
        },
        {
          "paperId": "20dfefb11a7cbf9d38838bc02418aa67254d5129",
          "title": "A Dual-Attention Learning Network with Word and Sentence Embedding for Medical Visual Question Answering"
        },
        {
          "paperId": "6099697f115c7fb73310b2c991086feeabb80ec1",
          "title": "Husformer: A Multi-Modal Transformer for Multi-Modal Human State Recognition"
        },
        {
          "paperId": "3664a3f9a386a93f5bc98bea70a098998e38cdf4",
          "title": "Mega: Moving Average Equipped Gated Attention"
        },
        {
          "paperId": "d6bf7e75662363c306c4c791d9ab1b4dd6a0b6f5",
          "title": "Hyperspectral Image Classification via Spectral Pooling and Hybrid Transformer"
        },
        {
          "paperId": "1df156d53295354f28fad0e7103f94e624adca4c",
          "title": "Pre-Training a Graph Recurrent Network for Language Representation"
        },
        {
          "paperId": "acd082f28074e047792691c5236819bb50132a7a",
          "title": "Improving Micro-video Recommendation by Controlling Position Bias"
        },
        {
          "paperId": "076deb54a5d776cd21eabf2c40cdd839f53d6d77",
          "title": "giMLPs: Gate with Inhibition Mechanism in MLPs"
        },
        {
          "paperId": "e9d1a851d4f7950db360d12fe7f96647aaf547da",
          "title": "Generalized Attention Mechanism and Relative Position for Transformer"
        },
        {
          "paperId": "4299353235595391e2b4f7298baffd00b5acf9d1",
          "title": "LordBERT: Embedding Long Text by Segment Ordering with BERT"
        },
        {
          "paperId": "4d80dae4039b4de6c1ba30227d5981fd1cbb4a1f",
          "title": "Parameterization of Cross-token Relations with Relative Positional Encoding for Vision MLP"
        },
        {
          "paperId": "9692886ba8e2c9d8990b0505e9c67a696d9f28a7",
          "title": "A Closer Look into Transformer-Based Code Intelligence Through Code Transformation: Challenges and Opportunities"
        },
        {
          "paperId": "bc67a1d69d53bb6b5a4efa86721122f09532c97e",
          "title": "Progressive Self-Attention Network with Unsymmetrical Positional Encoding for Sequential Recommendation"
        },
        {
          "paperId": "e28adeb4db46469df9f9bd653501871ddc5f4318",
          "title": "MuchSUM: Multi-channel Graph Neural Network for Extractive Summarization"
        },
        {
          "paperId": "8d47a9950a44b0d2f1172620966c4737908ad542",
          "title": "R^2VOS: Robust Referring Video Object Segmentation via Relational Multimodal Cycle Consistency"
        },
        {
          "paperId": "1fd8e0fc48300d765ac35b8e6ed72ea1d3061f8e",
          "title": "LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks"
        },
        {
          "paperId": "9b1b92ee4f029a0fdb18c9effbc78fdb8b5f38e6",
          "title": "MetaTPTrans: A Meta Learning Approach for Multilingual Code Representation Learning"
        },
        {
          "paperId": "a3a125d28fed30b72e676914aa810b70644e98a1",
          "title": "Your Transformer May Not be as Powerful as You Expect"
        },
        {
          "paperId": "3884307cb95329275755baaf99600e7431be695d",
          "title": "Efficient U-Transformer with Boundary-Aware Loss for Action Segmentation"
        },
        {
          "paperId": "8e6ada0242dc9b3680ccf6ddeec9e7c3cd41766f",
          "title": "Dynamically Relative Position Encoding-Based Transformer for Automatic Code Edit"
        },
        {
          "paperId": "7a93df4da51f45e2af79856d417c77ca697016ab",
          "title": "Neural Additive Models for Nowcasting"
        },
        {
          "paperId": "d6c5aab433d9871cabc01ffb1e5e1ea89141155b",
          "title": "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"
        },
        {
          "paperId": "6e98a35c439c5c7387de6bffd96dea9b2943a548",
          "title": "Trading Positional Complexity vs. Deepness in Coordinate Networks"
        },
        {
          "paperId": "9d03a164759bb5cc2fa6b575254b58f790ab6785",
          "title": "Zero-shot Code-Mixed Offensive Span Identification through Rationale Extraction"
        },
        {
          "paperId": "7f9290803a6df981cc460894487868226e2131f3",
          "title": "RM-Transformer: A Transformer-based Model for Mandarin Speech Recognition"
        },
        {
          "paperId": "b0773e4edee5a5f2bfc11ce793a7011a922ce743",
          "title": "Spatial-Temporal Interval Aware Sequential POI Recommendation"
        },
        {
          "paperId": "501eb473c1ca97e66f7a0ff6379a00756d869d6c",
          "title": "Decoupled Side Information Fusion for Sequential Recommendation"
        },
        {
          "paperId": "2193d898e556ad2ac14f0ababf02f90e6fdfe663",
          "title": "DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks"
        },
        {
          "paperId": "c6dc862d4d105de77a39d30a7e79366ec891e486",
          "title": "Dynamic Position Encoding for Transformers"
        },
        {
          "paperId": "682ca4de7fc91ed44a86ce7762d74f58791f63e7",
          "title": "3D Shuffle-Mixer: An Efficient Context-Aware Vision Learner of Transformer-MLP Paradigm for Dense Prediction in Medical Volume"
        },
        {
          "paperId": "b6ec1e8f18185b4b3d46201359a440404575460c",
          "title": "METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals"
        },
        {
          "paperId": "02f36289e227595dd2786b32c3975ce7e61dff48",
          "title": "Pretraining Text Encoders with Adversarial Mixture of Training Signal Generators"
        },
        {
          "paperId": "838a2297b94f7bad96c4f8370a5f58487f194f44",
          "title": "Visual Abductive Reasoning"
        },
        {
          "paperId": "34f2ed08461edc261984fc1c86a14ff7b4c3d2db",
          "title": "KinyaBERT: a Morphology-aware Kinyarwanda Language Model"
        },
        {
          "paperId": "4e9cd6be4a8fcad2ca562fcf41a1f882387a3167",
          "title": "LGT-Net: Indoor Panoramic Room Layout Estimation with Geometry-Aware Transformer Network"
        },
        {
          "paperId": "60c07da8f233dd3af214d4d5fbd2582a165797d8",
          "title": "Probabilistic deep learning model as a tool for supporting the fast simulation of a thermal-hydraulic code"
        },
        {
          "paperId": "50af83ea20201b51014358534650213e6133650c",
          "title": "FastRPB: a Scalable Relative Positional Encoding for Long Sequence Tasks"
        },
        {
          "paperId": "12809bcb734beafeb47876f42e7b438e27fe99fe",
          "title": "General-purpose, long-context autoregressive modeling with Perceiver AR"
        },
        {
          "paperId": "5f104a804ed245c79847ad0593e8f86196d697b1",
          "title": "Transformers in Time Series: A Survey"
        },
        {
          "paperId": "c72a8f823f9e2d4c9820df5f78e3f6c4922910b6",
          "title": "Improving Sample Efficiency of Value Based Models Using Attention and Vision Transformers"
        },
        {
          "paperId": "3ee219ce028b78f6d0c099043f9e0a24106d8aa5",
          "title": "Rewiring with Positional Encodings for Graph Neural Networks"
        },
        {
          "paperId": "be0fbb810583930c071d0b9b2c5187fe260783f5",
          "title": "Swin Transformer V2: Scaling Up Capacity and Resolution"
        },
        {
          "paperId": "28a65397488d826e0063df6352f00e6a660d0a2f",
          "title": "Theme Transformer: Symbolic Music Generation with Theme-Conditioned Transformer"
        },
        {
          "paperId": "87e823d2cb58e741230c0fa3b83f3459c7e32241",
          "title": "PiSLTRc: Position-Informed Sign Language Transformer With Content-Aware Convolution"
        },
        {
          "paperId": "e38dc0554dd89745bb17039a4d4ee9d714cf77f1",
          "title": "SpectralFormer: Rethinking Hyperspectral Image Classification With Transformers"
        },
        {
          "paperId": "d8d2e574965fe733eb1416e03df2b5c2914fc530",
          "title": "A Survey of Transformers"
        },
        {
          "paperId": "07e987364bf0be1949e379f976f8dea675977337",
          "title": "MSG-Transformer: Exchanging Local Spatial Information by Manipulating Messenger Tokens"
        },
        {
          "paperId": "b8cee43a51c44f8f4448e78e41ecf081987707cf",
          "title": "Towards Robust Vision Transformer"
        },
        {
          "paperId": "7072db6eddb85ecd2c117365d91bd694760f726e",
          "title": "Position Information in Transformers: An Overview"
        },
        {
          "paperId": "5cea5af1822e7407c8393bd7c5d0208af26ea57e",
          "title": "Priming Ancient Korean Neural Machine Translation"
        },
        {
          "paperId": "08013183713f427258d36332b1bee9d6ce37cf30",
          "title": "S TRUCTURE -A WARE T RANSFORMER P OLICY FOR I NHOMOGENEOUS M ULTI -T ASK R EINFORCEMENT L EARNING"
        },
        {
          "paperId": "9fe29c834afbe1848d9df713ae6e0ca3bd053605",
          "title": "Probing the Role of Positional Information in Vision-Language Models"
        },
        {
          "paperId": "7865ae1c4d7ba0e3ba48aef54270c3fdf61ec11d",
          "title": "Symbolic Data Augmentation for Assisted Neural Reasoning"
        },
        {
          "paperId": "6410348a7b9da61ea5ca2979aaebb164d4b87b01",
          "title": "Going “Deeper”: Structured Sememe Prediction via Transformer with Tree Attention"
        },
        {
          "paperId": "5fb29bd2eb585be9c7e2f8a483cc5fe8255d77c2",
          "title": "SoK: Vehicle Orientation Representations for Deep Rotation Estimation"
        },
        {
          "paperId": "91a4cbae6553e975ddc3b2f6850ed725ff475307",
          "title": "SwinTrack: A Simple and Strong Baseline for Transformer Tracking"
        },
        {
          "paperId": "63d70dba02c34e465f36fd8b123390efe7aa67e0",
          "title": "Can Vision Transformers Perform Convolution?"
        },
        {
          "paperId": "2945348e0843b8a414cd54fe2588a45430249355",
          "title": "Position-Augmented Transformers with Entity-Aligned Mesh for TextVQA"
        },
        {
          "paperId": "c501d6f1eecf3b0fdae7d428e1829bb6607a6a37",
          "title": "Relative Molecule Self-Attention Transformer"
        },
        {
          "paperId": "2b63f69c6eff99c06d0ff5d7fd3aafce1d3b6bdc",
          "title": "Learning to Iteratively Solve Routing Problems with Dual-Aspect Collaborative Transformer"
        },
        {
          "paperId": "232acef2483a26fe95c70b619f88fa0b82c1a105",
          "title": "Multiplicative Position-aware Transformer Models for Language Understanding"
        },
        {
          "paperId": "33c831326bb47b2ba2031fd7213b6918d23eb01e",
          "title": "The Impact of Positional Encodings on Multilingual Compression"
        },
        {
          "paperId": "0f2199296f01694ee46b6059879260fb80a84fa6",
          "title": "Teaching Autoregressive Language Models Complex Tasks By Demonstration"
        },
        {
          "paperId": "51b5db5c679be0ce9a39a2ee21def42bca165efe",
          "title": "Shatter: An Efficient Transformer Encoder with Single-Headed Self-Attention and Relative Sequence Partitioning"
        },
        {
          "paperId": "3c8146b0c77af1e3177c6497fcac87046578f81b",
          "title": "Conditional DETR for Fast Training Convergence"
        },
        {
          "paperId": "79678d2f10bddf14b2aedf3427f8a4c39908931f",
          "title": "Variable-Length Music Score Infilling via XLNet and Musically Specialized Positional Encoding"
        },
        {
          "paperId": "d3f51870f4da5dd9c2a08a55cfa8a380b8d49208",
          "title": "Lighter and Better: Low-Rank Decomposed Self-Attention Networks for Next-Item Recommendation"
        },
        {
          "paperId": "fbac64d617914ab8dac0682639fbd6012faed771",
          "title": "Rethinking Positional Encoding"
        },
        {
          "paperId": "c6dad3f9c9f29602b8c89585c23c73377ef00601",
          "title": "Bootstrap Representation Learning for Segmentation on Medical Volumes and Sequences"
        },
        {
          "paperId": "0d508600d77d8a7e6a655cdb6d139779732f649f",
          "title": "Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding"
        },
        {
          "paperId": "600a4066e6264a9f604ee03d49b246c8a17da645",
          "title": "Do Large Scale Molecular Language Representations Capture Important Structural Information?"
        },
        {
          "paperId": "1dbb523a6555d6e0c5727620e2b57daaa5b79dc0",
          "title": "Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models"
        },
        {
          "paperId": "47ae807cd511b35e78a2cd4e198283dea6dafd41",
          "title": "Do Transformers Really Perform Bad for Graph Representation?"
        },
        {
          "paperId": "7509c66a666e2e3f14bc8676b969b945ee6e136f",
          "title": "CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings"
        },
        {
          "paperId": "d8e7bad2681ce70277c900c77a22181d4b03d705",
          "title": "The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models"
        },
        {
          "paperId": "957424a1f3319a2aab1a91539a00e712477b4b4a",
          "title": "Self-Attention Networks Can Process Bounded Hierarchical Languages"
        },
        {
          "paperId": "08ffdec40291a2ccb5f8a6cc048b01247fb34b96",
          "title": "Relative Positional Encoding for Transformers with Linear Complexity"
        },
        {
          "paperId": "e563ba6db52146dd002e3f1527f92dde9a1d3cfa",
          "title": "MuseMorphose: Full-Song and Fine-Grained Music Style Transfer with Just One Transformer VAE"
        },
        {
          "paperId": "1c0752fb3e9ab5c9392f196225075422f26b5110",
          "title": "How could Neural Networks understand Programs?"
        },
        {
          "paperId": "f9a61d89209da81df0b232f788c64cbf18adaba5",
          "title": "Predicting functional effect of missense variants using graph attention neural networks"
        },
        {
          "paperId": "d76bf20d059359f9e05b342e56cfc7785af71bf4",
          "title": "LANA: Towards Personalized Deep Knowledge Tracing Through Distinguishable Interactive Sequences"
        },
        {
          "paperId": "66c10bf1f11bc1b2d92204d8f8391d087f6de1c4",
          "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding"
        },
        {
          "paperId": "36938a8ccb323cdd45c304d4461a29aee3ebc28a",
          "title": "SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization"
        },
        {
          "paperId": "a1ad15d2333cf9d9b55bbc97a3aacd244a8b9fdf",
          "title": "Demystifying the Better Performance of Position Encoding Variants for Transformer"
        },
        {
          "paperId": "db46b0de44c5113c47f0ec5392eb91d0726497bf",
          "title": "A Simple and Effective Positional Encoding for Transformers"
        },
        {
          "paperId": "111dbe14083359ab39886790632e7f1421732a8a",
          "title": "Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models"
        },
        {
          "paperId": "2cc3ab9fa41ba2804e301f7eae9598636e62422a",
          "title": "Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"
        },
        {
          "paperId": "dc3ee859157d163c7dd0ba497d8736a802fe59a0",
          "title": "Less is More: Pre-training a Strong Siamese Encoder Using a Weak Decoder"
        },
        {
          "paperId": "1e9ff5f2e9aa3e6c01bc89c81ba16442f1b5938d",
          "title": "Less is More: Pre-train a Strong Text Encoder for Dense Retrieval Using a Weak Decoder"
        },
        {
          "paperId": "19537be34dbadbcaa4fffcf028a8ada5095b1b5c",
          "title": "COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining"
        },
        {
          "paperId": "594db112d65891fbaf45b27f17d2f9de88ddcd82",
          "title": "Revisiting Language Encoding in Learning Multilingual Representations"
        },
        {
          "paperId": "f5a2ce064ea0efc3d7f26acd91041824c3254cd8",
          "title": "NRTSI: Non-Recurrent Time Series Imputation for Irregularly-sampled Data"
        },
        {
          "paperId": "eb0931c39904a40c6cb4aa35c9b21d5e3b7dc856",
          "title": "Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs"
        },
        {
          "paperId": "0822f8d7e6a72a65e65f147d3a8d8fccd485da40",
          "title": "Shortformer: Better Language Modeling using Shorter Inputs"
        },
        {
          "paperId": "829580d6fc73fa601c4982e2b1b6832f2796270b",
          "title": "Positional Artefacts Propagate Through Masked Language Model Embeddings"
        },
        {
          "paperId": "09c896e30d1021b1284b68ed65b93b593f2c3f4f",
          "title": "ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding"
        },
        {
          "paperId": "c2d50f17ea6769f6f5663ccac37a9627a0543184",
          "title": "Mathematical Reasoning in General Artificial Intelligence Workshop , ICLR 2021"
        },
        {
          "paperId": "c687acfeaafb86ab045f9fcbb6e2ff691827a0c2",
          "title": "On Position Embeddings in BERT O N P OSITION E MBEDDINGS IN BERT"
        },
        {
          "paperId": "2a405483796dfedf5d95483aa8880c57626e0e9f",
          "title": "Integrating Tree Path in Transformer for Code Representation"
        },
        {
          "paperId": "8d6b1929b92211ad0eb3e14b2c2b41789ccf053a",
          "title": "Explore Better Relative Position Embeddings from Encoding Perspective for Transformer Models"
        },
        {
          "paperId": "5be217c678916543884c354654263f27c0a6bd9f",
          "title": "Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder"
        },
        {
          "paperId": "79a2ac8ec0c4d3bfc1762d330e3446af5f8f1922",
          "title": "S2T: Learning Semantic-Spatial-Temporal Representation for Robust Active Object Tracking"
        },
        {
          "paperId": "dc35daba3fb34b2e6a5b12530badb7b799262bbf",
          "title": "On Position Embeddings in BERT"
        },
        {
          "paperId": "52d01d9f71caf0d021fccb75b75b7d3dfc7460f5",
          "title": "On Scalar Embedding of Relative Positions in Attention Models"
        },
        {
          "paperId": "976a609cf540d1ded373b872d34779f7164d840a",
          "title": "Rethinking the Design Principles of Robust Vision Transformer"
        },
        {
          "paperId": "5d76c2591334f56dc9155568f793b013df0f6613",
          "title": "Focusing More on Conflicts with Mis-Predictions Helps Language Pre-Training"
        },
        {
          "paperId": "eb2fc03b8865b8e1b4cb933d917ea269ebe14584",
          "title": "Learning from Mistakes: Using Mis-predictions as Harm Alerts in Language Pre-Training"
        },
        {
          "paperId": "ac4ae352e2434d4a71c6a79bf5f93df5f600b058",
          "title": "Increasing Learning Efficiency of Self-Attention Networks through Direct Position Interactions, Learnable Temperature, and Convoluted Attention"
        },
        {
          "paperId": "0971ad49ed7f063375d7a21bdfc2fab347d68ba8",
          "title": "Transformer-based Arabic Dialect Identification"
        },
        {
          "paperId": "6482dbb26e6441c0cfdaa43c52b868751d16d10b",
          "title": "Contextual BERT: Conditioning the Language Model Using a Global State"
        }
      ],
      "references": [
        {
          "paperId": "05f5f8b2065a520846d89771ebaea2bb1534e9c6",
          "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
        },
        {
          "paperId": "756810258e3419af76aff38c895c20343b0602d0",
          "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"
        },
        {
          "paperId": "e8984c6e6c24aab26c332728a5fff616dfb3adbb",
          "title": "Learning to Encode Position for Transformer with Continuous Dynamical Model"
        },
        {
          "paperId": "f64e1d6bc13aae99aab5449fc9ae742a9ba7761e",
          "title": "UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"
        },
        {
          "paperId": "748629cb0b8e5a5708e1c6605f71b36eb525a3ce",
          "title": "On Layer Normalization in the Transformer Architecture"
        },
        {
          "paperId": "72aa8d7f6f3a36d5e208cd1f41e90613ed6aee75",
          "title": "Encoding word order in complex embeddings"
        },
        {
          "paperId": "3cfb319689f06bf04c2e28399361f414ca32c4b3",
          "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
          "paperId": "37a23c43ddf09ea97b82b38e2827a2229cfae545",
          "title": "Novel positional encodings to enable tree-based transformers"
        },
        {
          "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
          "paperId": "e0c6abdbdecf04ffac65c440da77fb9d66bb474c",
          "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
        },
        {
          "paperId": "95a251513853c6032bdecebd4b74e15795662986",
          "title": "What Does BERT Look at? An Analysis of BERT’s Attention"
        },
        {
          "paperId": "5a3749929bf5fb8b1f98a7b2a43c3b957bcf6c88",
          "title": "Efficient Training of BERT by Progressively Stacking"
        },
        {
          "paperId": "faadd7d081c8d67e8c2567e8a5579e46cd6b2280",
          "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling"
        },
        {
          "paperId": "c4744a7c2bb298e4a52289a1e085c71cc3d37bc6",
          "title": "Transformer-XL: Attentive Language Models beyond a Fixed-Length Context"
        },
        {
          "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
          "paperId": "9405cc0d6169988371b2755e573cc28650d14dfe",
          "title": "Language Models are Unsupervised Multitask Learners"
        },
        {
          "paperId": "93b8da28d006415866bf48f9a6e06b5242129195",
          "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
        },
        {
          "paperId": "c8efcc854d97dfc2a42b83316a2109f9d166e43f",
          "title": "Self-Attention with Relative Position Representations"
        },
        {
          "paperId": "2e10560579f2bdeae0143141f26bd9f0a195b4b7",
          "title": "Mixed Precision Training"
        },
        {
          "paperId": "b36a5bb1707bb9c70025294b3a310138aae8327a",
          "title": "Automatic differentiation in PyTorch"
        },
        {
          "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
          "title": "Attention is All you Need"
        },
        {
          "paperId": "5feb32a73dd1bd9e13f84a7b3344497a5545106b",
          "title": "FastText.zip: Compressing text classification models"
        },
        {
          "paperId": "97fb4e3d45bb098e27e0071448b6152217bd35a5",
          "title": "Layer Normalization"
        },
        {
          "paperId": "1af68821518f03568f913ab03fc02080247a27ff",
          "title": "Neural Machine Translation of Rare Words with Subword Units"
        },
        {
          "paperId": "0e6824e137847be0599bb0032e37042ed2ef5045",
          "title": "Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"
        },
        {
          "paperId": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
          "title": "Adam: A Method for Stochastic Optimization"
        },
        {
          "paperId": "f37e1b62a767a307c046404ca96bc140b3e68cb5",
          "title": "GloVe: Global Vectors for Word Representation"
        },
        {
          "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
          "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
          "paperId": "87f40e6f3022adbc1f1905e3e506abad05a9964f",
          "title": "Distributed Representations of Words and Phrases and their Compositionality"
        },
        {
          "paperId": "4ee2eab4c298c1824a9fb8799ad8eed21be38d21",
          "title": "Moses: Open Source Toolkit for Statistical Machine Translation"
        },
        {
          "paperId": "99e8d34817ae10d7304521e89c5fbf908b9d856b",
          "title": "Open Source Toolkit for Statistical Machine Translation: Factored Translation Models and Lattice Decoding"
        },
        {
          "paperId": "79d1b330f0ef51f63ecb9b291dd5a05de5a858c0",
          "title": "Toeplitz and Circulant Matrices: A Review"
        },
        {
          "paperId": "8f424d35b3e5af10b2e492f073b3347f1f5176f8",
          "title": "Quantitative Linguistik / Quantitative Linguistics - Ein internationales Handbuch / An International Handbook"
        },
        {
          "paperId": "0c2d8f329f6809a90168dbfea2f11c7b0c3abc06",
          "title": "Speech and Language: Advances in Basic Research and Practice"
        }
      ],
      "summary": "This work investigates the problems in the previous formulations and proposes a new positional encoding method for BERT called Transformer with Untied Positional Encoding (TUPE), which can achieve a higher score than baselines while only using 30% pre-training computational costs.",
      "title": "Rethinking Positional Encoding in Language Pre-training",
      "venue": "ICLR",
      "year": 2020,
      "authors": "Guolin Ke,Di He,Tie-Yan Liu"
    },
    {
      "id": "51c62d63c6204deecb24a1d3f9ea8e0a42d23817",
      "url": "https://www.semanticscholar.org/paper/51c62d63c6204deecb24a1d3f9ea8e0a42d23817",
      "citationCount": 7,
      "influentialCitationCount": 0,
      "referenceCount": 31,
      "citations": [
        {
          "paperId": "e468039998895418ec78fc5f54e368534430d19a",
          "title": "A Solver-Free Framework for Scalable Learning in Neural ILP Architectures"
        },
        {
          "paperId": "6aaddf297eed5a61d1beace6e1a26a6919be57e7",
          "title": "Unsupervised Learning for Combinatorial Optimization with Principled Objective Relaxation"
        },
        {
          "paperId": "b42786bf595fcb0417c9e3947683e85a6924cd7f",
          "title": "Conditional set generation using Seq2seq models"
        },
        {
          "paperId": "5cbe278b65a81602a864184bbca37de91448a5f5",
          "title": "Competition-Level Code Generation with AlphaCode"
        },
        {
          "paperId": "6294b2966523a264e99cf90dffaf31e40874c6cb",
          "title": "Neural Models for Output-Space Invariance in Combinatorial Problems"
        },
        {
          "paperId": "bea00213becb832b2dd3ec6692d14145db168e8d",
          "title": "Unsupervised Learning for Combinatorial Optimization with Principled Objective Design"
        },
        {
          "paperId": "0e282a0725ff485bd97a344668fe667027d26c93",
          "title": "End-to-End Neuro-Symbolic Architecture for Image-to-Image Reasoning Tasks"
        }
      ],
      "references": [
        {
          "paperId": "3cb4239741e5b7496b60ee7af24c08b4341af0a2",
          "title": "Structured Prediction with Partial Labelling through the Infimum Loss"
        },
        {
          "paperId": "cb4571fa905abb70868d0bb9d4681f0a612c2d0f",
          "title": "Differentiable Reasoning on Large Knowledge Bases and Natural Language"
        },
        {
          "paperId": "918fea871683bb51f7ca97540c767b067ef9e3c1",
          "title": "Partial Label Learning via Label Enhancement"
        },
        {
          "paperId": "d3850595d3ae7c73e9488054c9b437f75511b569",
          "title": "SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver"
        },
        {
          "paperId": "3a6447361b20c249f5306ae17dee43f645430e31",
          "title": "Neural Logic Machines"
        },
        {
          "paperId": "0b86e5561f0de00047b96a1bb42a3e6379ed58c2",
          "title": "Partial Label Learning with Self-Guided Retraining"
        },
        {
          "paperId": "fd4ae71916cf400bfd1490f275e91b154eb69160",
          "title": "Relational recurrent neural networks"
        },
        {
          "paperId": "bbe56733113671455a9fee900bfca1a6d7f247bd",
          "title": "Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning"
        },
        {
          "paperId": "b3dae9529f3caeeec9cc6872e94aa690418acb22",
          "title": "Reinforcement Learning for Relation Classification From Noisy Data"
        },
        {
          "paperId": "de3b9eb697feed3d097e3f671afe395f48c1ab76",
          "title": "Stochastic Video Generation with a Learned Prior"
        },
        {
          "paperId": "dea6aeb514b1969ab879c793d46a0d2eceaa2cbf",
          "title": "Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis"
        },
        {
          "paperId": "34273979fd2a62fd7b49ee6d14a925864ff94e74",
          "title": "Recurrent Relational Networks"
        },
        {
          "paperId": "4df7bbe3ca7806f39a490c99f17867a0ac299bc3",
          "title": "Learning Explanatory Rules from Noisy Data"
        },
        {
          "paperId": "032274e57f7d8b456bd255fe76b909b2c1d7458e",
          "title": "A Deep Reinforced Model for Abstractive Summarization"
        },
        {
          "paperId": null,
          "title": "2018] 11 as the prediction network for this task. RRN uses a message passing based"
        },
        {
          "paperId": null,
          "title": "Can convolutional neural networks crack sudoku puzzles?"
        },
        {
          "paperId": "bfdb693df3a04fa9645233c444ccd8ec16c6c477",
          "title": "Prediction Under Uncertainty with Error-Encoding Networks"
        },
        {
          "paperId": "3ff0af64279929a952ee340e645256b7e0580f65",
          "title": "RobustFill: Neural Program Learning under Noisy I/O"
        },
        {
          "paperId": "1bc49abe5145055f1fa259bd4e700b1eb6b7f08d",
          "title": "SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents"
        },
        {
          "paperId": "62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e",
          "title": "Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge"
        },
        {
          "paperId": "bf55591e09b58ea9ce8d66110d6d3000ee804bdd",
          "title": "Image Captioning with Semantic Attention"
        },
        {
          "paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
          "title": "Neural Machine Translation by Jointly Learning to Align and Translate"
        },
        {
          "paperId": "822f1ed9a76a57cc19d8fda7745365b97130b97a",
          "title": "Injecting Logical Background Knowledge into Embeddings for Relation Extraction"
        },
        {
          "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
          "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
          "paperId": "6694e7012d6ebc2b19a0e8c90c81b0bde21bc961",
          "title": "There Is No 16-Clue Sudoku: Solving the Sudoku Minimum Number of Clues Problem via Hitting Set Enumeration"
        },
        { "paperId": null, "title": "Minimum sudoku" },
        {
          "paperId": "442f09ddb5bb7ba4e824c0795e37cad754967208",
          "title": "Learning from Partial Labels"
        },
        {
          "paperId": "a6ccfe1ac31444fb5a0d32b58182e0fb1b17c0e4",
          "title": "Multi-Label Classification: An Overview"
        },
        {
          "paperId": "cf376cd3d516e68dfbec2f08e552aa97d88e975b",
          "title": "Zchaff2004: An Efficient SAT Solver"
        },
        {
          "paperId": "89c3aed3e1219985c4a832f09adcd1df7096bf60",
          "title": "Learning with Multiple Labels"
        },
        {
          "paperId": "754b96e00671c74de0add9df1ef60dcf21160483",
          "title": "Local search strategies for satisfiability testing"
        }
      ],
      "summary": "This paper formally defines the task of learning one-of-many solutions for combinatorial problems in structured output spaces, which is applicable for solving several problems of interest such as N-Queens, and Sudoku, and proposes an RL based approach to jointly train the selection module with the prediction network.",
      "title": "Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces",
      "venue": "ICLR",
      "year": 2020,
      "authors": "Yatin Nandwani,Deepanshu Jindal,Mausam,Parag Singla"
    },
    {
      "id": "0c7f81e26ae77b4be5e5e6ab96641effe219ab1c",
      "url": "https://www.semanticscholar.org/paper/0c7f81e26ae77b4be5e5e6ab96641effe219ab1c",
      "citationCount": 7,
      "influentialCitationCount": 0,
      "referenceCount": 10,
      "citations": [
        {
          "paperId": "03d73f3073ac0d73d60d5567fc1ed558367c8279",
          "title": "Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks"
        },
        {
          "paperId": "8fe49d025e8b83816f7169daa74becaac6184f9e",
          "title": "Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks"
        },
        {
          "paperId": "37588705a2af7d5b24d901dd33ade1ff293aabdd",
          "title": "Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"
        },
        {
          "paperId": "28a5a53dafacebad8a7c47773079caeffb9a5baa",
          "title": "Representing Numbers in NLP: a Survey and a Vision"
        },
        {
          "paperId": "2cc3ab9fa41ba2804e301f7eae9598636e62422a",
          "title": "Investigating the Limitations of the Transformers with Simple Arithmetic Tasks"
        },
        {
          "paperId": "c2d50f17ea6769f6f5663ccac37a9627a0543184",
          "title": "Mathematical Reasoning in General Artificial Intelligence Workshop , ICLR 2021"
        },
        {
          "paperId": "31699d03a49e38295298f1b1a53185644abba12e",
          "title": "Numeracy enhances the Literacy of Language Models"
        }
      ],
      "references": [
        {
          "paperId": "a0e49f65b6847437f262c59d0d399255101d0b75",
          "title": "What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models"
        },
        {
          "paperId": "1fa9ed2bea208511ae698a967875e943049f16b6",
          "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"
        },
        {
          "paperId": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
          "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
        },
        {
          "paperId": "0427110f0e79f41e69a8eb00a3ec8868bac26a4f",
          "title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings"
        },
        {
          "paperId": "ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc",
          "title": "Cross-lingual Language Model Pretraining"
        },
        {
          "paperId": "668f42a4d4094f0a66d402a16087e14269b31a1f",
          "title": "Analysis Methods in Neural Language Processing: A Survey"
        },
        {
          "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
          "paperId": "53d43ccc593bf44e9aa52e3971df1b9dd396e30d",
          "title": "Probing for semantic evidence of composition by means of simple classification tasks"
        },
        {
          "paperId": "70ba4f5adfbb88e88b0dbfb60f3c9dfb97bf9940",
          "title": "Preschool Origins of Cross-National Differences in Mathematical Competence: The Role of Number-Naming Systems"
        },
        {
          "paperId": null,
          "title": ", Morgan Funtow - icz , and Jamie Brew . 2019 . Huggingface ’ s transformers : State - ofthe - art natural language process"
        }
      ],
      "summary": "Novel multilingual probing tasks tested on DistilBERT, XLM, and BERT find evidence that the information encoded in these pretrained models’ embeddings is sufficient for grammaticality judgments but generally not for value comparisons.",
      "title": "Probing for Multilingual Numerical Understanding in Transformer-Based Language Models",
      "venue": "BLACKBOXNLP",
      "year": 2020,
      "authors": "Devin J. Johnson,Denise Mak,Drew Barker,Lexi Loessberg-Zahl"
    },
    {
      "id": "a20712b1b9779ee43ce143a19b3f67f0cacbbf57",
      "url": "https://www.semanticscholar.org/paper/a20712b1b9779ee43ce143a19b3f67f0cacbbf57",
      "citationCount": 4,
      "influentialCitationCount": 0,
      "referenceCount": 58,
      "citations": [
        {
          "paperId": "f49feb36a148fa0c5be0f2a2353583367e7cf4bd",
          "title": "QuTE: Answering Quantity Queries from Web Tables"
        },
        {
          "paperId": "d331de3b6bebb0f9af1fddf1b730ec057a7026d4",
          "title": "Relational World Knowledge Representation in Contextual Language Models: A Review"
        },
        {
          "paperId": "73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19",
          "title": "Measuring and Improving Consistency in Pretrained Language Models"
        },
        {
          "paperId": "d51bec1426494f23de57b2c4184c7e26583eda53",
          "title": "Relational Pretrained Transformers towards Democratizing Data Preparation [Vision]"
        }
      ],
      "references": [
        {
          "paperId": "0b09448f7543453cc066416f547292dc1e4471f6",
          "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks"
        },
        {
          "paperId": "bde0c85ed3d61de2a8874ddad70497b3d68bc8ad",
          "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"
        },
        {
          "paperId": "2cbb8de53759e75411bc528518947a3094fbce3a",
          "title": "Billion-Scale Similarity Search with GPUs"
        },
        {
          "paperId": "0544af8d6b6b6e9ce14be9c6425e520a638be380",
          "title": "Photon: A Robust Cross-Domain Text-to-SQL System"
        },
        {
          "paperId": "c0b79e6a5fd88ef13aa4780df5aae0aaa6b2be87",
          "title": "Linformer: Self-Attention with Linear Complexity"
        },
        {
          "paperId": "6b85b63579a916f705a8e10a49bd8d849d91b1fc",
          "title": "Language Models are Few-Shot Learners"
        },
        {
          "paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22",
          "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
        },
        {
          "paperId": "79cd9f77e5258f62c0e15d11534aea6393ef73fe",
          "title": "Dense Passage Retrieval for Open-Domain Question Answering"
        },
        {
          "paperId": "512f34906ddaefe885af2e5eec9b2b3b50ffd377",
          "title": "Deep entity matching with pre-trained language models"
        },
        {
          "paperId": "15ad2b27c5248e7d1db5456794ca1ca8a8198f5d",
          "title": "Transformers as Soft Reasoners over Language"
        },
        {
          "paperId": "832fff14d2ed50eb7969c4c4b976c35776548f56",
          "title": "REALM: Retrieval-Augmented Language Model Pre-Training"
        },
        {
          "paperId": "71c908529b12ef6ee8d735127a63d48b1fc5c43c",
          "title": "Break It Down: A Question Understanding Benchmark"
        },
        {
          "paperId": "cb4571fa905abb70868d0bb9d4681f0a612c2d0f",
          "title": "Differentiable Reasoning on Large Knowledge Bases and Natural Language"
        },
        {
          "paperId": "521b4e26df0f1cf5763dece14cbb218df152dc59",
          "title": "Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering"
        },
        {
          "paperId": "3cfb319689f06bf04c2e28399361f414ca32c4b3",
          "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
          "paperId": "815a3d56401483b635cfad9468852cddb46350ee",
          "title": "Compositionality Decomposed: How do Neural Networks Generalise?"
        },
        {
          "paperId": "1fa9ed2bea208511ae698a967875e943049f16b6",
          "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing"
        },
        {
          "paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3",
          "title": "Language Models as Knowledge Bases?"
        },
        {
          "paperId": "52fa450740913a6cdcb4d9395b45e203f46cab79",
          "title": "Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"
        },
        {
          "paperId": "d3cacb4806886eb2fe59c90d4b6f822c24ff1822",
          "title": "Visualizing and Understanding the Effectiveness of BERT"
        },
        {
          "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
          "paperId": "b9372e972997c5056bb79c70526230baed2e372b",
          "title": "Multi-hop Reading Comprehension through Question Decomposition and Rescoring"
        },
        {
          "paperId": "e2587eddd57bc4ba286d91b27c185083f16f40ee",
          "title": "What do you learn from context? Probing for sentence structure in contextualized word representations"
        },
        {
          "paperId": "dda6fb309f62e2557a071522354d8c2c897a2805",
          "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"
        },
        {
          "paperId": "660d3472d9c3733dedcf911187b234f2b65561b5",
          "title": "BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning"
        },
        {
          "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
          "paperId": "cd832f7081ab7b83240140c4e5e58b4fb1f8e0e6",
          "title": "Interpretation of Natural Language Rules in Conversational Machine Reading"
        },
        {
          "paperId": "ac11062f1f368d97f4c826c317bf50dcc13fdb59",
          "title": "Dissecting Contextual Word Embeddings: Architecture and Representation"
        },
        {
          "paperId": "4d1c856275744c0284312a3a50efb6ca9dc4cd4c",
          "title": "Know What You Don’t Know: Unanswerable Questions for SQuAD"
        },
        {
          "paperId": "c0a29cb35c2965930566d6a407da043e18431eaa",
          "title": "Deep Learning for Entity Matching: A Design Space Exploration"
        },
        {
          "paperId": "c8725f13be7434b69738491c66b45c9225258253",
          "title": "The Web as a Knowledge-Base for Answering Complex Questions"
        },
        {
          "paperId": "b1d24e8e08435b7c52335485a0d635abf9bc604c",
          "title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification"
        },
        {
          "paperId": "f6eee02a2f4c74c2b543bf419f76cef60d5752f8",
          "title": "The Case for Learned Index Structures"
        },
        {
          "paperId": "7d5cf22c70484fe217936c66741fb73b2a278bde",
          "title": "Constructing Datasets for Multi-hop Reading Comprehension Across Documents"
        },
        {
          "paperId": "cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
          "title": "Improving Language Understanding by Generative Pre-Training"
        },
        {
          "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
          "title": "Attention is All you Need"
        },
        {
          "paperId": "5889e9afbcc3935867f9ae16fe46c71b9f2b071f",
          "title": "End-to-end Differentiable Proving"
        },
        {
          "paperId": "4c41104e871bccbd56494350a71d77a7f1da5bb0",
          "title": "Understanding Neural Networks through Representation Erasure"
        },
        {
          "paperId": "05dd7254b632376973f3a1b4d39485da17814df5",
          "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
        },
        {
          "paperId": "5091316bb1c6db6c6a813f4391911a5c311fdfe0",
          "title": "\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier"
        },
        {
          "paperId": "21c99706bb26e9012bfb4d8d48009a3d45af59b2",
          "title": "Neural Module Networks"
        },
        {
          "paperId": "abb33d75dc297993fcc3fb75e0f4498f413eb4f6",
          "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks"
        },
        {
          "paperId": "b41e95c8c97846d5ca4c11ef79d7814499cc9663",
          "title": "Compositional Semantic Parsing on Semi-Structured Tables"
        },
        {
          "paperId": "97ad70a9fa3f99adf18030e5e38ebe3d90daa2db",
          "title": "VQA: Visual Question Answering"
        },
        {
          "paperId": "4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e",
          "title": "End-To-End Memory Networks"
        },
        {
          "paperId": "dab7e605237ad4f4fe56dcba2861b8f0a57112be",
          "title": "Wikidata: a free collaborative knowledgebase"
        },
        {
          "paperId": "cb9588146a4335b96e3bd8c51e4b4e02fc10b7cd",
          "title": "Constructing an Interactive Natural Language Interface for Relational Databases"
        },
        { "paperId": null, "title": "Neural turing machines. CoRR, abs/1410" },
        {
          "paperId": "e925291a9358ed85b9ea1d2a767f1e657ea40ac8",
          "title": "DB-IR integration using tight-coupling in the Odysseus DBMS"
        },
        {
          "paperId": "564257469fa44cdb57e4272f85253efb9acfd69d",
          "title": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text"
        },
        {
          "paperId": "b29447ba499507a259ae9d8f685d60cc1597d7d3",
          "title": "Semantic Parsing on Freebase from Question-Answer Pairs"
        },
        {
          "paperId": "672b3d3976aa106c72b845db7734cd8f194cc261",
          "title": "ODYS: an approach to building a massively-parallel search engine using a DB-IR tightly-integrated parallel DBMS for higher-level functionality"
        },
        {
          "paperId": "6304131a1fb60b9883f1469072eeda8a0bd441f5",
          "title": "DB&IR: both sides now"
        },
        {
          "paperId": "52e40e9466178129a15e6584f31d89d31800308f",
          "title": "Report on the DB/IR panel at SIGMOD 2005"
        },
        {
          "paperId": "03b7e51c52084ac1db5118342a00b5fbcfc587aa",
          "title": "Q-learning"
        },
        {
          "paperId": "8d43a96e12a07b53014997f050005e09a62b7cef",
          "title": "Crossing the Structure Chasm"
        },
        {
          "paperId": "c5f96d281b436ad4f1e0a3c37cbbcda4c5b17eea",
          "title": "Models for Integrated Information Retrieval and Database Systems"
        },
        {
          "paperId": "1df699c7da1b948bffd449ac44809017ee4206e0",
          "title": "Natural language interfaces to databases - an introduction"
        }
      ],
      "summary": "This paper describes NeuralDB, a database system with no pre-defined schema, in which updates and queries are given in natural language, and describes an algorithm that learns how to create the appropriate sets of facts to be fed into each of the Neural SPJ operators.",
      "title": "Neural Databases",
      "venue": "ArXiv",
      "year": 2020,
      "authors": "James Thorne,Majid Yazdani,Marzieh Saeidi,F. Silvestri,Sebastian Riedel,A. Halevy"
    },
    {
      "id": "2cc3ab9fa41ba2804e301f7eae9598636e62422a",
      "url": "https://www.semanticscholar.org/paper/2cc3ab9fa41ba2804e301f7eae9598636e62422a",
      "citationCount": 22,
      "influentialCitationCount": 5,
      "referenceCount": 46,
      "citations": [
        {
          "paperId": "4c2534f9b03ac2f3810c07abc398a11bcf47258e",
          "title": "Transformers Learn Shortcuts to Automata"
        },
        {
          "paperId": "8e5ca53f7633450e2756950c234c5f9d04b5c9f2",
          "title": "One Clinician Is All You Need–Cardiac Magnetic Resonance Imaging Measurement Extraction: Deep Learning Algorithm Development"
        },
        {
          "paperId": "01b2f7601ab3df0d2982a204e2fb309f6622646f",
          "title": "Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models"
        },
        {
          "paperId": "2a7ae3e98357569c41424dacd60c62d3df78a0db",
          "title": "Limitations of Language Models in Arithmetic and Symbolic Induction"
        },
        {
          "paperId": "4f451ba06c4c9effd6c4ac0bae222495501a6200",
          "title": "Innovations in Neural Data-to-text Generation"
        },
        {
          "paperId": "6d72226ccdf1a883c4b866f14d3d1f06db871ee1",
          "title": "SALSA: Attacking Lattice Cryptography with Transformers"
        },
        {
          "paperId": "1fd8e0fc48300d765ac35b8e6ed72ea1d3061f8e",
          "title": "LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks"
        },
        {
          "paperId": "d2018d1b0f69ca6805aa18a22be95cab9b5c44c7",
          "title": "On Neural Architecture Inductive Biases for Relational Tasks"
        },
        {
          "paperId": "4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80",
          "title": "Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence"
        },
        {
          "paperId": "edd80013e8b9fcba9231cf99884f32e5236ff329",
          "title": "AA-TransUNet: Attention Augmented TransUNet For Nowcasting Tasks"
        },
        {
          "paperId": "b8b813111c411ae61881ab9cd25707d9de6444ec",
          "title": "Compositional Attention: Disentangling Search and Retrieval"
        },
        {
          "paperId": "aead4418733b998792deb9cbf198a834449e00d2",
          "title": "Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics"
        },
        {
          "paperId": "9ca329408813d209b1dcb36936f7f9cba82506bd",
          "title": "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"
        },
        {
          "paperId": "a5808ccc50f77083bd3be926fb2af05cf34563ff",
          "title": "Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition"
        },
        {
          "paperId": "c78f267dfcca001384cb0d4e3b8de0173adc162d",
          "title": "Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF"
        },
        {
          "paperId": "58d5e4cbb5f3a944bae0bcc2bff93f06696f8196",
          "title": "JRIRD at the NTCIR-16 FinNum-3 Task: Investigating the Effect of Numerical Representations in Manager’s Claim Detection"
        },
        {
          "paperId": "7a8e40254435966e0568df52929d9779add28c7d",
          "title": "Linear algebra with transformers"
        },
        {
          "paperId": "37588705a2af7d5b24d901dd33ade1ff293aabdd",
          "title": "Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model"
        },
        {
          "paperId": "0f2199296f01694ee46b6059879260fb80a84fa6",
          "title": "Teaching Autoregressive Language Models Complex Tasks By Demonstration"
        },
        {
          "paperId": "a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99",
          "title": "Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning"
        },
        {
          "paperId": "28a5a53dafacebad8a7c47773079caeffb9a5baa",
          "title": "Representing Numbers in NLP: a Survey and a Vision"
        },
        {
          "paperId": "b59947541d2ac4211c4b17554b2e16c260299bed",
          "title": "Have You Seen That Number? Investigating Extrapolation in Question Answering Models"
        }
      ],
      "references": [
        {
          "paperId": "4383a975c09b72ba2f1a77cd779bb6965dbfb2fb",
          "title": "Scaling Laws for Transfer"
        },
        {
          "paperId": "8256f48f759cf85044db251cc512f965834945b3",
          "title": "Rethinking Positional Encoding in Language Pre-training"
        },
        {
          "paperId": "05f5f8b2065a520846d89771ebaea2bb1534e9c6",
          "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
        },
        {
          "paperId": "02791e807dc9a91f854a1f3d5f6005122a546109",
          "title": "Object-based attention for spatio-temporal reasoning: Outperforming neuro-symbolic models with flexible distributed architectures"
        },
        {
          "paperId": "3efbcfeeb0ea1051a71101d3318da4411081f0b8",
          "title": "Scaling Laws for Autoregressive Generative Modeling"
        },
        {
          "paperId": "227fe850a72fab24998c7e08d75db214715dc74e",
          "title": "The EOS Decision and Length Extrapolation"
        },
        {
          "paperId": "45cdda3f96ae32927c7b073ebbedf46f5e0fbdc5",
          "title": "Enhancing the Numeracy of Word Embeddings: A Linear Algebraic Perspective"
        },
        {
          "paperId": "0c7f81e26ae77b4be5e5e6ab96641effe219ab1c",
          "title": "Probing for Multilingual Numerical Understanding in Transformer-Based Language Models"
        },
        {
          "paperId": "3118c0633cb2d0f91b5ef88840343e47c3ca5623",
          "title": "Do Language Embeddings capture Scales?"
        },
        {
          "paperId": "84476fdf6ead3553f4493dff8e02308439d6222b",
          "title": "Improve Transformer Models with Better Relative Position Embeddings"
        },
        {
          "paperId": "5fe0a4af3bd1479d5e39fbda2215c86bce54722b",
          "title": "Generative Language Modeling for Automated Theorem Proving"
        },
        {
          "paperId": "6b85b63579a916f705a8e10a49bd8d849d91b1fc",
          "title": "Language Models are Few-Shot Learners"
        },
        {
          "paperId": "72e6b14c081ad3e6a1c295b60e5c834837e6b304",
          "title": "Towards Question Format Independent Numerical Reasoning: A Set of Prerequisite Tasks"
        },
        {
          "paperId": "016760dc4a05489ddf5dbb48aecbb49e214e1b71",
          "title": "Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"
        },
        {
          "paperId": "ac67ec5b985a30239926c3362fa45a9a03f017af",
          "title": "Learning to Generate Correct Numeric Values in News Headlines"
        },
        {
          "paperId": "3dd61d97827e3f380bf9304101149a3f865051fc",
          "title": "Injecting Numerical Reasoning Skills into Language Models"
        },
        {
          "paperId": "5e0cffc51e8b64a8f11326f955fa4b4f1803e3be",
          "title": "oLMpics-On What Language Model Pre-training Captures"
        },
        {
          "paperId": "d5fcad8a3b183642fcf609519a4dbbda9c3541ff",
          "title": "Learning Numeral Embeddings"
        },
        {
          "paperId": "72aa8d7f6f3a36d5e208cd1f41e90613ed6aee75",
          "title": "Encoding word order in complex embeddings"
        },
        {
          "paperId": "b39eed03d345f5c244eac12fd1315d26eba77d62",
          "title": "Deep Learning for Symbolic Mathematics"
        },
        {
          "paperId": "3cfb319689f06bf04c2e28399361f414ca32c4b3",
          "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
          "paperId": "5414be4432fcfe1e83932127ee2463d107e5b61c",
          "title": "A Sequence-to-sequence Approach for Numerical Slot-filling Dialog Systems"
        },
        {
          "paperId": "cd36b981e39edc5517d2a508e879b4d9bdc1b1c6",
          "title": "Tree-structured Decoding for Solving Math Word Problems"
        },
        {
          "paperId": "730043364aed106241ef18ab3e3b5e316802a254",
          "title": "NumNet: Machine Reading Comprehension with Numerical Reasoning"
        },
        {
          "paperId": "136396e988dbbdaca433096e931b22732c3d95dd",
          "title": "Text2Math: End-to-end Parsing Text into Math Expressions"
        },
        {
          "paperId": "d88f31a0091eee02c5a2aa2013914818cdef114e",
          "title": "Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving"
        },
        {
          "paperId": "0427110f0e79f41e69a8eb00a3ec8868bac26a4f",
          "title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings"
        },
        {
          "paperId": "52fa450740913a6cdcb4d9395b45e203f46cab79",
          "title": "Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension"
        },
        {
          "paperId": "5435f997d98754f68492334eeb87d027047e60cb",
          "title": "Quantity Tagger: A Latent-Variable Sequence Labeling Approach to Solving Addition-Subtraction Word Problems"
        },
        {
          "paperId": "865f5167c4353d2b120f0469ed1c298bc92794fa",
          "title": "Modeling Intra-Relation in Math Word Problems with Different Functional Multi-Head Attentions"
        },
        {
          "paperId": "a5dde0a71fb0465d33b95496c4bb64914b3d62d9",
          "title": "Exploring Numeracy in Word Embeddings"
        },
        {
          "paperId": "97906df07855b029b7aae7c2a1c6c5e8df1d531c",
          "title": "BERT Rediscovers the Classical NLP Pipeline"
        },
        {
          "paperId": "afed6dc6900d3b37e528b9086661bba583d60bf6",
          "title": "Analysing Mathematical Reasoning Abilities of Neural Models"
        },
        {
          "paperId": "dda6fb309f62e2557a071522354d8c2c897a2805",
          "title": "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"
        },
        {
          "paperId": "1eff94b44432a8e6af29288b8234494516579ad3",
          "title": "EQUATE: A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference"
        },
        {
          "paperId": "ac4dafdef1d2b685b7f28a11837414573d39ff4e",
          "title": "Universal Transformers"
        },
        {
          "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
          "title": "Decoupled Weight Decay Regularization"
        },
        {
          "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
          "paperId": "5fc548f3f3112de7eddad3744717dc2f9d22ca38",
          "title": "Neural Arithmetic Logic Units"
        },
        {
          "paperId": "3febb2bed8865945e7fddc99efd791887bb7e14f",
          "title": "Deep Contextualized Word Representations"
        },
        {
          "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
          "title": "Attention is All you Need"
        },
        {
          "paperId": "3058d4a4fa6ebaac30e0279c6b8c30e5f969d315",
          "title": "Extensions and Limitations of the Neural GPU"
        },
        {
          "paperId": "5e4eb58d5b47ac1c73f4cf189497170e75ae6237",
          "title": "Neural GPUs Learn Algorithms"
        },
        {
          "paperId": "5b791cd374c7109693aaddee2c12d659ae4e3ec0",
          "title": "Grid Long Short-Term Memory"
        },
        {
          "paperId": "d38e8631bba0720becdaf7b89f79d9f9dca45d82",
          "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"
        },
        {
          "paperId": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
          "title": "Learning internal representations by error propagation"
        }
      ],
      "summary": "It is found that how a number is represented in its surface form has a strong influence on the model’s accuracy, and this result bolsters evidence that subword tokenizers and positional encodings are components in current transformer designs that might need improvement.",
      "title": "Investigating the Limitations of the Transformers with Simple Arithmetic Tasks",
      "venue": "ArXiv",
      "year": 2021,
      "authors": "Rodrigo Nogueira,Zhiying Jiang,Jimmy J. Li"
    },
    {
      "id": "57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
      "url": "https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5",
      "citationCount": 56,
      "influentialCitationCount": 12,
      "referenceCount": 65,
      "citations": [
        {
          "paperId": "711d5e8ddbb840ad31a9ffa3d38590603ba69a92",
          "title": "Prompting GPT-3 To Be Reliable"
        },
        {
          "paperId": "3b209a45369921ab5f19759b31bd7dc5bb139c6b",
          "title": "Learning to Reason With Relational Abstractions"
        },
        {
          "paperId": "7cdf6ed502c5c294b84d357e31405a07b836b080",
          "title": "When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment"
        },
        {
          "paperId": "2fe6060ced80c1c245a718e6188b6516207bf0a8",
          "title": "Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions"
        },
        {
          "paperId": "53b3b15e8cbe2baf82cd0de3709e0a1e6f677415",
          "title": "Limits of an AI program for solving college math problems"
        },
        {
          "paperId": "465dfa563e7d5ddef794f90ad3b0b19cf46d91a7",
          "title": "Compute Like Humans: Interpretable Step-by-step Symbolic Computation with Deep Neural Network"
        },
        {
          "paperId": "0a9df881784009cbb8efcd037d82ae222440aade",
          "title": "An Interpretability Evaluation Benchmark for Pre-trained Language Models"
        },
        {
          "paperId": "e91c5ef42dc5d7a21d04175e17862a67aaedf5ae",
          "title": "Effect of pre-training scale on intra- and inter-domain, full and few-shot transfer learning for natural and X-Ray chest images"
        },
        {
          "paperId": "b5276ca7530d6442b5f5a75a1a0bc4c1095497e2",
          "title": "Exploring Length Generalization in Large Language Models"
        },
        {
          "paperId": "041edc8b14bdd0e5627377956fd0e6c6c011146a",
          "title": "Machine Learning Model Sizes and the Parameter Gap"
        },
        {
          "paperId": "6059e073b11b96af7566efddd1d4ee0e25046c54",
          "title": "Forecasting Future World Events with Neural Networks"
        },
        {
          "paperId": "ab0e3d3e4d42369de5933a3b4c237780b41c0d77",
          "title": "Solving Quantitative Reasoning Problems with Language Models"
        },
        {
          "paperId": "e6ecdbbceff06cc1e667e3261596fd0fa6b32c4b",
          "title": "How General-Purpose Is a Language Model? Usefulness and Safety with Human Prompters in the Wild"
        },
        {
          "paperId": "e10ed48cceca216d8ac43113c0562cf340dbdce3",
          "title": "Unveiling Transformers with LEGO: a synthetic reasoning task"
        },
        {
          "paperId": "0b7e9b6b588baa3f24fdde06feec26a067aa74bd",
          "title": "MultiHiertt: Numerical Reasoning over Multi Hierarchical Tabular and Textual Data"
        },
        {
          "paperId": "0efa0441da820b1905572666ba1974a06a9663fb",
          "title": "NaturalProver: Grounded Mathematical Proof Generation with Language Models"
        },
        {
          "paperId": "c28e95a06dfcf13fc65a1cac83722f53e34f12a5",
          "title": "Autoformalization with Large Language Models"
        },
        {
          "paperId": "354bf043179e3e9f05df73e3f04517e53c326d1f",
          "title": "TALM: Tool Augmented Language Models"
        },
        {
          "paperId": "c9f48406851954cb098911eccb4124ea5f966675",
          "title": "A Survey on Multi-hop Question Answering and Generation"
        },
        {
          "paperId": "e37018d3cfab9cfc29a7b78404e6c86ea18a907e",
          "title": "GPT-NeoX-20B: An Open-Source Autoregressive Language Model"
        },
        {
          "paperId": "39238a92de090c104936a4f78375b95600e42ce5",
          "title": "NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks"
        },
        {
          "paperId": "2e550122893e7c27f3006e14ad383a6be75fdd75",
          "title": "Capturing Failures of Large Language Models via Human Cognitive Biases"
        },
        {
          "paperId": "7b5aa186ca8abc585607c5ec91562e127a398601",
          "title": "Open-Ended Knowledge Tracing"
        },
        {
          "paperId": "04ff95e0edc3759fc5d18a1b929b3ccf79b032b2",
          "title": "Deconstructing Distributions: A Pointwise Framework of Learning"
        },
        {
          "paperId": "916a06a6d51aa93de27aac2f3e14faed08dd6706",
          "title": "Formal Mathematics Statement Curriculum Learning"
        },
        {
          "paperId": "5d0db797a45ce2453f821f7ded0b547d3fdab054",
          "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
        },
        {
          "paperId": "77d956cdab4508d569ae5741549b78e715fd0749",
          "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
        },
        {
          "paperId": "7ba98b00a224094c09676090f5d6d69498f5b299",
          "title": "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics"
        },
        {
          "paperId": "dc4da699b547a38e7f28ae6b39a779338b701131",
          "title": "Structural information in mathematical formulas for exercise difficulty prediction: a comparison of NLP representations"
        },
        {
          "paperId": "c8e727c4e2bbdbdfbc77610215e8c9e9b09ce63b",
          "title": "Transformer-Encoder and Decoder Models for Questions on Math"
        },
        {
          "paperId": "a5808ccc50f77083bd3be926fb2af05cf34563ff",
          "title": "Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition"
        },
        {
          "paperId": "9529ef8c65a148ab0542ff3b7e9b81d73ac3d8b2",
          "title": "Hybrid Tokenization and Datasets for Solving Mathematics and Science Problems Using Transformers"
        },
        {
          "paperId": "8eabf511ce56c56b99dd79ac3e9935624313eaac",
          "title": "Analyzing the Effects of Annotator Gender across NLP Tasks"
        },
        {
          "paperId": "cc8417aa578016203cb52efc63592bba64b08bb3",
          "title": "FinMath: Injecting a Tree-structured Solver for Question Answering over Financial Reports"
        },
        {
          "paperId": "15e767aa26a14455da95a2b2f11e3d59f2c250f6",
          "title": "Autoformalization for Neural Theorem Proving"
        },
        {
          "paperId": "c78f267dfcca001384cb0d4e3b8de0173adc162d",
          "title": "Applying Structural and Dense Semantic Matching for the ARQMath Lab 2022, CLEF"
        },
        {
          "paperId": "15789db234a9338e57aff9709868dcbd290727d3",
          "title": "Continual Pre-training of Language Models for Math Problem Understanding with Syntax-Aware Memory Network"
        },
        {
          "paperId": "739715750d8eacbf04f16e36770089afad358cfa",
          "title": "Towards More Robust Natural Language Understanding"
        },
        {
          "paperId": "f7987fa2aadc0b368c185dc4d2fdb1337a202c32",
          "title": "Solving Probability and Statistics Problems by Program Synthesis"
        },
        {
          "paperId": "f729e16ee6765e2fd76772cbf9d3dd17b4d25438",
          "title": "Towards Tractable Mathematical Reasoning: Challenges, Strategies, and Opportunities for Solving Math Word Problems"
        },
        {
          "paperId": "d6045d2ccc9c09ca1671348de86d07da6bc28eea",
          "title": "Training Verifiers to Solve Math Word Problems"
        },
        {
          "paperId": "3ceca3cc39cd90515ccd73afe6539cc953762140",
          "title": "How much coffee was consumed during EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI"
        },
        {
          "paperId": "320c1c6647a5b975c901347f71638c881888686b",
          "title": "NQuAD: 70,000+ Questions for Machine Comprehension of the Numerals in Text"
        },
        {
          "paperId": "21f455ca0bbf9a355611bc0593dd1cf8a8d32584",
          "title": "The Computational Complexity of Finding Arithmetic Expressions With and Without Parentheses"
        },
        {
          "paperId": "52647a2b0d19ef1943db2237a62521761015e44c",
          "title": "Pretrained Language Models are Symbolic Mathematics Solvers too!"
        },
        {
          "paperId": "0f2199296f01694ee46b6059879260fb80a84fa6",
          "title": "Teaching Autoregressive Language Models Complex Tasks By Demonstration"
        },
        {
          "paperId": "d2824febb0c0c1fbfedc35e1a4d0532017603753",
          "title": "Solving Machine Learning Problems"
        },
        {
          "paperId": "096cffc20d3bc89e8bc337607435a67e79d888d9",
          "title": "Effect of Pre-Training Scale on Intra- and Inter-Domain Full and Few-Shot Transfer Learning for Natural and Medical X-Ray Chest Images"
        },
        {
          "paperId": "1ccd031f28dccfb226f6c0c588c93a97a50bf95f",
          "title": "Measuring Coding Challenge Competence With APPS"
        },
        {
          "paperId": "4cc1fb128fa3abf6f90d567744767e8fd6315e1d",
          "title": "NaturalProofs: Mathematical Theorem Proving in Natural Language"
        },
        {
          "paperId": "28a5a53dafacebad8a7c47773079caeffb9a5baa",
          "title": "Representing Numbers in NLP: a Survey and a Vision"
        },
        {
          "paperId": "6a1b25f7a67395ad1e676027322913acbb0a0635",
          "title": "CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review"
        },
        {
          "paperId": "8424082e3bf4792462eb112d7ebcecf5b0dc3613",
          "title": "Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"
        },
        {
          "paperId": "4ff14a0580f3b36a9564ec18a51d2ce1d4eafebc",
          "title": "Learning Methods for Solving Astronomy Course Problems"
        },
        { "paperId": null, "title": "Overleaf Example" },
        {
          "paperId": "f1362000a1561924a3a07d7b9ab3d8cc3fd4e96d",
          "title": "Effect of large-scale pre-training on full and few-shot transfer learning for natural and medical images"
        }
      ],
      "references": [
        {
          "paperId": "a38e0f993e4805ba8a9beae4c275c91ffcec01df",
          "title": "Program Synthesis with Large Language Models"
        },
        {
          "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
          "title": "Evaluating Large Language Models Trained on Code"
        },
        {
          "paperId": "4383a975c09b72ba2f1a77cd779bb6965dbfb2fb",
          "title": "Scaling Laws for Transfer"
        },
        {
          "paperId": "58c74cec28f3416b9a1d308bb2d6519d21d53ab0",
          "title": "LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning"
        },
        {
          "paperId": "10bb7e2c54b947fa50e7bb65b0b5c700fe998044",
          "title": "Measuring Massive Multitask Language Understanding"
        },
        {
          "paperId": "45bb43cdc35324fea4350ed335c500d4a5fd6ef5",
          "title": "Mathematical Reasoning via Self-supervised Skip-tree Training"
        },
        {
          "paperId": "05f5f8b2065a520846d89771ebaea2bb1534e9c6",
          "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
        },
        { "paperId": null, "title": "Program synthesis with large language" },
        {
          "paperId": "3efbcfeeb0ea1051a71101d3318da4411081f0b8",
          "title": "Scaling Laws for Autoregressive Generative Modeling"
        },
        {
          "paperId": "5fe0a4af3bd1479d5e39fbda2215c86bce54722b",
          "title": "Generative Language Modeling for Automated Theorem Proving"
        },
        {
          "paperId": "da97bd6d2d0a2f11bb011b9925585e086010cff0",
          "title": "A Promising Path Towards Autoformalization and General Artificial Intelligence"
        },
        {
          "paperId": "70f2c1567ef94fdf4581e1290bf7667cc9a4dcfc",
          "title": "LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning"
        },
        {
          "paperId": "f13e41d24e5d0a68ca662c1b49de398a6fb68251",
          "title": "A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"
        },
        {
          "paperId": "53ebd9e0ef2a6a088a5f977f331e434de256ad26",
          "title": "Modelling High-Level Mathematical Reasoning in Mechanised Declarative Proofs"
        },
        {
          "paperId": "e02b0fc9255d58cae698121a5935bd9793c1883c",
          "title": "MathZero, The Classification Problem, and Set-Theoretic Type Theory"
        },
        {
          "paperId": "bd49e66af9755e6138967eba6aeb37d8190d2b4f",
          "title": "ExpBERT: Representation Engineering with Natural Language Explanations"
        },
        {
          "paperId": "ad5970584754cc7a1d91c95ab84a1e210258183a",
          "title": "UnifiedQA: Crossing Format Boundaries With a Single QA System"
        },
        {
          "paperId": "e816f788767eec6a8ef0ea9eddd0e902435d4271",
          "title": "Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"
        },
        {
          "paperId": "43f2ad297941db230c089ba353efc3f281ab678c",
          "title": "5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
          "paperId": "e6c561d02500b2596a230b341a8eb8b921ca5bf2",
          "title": "Scaling Laws for Neural Language Models"
        },
        {
          "paperId": "b39eed03d345f5c244eac12fd1315d26eba77d62",
          "title": "Deep Learning for Symbolic Mathematics"
        },
        {
          "paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481",
          "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
        },
        {
          "paperId": "3cfb319689f06bf04c2e28399361f414ca32c4b3",
          "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
        },
        {
          "paperId": "af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2",
          "title": "Transformers: State-of-the-Art Natural Language Processing"
        },
        { "paperId": null, "title": "Language models are few-shot" },
        {
          "paperId": null,
          "title": "2020) introduce BART, which has a bidirectional encoder and unidirectional decoder"
        },
        {
          "paperId": "36efa56d011bc3dc84c2499bd0dfcdcba55cfefb",
          "title": "Improving Graph Neural Network Representations of Logical Formulae with Subgraph Pooling"
        },
        {
          "paperId": "eef7cfe8267954adbb4675576072a1d80ca7a3a8",
          "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"
        },
        {
          "paperId": "d9f6ada77448664b71128bb19df15765336974a6",
          "title": "SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"
        },
        {
          "paperId": "9ef2e09a9e16e176e19c3fdc3b6ee22c5d3f3c97",
          "title": "HOList: An Environment for Machine Learning of Higher-Order Theorem Proving (extended version)"
        },
        {
          "paperId": "afed6dc6900d3b37e528b9086661bba583d60bf6",
          "title": "Analysing Mathematical Reasoning Abilities of Neural Models"
        },
        {
          "paperId": "6f79e82b19470a78eb1f597e1662674b9320a6d9",
          "title": "GamePad: A Learning Environment for Theorem Proving"
        },
        {
          "paperId": "d07284a6811f1b2745d91bdb06b040b57f226882",
          "title": "Decoupled Weight Decay Regularization"
        },
        {
          "paperId": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
          "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
        },
        {
          "paperId": "b123a0d46ad917b79c43c5ae981e03ed2458ed11",
          "title": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"
        },
        {
          "paperId": "6ff2a434578ff2746b9283e45abf296887f48a2d",
          "title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks"
        },
        { "paperId": null, "title": "Attention is all you" },
        {
          "paperId": "09900b36ca35af12301507cf992675808a709838",
          "title": "How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"
        },
        {
          "paperId": "8388f1be26329fa45e5807e968a641ce170ea078",
          "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"
        },
        {
          "paperId": "cea967b59209c6be22829699f05b8b1ac4dc092d",
          "title": "Sequence to Sequence Learning with Neural Networks"
        },
        {
          "paperId": "687684749814cbe54672d3b53ea08df6c66382bb",
          "title": "Beyond the Turing Test"
        },
        {
          "paperId": "8e8ec502208f29ee9f78ded19226578e027ecd16",
          "title": "Universal Intelligence: A Definition of Machine Intelligence"
        },
        {
          "paperId": "395cc693f69e61bb5212f77015560570a97a1b20",
          "title": "Solve it."
        },
        {
          "paperId": "3ac7cae59622f1f91833dbd0d9a8313f21b97614",
          "title": "The Unreasonable Effectiveness of Mathematics in the Natural Sciences"
        },
        {
          "paperId": null,
          "title": "Don’t stop pretraining: Adapt language models to domains and tasks. ArXiv"
        },
        {
          "paperId": null,
          "title": "Don't stop pretraining: Adapt language models to domains and tasks. ArXiv, abs"
        },
        {
          "paperId": "6e51d8e92729366721de4b886ce3c26b124c9e58",
          "title": "A Formal Definition of Intelligence Based on an Intensional Variant of Algorithmic Complexity"
        },
        {
          "paperId": null,
          "title": "Scaling laws for transfer. arXiv, 2021. J. Hernández-Orallo. Beyond the turing test"
        },
        {
          "paperId": "fa96245255f18739dab9e70cc4ae5dda0765b509",
          "title": "The Unreasonable Effectiveness of Mathematics in the Natural Sciences (reprint)"
        },
        {
          "paperId": null,
          "title": "hopital's rule: 0/0; l'hopital's rule: ∞/∞; lagrange error bound"
        },
        {
          "paperId": null,
          "title": "integration by parts; integration by parts: definite integrals"
        },
        {
          "paperId": null,
          "title": "integration using completing the square; integration using long division; integration using trigonometric identities"
        },
        {
          "paperId": null,
          "title": "multiplying rational numbers; multivariable chain rule; multivariable chain rule intro; negative exponents"
        },
        {
          "paperId": null,
          "title": "Khan Academy Modules (3/4): integrate and differentiate power series"
        },
        {
          "paperId": null,
          "title": "number of solutions of quadratic equations; one step equations; one step equations with multiplication"
        },
        {
          "paperId": null,
          "title": "special right triangles; square and cube challenge; square roots of perfect squares"
        },
        {
          "paperId": null,
          "title": "parametric curve arc length; parametric equations differentiation; parametric velocity and speed"
        },
        {
          "paperId": null,
          "title": "Khan Academy Modules (4/4): properties of exponents (rational exponents); proportion word problems; pythagorean identities; pythagorean theorem"
        },
        {
          "paperId": null,
          "title": "multiplying polynomials; multiplying polynomials 0.5; multiplying positive and negative fractions"
        },
        {
          "paperId": null,
          "title": "systems of equations with substitution; systems of equations word problems; tangents to polar curves"
        },
        {
          "paperId": null,
          "title": "-digit divisors); divide with remainders (2-digit by 1-digit); dividing complex numbers"
        },
        {
          "paperId": null,
          "title": "probability with permutations and combinations; problems involving definite integrals (algebraic)"
        },
        {
          "paperId": null,
          "title": "integration with partial fractions; intercepts from an equation; interpret quadratic models; interval of convergence; inverse of a 3x3 matrix; inverses of functions"
        },
        {
          "paperId": null,
          "title": "Deep learning for symbolic mathematics. ArXiv, abs"
        },
        {
          "paperId": null,
          "title": "relative minima and maxima; remainder theorem; remainder theorem and factors"
        }
      ],
      "summary": "This work introduces MATH, a new dataset of 12, 500 challenging competition mathematics problems which can be used to teach models to generate answer derivations and explanations, and shows that accuracy remains relatively low, even with enormous Transformer models.",
      "title": "Measuring Mathematical Problem Solving With the MATH Dataset",
      "venue": "NeurIPS Datasets and Benchmarks",
      "year": 2021,
      "authors": "Dan Hendrycks,Collin Burns,Saurav Kadavath,Akul Arora,Steven Basart,Eric Tang,D. Song,J. Steinhardt"
    },
    {
      "id": "13c4e5a6122f3fa2663f63e49537091da6532f35",
      "url": "https://www.semanticscholar.org/paper/13c4e5a6122f3fa2663f63e49537091da6532f35",
      "citationCount": 42,
      "influentialCitationCount": 15,
      "referenceCount": 25,
      "citations": [
        {
          "paperId": "a4f1793b23a7f7ec3d65824d6ca3e6011567980d",
          "title": "ELASTIC: Numerical Reasoning with Adaptive Symbolic Compiler"
        },
        {
          "paperId": "2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8",
          "title": "Mind's Eye: Grounded Language Model Reasoning through Simulation"
        },
        {
          "paperId": "90350aa626bed47b02d0c162462e5b0ca82be6b2",
          "title": "Automatic Chain of Thought Prompting in Large Language Models"
        },
        {
          "paperId": "2fe6060ced80c1c245a718e6188b6516207bf0a8",
          "title": "Augmenting Operations Research with Auto-Formulation of Optimization Models from Problem Descriptions"
        },
        {
          "paperId": "7cb3162d4a171ffc4826b8937dc66db32831f7ea",
          "title": "Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning"
        },
        {
          "paperId": "60f208b19bb63d82fda5759897677f92d4e1e2fc",
          "title": "Improving Compositional Generalization in Math Word Problem Solving"
        },
        {
          "paperId": "74cc3d340039c67bdabaef090d1386fe2c5376ca",
          "title": "CLEVR-Math: A Dataset for Compositional Language, Visual and Mathematical Reasoning"
        },
        {
          "paperId": "290732e9fb08a29af8892a7c1f73c9d2a1b9d7db",
          "title": "Language models show human-like content effects on reasoning"
        },
        {
          "paperId": "2a0c444d14c4758ec37c32b20551f6048541f035",
          "title": "Expression Syntax Information Bottleneck for Math Word Problems"
        },
        {
          "paperId": "a0ec4e6fb5b6830a57139dc12b65cda4f0fdf635",
          "title": "Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)"
        },
        {
          "paperId": "4288d44c2b8e6a89607780caf1272061028f6f97",
          "title": "On the Advance of Making Language Models Better Reasoners"
        },
        {
          "paperId": "3ce8c07349d91bb3f022a211be36e98eef0e1046",
          "title": "MMTM: Multi-Tasking Multi-Decoder Transformer for Math Word Problems"
        },
        {
          "paperId": "d9055c52b9454d91be09c197b11ed3f60ee7bb0a",
          "title": "Why are NLP Models Fumbling at Elementary Math? A Survey of Deep Learning based Word Problem Solvers"
        },
        {
          "paperId": "8abe4f7bfe89db1cf8f74831ca7e3af8c20d7808",
          "title": "Is a Question Decomposition Unit All We Need?"
        },
        {
          "paperId": "f75a0ccb3c9b7e12dfb8b9c9dc3d35ca4518a643",
          "title": "Large Language Models are Zero-Shot Reasoners"
        },
        {
          "paperId": "5e3a7235c730b512f0bb3004a6deb0c88ec9fc8e",
          "title": "On the Paradox of Learning to Reason from Data"
        },
        {
          "paperId": "4290fb60fe185b564c318b6054755fc8fbde20e7",
          "title": "LogicSolver: Towards Interpretable Math Word Problem Solving with Logical Prompt-enhanced Learning"
        },
        {
          "paperId": "4feb4f49c5837cbb1c9c8e61d4e8056cafb102e6",
          "title": "Unbiased Math Word Problems Benchmark for Mitigating Solving Bias"
        },
        {
          "paperId": "645b10b7802a035e034488e3640fc0bc415de34c",
          "title": "UL2: Unifying Language Learning Paradigms"
        },
        {
          "paperId": "0232a71bd47b6410dea4b00559acb93755f65fff",
          "title": "Practice Makes a Solver Perfect: Data Augmentation for Math Word Problem Solvers"
        },
        {
          "paperId": "c9f48406851954cb098911eccb4124ea5f966675",
          "title": "A Survey on Multi-hop Question Answering and Generation"
        },
        {
          "paperId": "094ff971d6a8b8ff870946c9b3ce5aa173617bfb",
          "title": "PaLM: Scaling Language Modeling with Pathways"
        },
        {
          "paperId": "79b88230fabb59a1d368641bbc822af0f09bf262",
          "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
        },
        {
          "paperId": "4333161031b842949ef75d60d982438ca52d8ecc",
          "title": "Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction"
        },
        {
          "paperId": "833a2f1817cb9aeb292620454889cae78e26dda4",
          "title": "Impact of Pretraining Term Frequencies on Few-Shot Reasoning"
        },
        {
          "paperId": "5d0db797a45ce2453f821f7ded0b547d3fdab054",
          "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models"
        },
        {
          "paperId": "94c0a8be74d69787a1f3f6e91dcb480a2fd0dd56",
          "title": "Reasoning Like Program Executors"
        },
        {
          "paperId": "2ff977905770df1bb75b69cbbeb8e242c5ac46b7",
          "title": "Improving a Graph-to-Tree Model for Solving Math Word Problems"
        },
        {
          "paperId": "39df1a17da84f02bfcb8751de8965b798653a5ee",
          "title": "Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems"
        },
        {
          "paperId": "934bdb14923510a2dedd3682a3f2c89f7e1ab364",
          "title": "MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers"
        },
        {
          "paperId": "909003934f2e68527c5638fae1b60821eaf438f1",
          "title": "HAWP: a Dataset for Hindi Arithmetic Word Problem Solving"
        },
        {
          "paperId": "4aca69be58a271b1be45ec7ebb3586569cec50b0",
          "title": "ArMATH: a Dataset for Solving Arabic Math Word Problems"
        },
        {
          "paperId": "64fef156c226b9fc812d80eb119fb074cf575279",
          "title": "Evaluating State-of-the-Art Visual Question Answering Models Ability to Answer Complex Counting Questions"
        },
        {
          "paperId": "f729e16ee6765e2fd76772cbf9d3dd17b4d25438",
          "title": "Towards Tractable Mathematical Reasoning: Challenges, Strategies, and Opportunities for Solving Math Word Problems"
        },
        {
          "paperId": "927efd299cffcfca3716efefcc904331b70c153e",
          "title": "NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset"
        },
        {
          "paperId": "68900bf4b9cc820f4b47608871eafcac65a33917",
          "title": "Adversarial Examples for Evaluating Math Word Problem Solvers"
        },
        {
          "paperId": "28a5a53dafacebad8a7c47773079caeffb9a5baa",
          "title": "Representing Numbers in NLP: a Survey and a Vision"
        },
        {
          "paperId": "f645347e06b430d32f916a3c673acfd348e23058",
          "title": "Towards Interpretable Math Word Problem Solving with Grounded Linguistic Logic Reasoning"
        },
        {
          "paperId": "37cc867e3893f407b572113705ecd0f6efc8b9fc",
          "title": "Improving Equation Set Problems with Label Augmentation"
        },
        {
          "paperId": "3eae64c02e448e656305dde5d496f30db423683d",
          "title": "MATHion: Solving Math Word Problems with Logically Consistent Problems"
        },
        {
          "paperId": "15e2520e7f3bd96ba9d9d6e967771b34e448a9f1",
          "title": "Compositional Generalization and Neuro-Symbolic Architectures"
        },
        {
          "paperId": "8424082e3bf4792462eb112d7ebcecf5b0dc3613",
          "title": "Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning"
        }
      ],
      "references": [
        {
          "paperId": "f9c07ed1d2113c858b38861790af6a26310b8465",
          "title": "Semantically-Aligned Universal Tree-Structured Solver for Math Word Problems"
        },
        {
          "paperId": "91bad6519095404998f4ce23592547b409cdb60a",
          "title": "Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data"
        },
        {
          "paperId": "f13e41d24e5d0a68ca662c1b49de398a6fb68251",
          "title": "A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers"
        },
        {
          "paperId": "ac9173e4b8b34dc24c18980ee32be6bf15e7b38d",
          "title": "Graph-to-Tree Learning for Solving Math Word Problems"
        },
        {
          "paperId": "33ec7eb2168e37e3007d1059aa96b9a63254b4da",
          "title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList"
        },
        {
          "paperId": "d0cda85c030711aaa5383c80d5928a4d22f8d3bf",
          "title": "How Can We Accelerate Progress Towards Human-like Linguistic Generalization?"
        },
        {
          "paperId": "207da6d2c07289bf72a2b5974bb3f011ebb5dd0d",
          "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding"
        },
        {
          "paperId": "a810362ca5284e5894b07abb76b8f647802fe32c",
          "title": "The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem Solvers"
        },
        {
          "paperId": "ce177672b00ddf46e4906157a7e997ca9338b8b9",
          "title": "Attention is not not Explanation"
        },
        {
          "paperId": "5bf6ca86a4f21d07014ce60f3cd09345ad3414e2",
          "title": "A Goal-Driven Tree-Structured Neural Model for Math Word Problems"
        },
        {
          "paperId": "077f8329a7b6fa3b7c877a57b81eb6c18b5f87de",
          "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
        },
        {
          "paperId": "eef7cfe8267954adbb4675576072a1d80ca7a3a8",
          "title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"
        },
        {
          "paperId": "42ed4a9994e6121a9f325f5b901c5b3d7ce104f5",
          "title": "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference"
        },
        {
          "paperId": "668f42a4d4094f0a66d402a16087e14269b31a1f",
          "title": "Analysis Methods in Neural Language Processing: A Survey"
        },
        {
          "paperId": "2997b26ffb8c291ce478bd8a6e47979d5a55c466",
          "title": "Annotation Artifacts in Natural Language Inference Data"
        },
        {
          "paperId": "83cc0d20275fdc3a97cceacdc41fbb19953fa901",
          "title": "Mapping to Declarative Knowledge for Word Problem Solving"
        },
        {
          "paperId": "678fd7c48efe21434148b4b3482c2b8b3ee618fc",
          "title": "Deep Neural Solver for Math Word Problems"
        },
        {
          "paperId": "c20584e878efbc4d716710b9aa2f4954c4014638",
          "title": "Learning Fine-Grained Expressions to Solve Math Word Problems"
        },
        {
          "paperId": "7220d7ec31f6dc6ad7030f601743b5392513a2d9",
          "title": "Learning to Solve Geometry Problems from Natural Language Demonstrations in Textbooks"
        },
        {
          "paperId": "624527d8b92bc6d423784113ded0b9fd639add00",
          "title": "Pay Attention to the Ending:Strong Neural Baselines for the ROC Story Cloze Task"
        },
        {
          "paperId": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
          "title": "Attention is All you Need"
        },
        {
          "paperId": "09900b36ca35af12301507cf992675808a709838",
          "title": "How well do Computers Solve Math Word Problems? Large-Scale Dataset Construction and Evaluation"
        },
        {
          "paperId": "2bdbb07bc12b8d8c332b7a84aa05e76218c07cd9",
          "title": "MAWPS: A Math Word Problem Repository"
        },
        {
          "paperId": "93499a7c7f699b6630a86fad964536f9423bb6d0",
          "title": "Effective Approaches to Attention-based Neural Machine Translation"
        },
        {
          "paperId": null,
          "title": "Hyperparameters Scratch RoBERTa Scratch RoBERTa Scratch RoBERTa Scratch RoBERTa Embedding Size"
        }
      ],
      "summary": "It is shown that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs, and models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy.",
      "title": "Are NLP Models really able to Solve Simple Math Word Problems?",
      "venue": "NAACL",
      "year": 2021,
      "authors": "Arkil Patel,S. Bhattamishra,Navin Goyal"
    }
  ]
}
